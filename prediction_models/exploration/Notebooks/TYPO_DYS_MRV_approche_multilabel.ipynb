{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Inférence dudysfonctionnement  - Stratégie Multilabels\n",
    "Dans ce Notebook, nous cosntruisons un modèle qui permet d'inférer DYSFONCTIONNEMENT à partir de la classification de l'incident et des données textuelles\n",
    "\n",
    "Nous considérons ce problème comme un problème de classification multiclasses et multilabels. En effet, il y a plusieurs effets possibles et un incidents peut entrainer plusieurs effets.\n",
    "\n",
    "Ainsi, notre métrique d'évaluation sera le f1_samples\n",
    "\n",
    "Dans le Notebook précedent, nous n'avions pas pris en compte l'aspect multilabel et notre score était de  f1_weighted = 0,28.\n",
    "\n",
    "Volontairement dans un premier temps, nous ne modifions pas les paramères de notre modèle afin d'avaluer l'apport de la stratégie multilabelles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "import logging\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "lang ='french'\n",
    "\n",
    "import clean_text\n",
    "import skmultilearn\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,HashingVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GroupShuffleSplit,KFold, ShuffleSplit\n",
    "from sklearn.feature_extraction.text import TfidfTransformer,CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, balanced_accuracy_score,f1_score,classification_report\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.decomposition import TruncatedSVD,IncrementalPCA,SparsePCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "import spacy\n",
    "nlp =spacy.load('fr')\n",
    "from spacy.lang.fr.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 21.7 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "df_declaration_mrv = pd.read_csv(\"data/data_mrv/declaration_mrv_complet.csv\")#delimiter=';',encoding='ISO-8859-1')\n",
    "id_to_dco = pd.read_csv(\"data/ref_MRV/referentiel_dispositif.csv\",delimiter=';',encoding='ISO-8859-1')\n",
    "df_effets = pd.read_csv(\"data/ref_MRV/referentiel_dispositif_effets_connus.csv\",delimiter=';',encoding='ISO-8859-1')\n",
    "df_dys = pd.read_csv(\"data/ref_MRV/referentiel_dispositif_dysfonctionnement.csv\",delimiter=';',encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DCO_ID</th>\n",
       "      <th>DCO</th>\n",
       "      <th>TDY_ID</th>\n",
       "      <th>TYPE_DYSFONCTIONNEMENT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2360</td>\n",
       "      <td>EXPLORATION FONCTIONNELLE ( DIVERS )</td>\n",
       "      <td>1854</td>\n",
       "      <td>NON FONCTIONNEMENT / FONCTIONNEMENT INCORRECT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2360</td>\n",
       "      <td>EXPLORATION FONCTIONNELLE ( DIVERS )</td>\n",
       "      <td>1859</td>\n",
       "      <td>NON RENSEIGNE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2360</td>\n",
       "      <td>EXPLORATION FONCTIONNELLE ( DIVERS )</td>\n",
       "      <td>1821</td>\n",
       "      <td>MESURE ERRONEE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3296</td>\n",
       "      <td>DMU D' APHERESE ECHANGE PLASMATIQUE</td>\n",
       "      <td>2012</td>\n",
       "      <td>RUPTURE D'INTEGRITE POCHE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3296</td>\n",
       "      <td>DMU D' APHERESE ECHANGE PLASMATIQUE</td>\n",
       "      <td>2015</td>\n",
       "      <td>RUPTURE D'INTEGRITE TUBULURE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18990</th>\n",
       "      <td>2641</td>\n",
       "      <td>SUBSTITUT OSSEUX ( ORTHOPEDIE )</td>\n",
       "      <td>1823</td>\n",
       "      <td>MIGRATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18991</th>\n",
       "      <td>4981</td>\n",
       "      <td>RECHAUFFEUR DE SANG POUR MONITEUR D'HEMODIALYSE</td>\n",
       "      <td>1997</td>\n",
       "      <td>RUPTURE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18992</th>\n",
       "      <td>4981</td>\n",
       "      <td>RECHAUFFEUR DE SANG POUR MONITEUR D'HEMODIALYSE</td>\n",
       "      <td>1544</td>\n",
       "      <td>DECONNEXION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18993</th>\n",
       "      <td>4981</td>\n",
       "      <td>RECHAUFFEUR DE SANG POUR MONITEUR D'HEMODIALYSE</td>\n",
       "      <td>1854</td>\n",
       "      <td>NON FONCTIONNEMENT / FONCTIONNEMENT INCORRECT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18994</th>\n",
       "      <td>5021</td>\n",
       "      <td>BALLON DE DILATATION (OBSTETRIQUE)</td>\n",
       "      <td>1690</td>\n",
       "      <td>DM NON CONFORME</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18995 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       DCO_ID                                              DCO  TDY_ID  \\\n",
       "0        2360             EXPLORATION FONCTIONNELLE ( DIVERS )    1854   \n",
       "1        2360             EXPLORATION FONCTIONNELLE ( DIVERS )    1859   \n",
       "2        2360             EXPLORATION FONCTIONNELLE ( DIVERS )    1821   \n",
       "3        3296              DMU D' APHERESE ECHANGE PLASMATIQUE    2012   \n",
       "4        3296              DMU D' APHERESE ECHANGE PLASMATIQUE    2015   \n",
       "...       ...                                              ...     ...   \n",
       "18990    2641                  SUBSTITUT OSSEUX ( ORTHOPEDIE )    1823   \n",
       "18991    4981  RECHAUFFEUR DE SANG POUR MONITEUR D'HEMODIALYSE    1997   \n",
       "18992    4981  RECHAUFFEUR DE SANG POUR MONITEUR D'HEMODIALYSE    1544   \n",
       "18993    4981  RECHAUFFEUR DE SANG POUR MONITEUR D'HEMODIALYSE    1854   \n",
       "18994    5021               BALLON DE DILATATION (OBSTETRIQUE)    1690   \n",
       "\n",
       "                              TYPE_DYSFONCTIONNEMENT  \n",
       "0      NON FONCTIONNEMENT / FONCTIONNEMENT INCORRECT  \n",
       "1                                      NON RENSEIGNE  \n",
       "2                                     MESURE ERRONEE  \n",
       "3                          RUPTURE D'INTEGRITE POCHE  \n",
       "4                       RUPTURE D'INTEGRITE TUBULURE  \n",
       "...                                              ...  \n",
       "18990                                      MIGRATION  \n",
       "18991                                        RUPTURE  \n",
       "18992                                    DECONNEXION  \n",
       "18993  NON FONCTIONNEMENT / FONCTIONNEMENT INCORRECT  \n",
       "18994                                DM NON CONFORME  \n",
       "\n",
       "[18995 rows x 4 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2 Netoyage des données :\n",
    "- Elimination de l'effet : PAS D'EFFET NEFASTE DECLARE\n",
    "- Suppression des classes sous représentées (<15 occurences)\n",
    "- Netoyyage des donénes textuelles\n",
    "- Encodage de la classification et des effets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['D1790', 'D1859', 'D1848', 'D2091', 'D1689', 'D1932', 'D1886',\n",
       "       'D1468', 'D1693', 'D1909', 'D1570', 'D1656', nan, 'D1997', 'D1538',\n",
       "       'D1625', 'D1539', 'D1854', 'D2219', 'D2047', 'D1759', 'D1756',\n",
       "       'D1852', 'D1936', 'D1627', 'D1856', 'D1646', 'D1951', 'D1464',\n",
       "       'D2032', 'D1643', 'D1510', 'D1751', 'D1684', 'D2010', 'D1484',\n",
       "       'D2216', 'D1713', 'D2026', 'D1985', 'D2033', 'D2006', 'D1755',\n",
       "       'D1613', 'D1823', 'D1575', 'D1733', 'D2082', 'D2015', 'D1802',\n",
       "       'D1675', 'D2012', 'D1712', 'D1628', 'D1870', 'D1813', 'D1730',\n",
       "       'D1498', 'D1965', 'D1690', 'D1537', 'D1825', 'D1791', 'D1921',\n",
       "       'D1917', 'D1663', 'D1913', 'D1864', 'D2039', 'D2256', 'D1795',\n",
       "       'D1642', 'D1567', 'D1530', 'D1516', 'D1837', 'D1680', 'D1692',\n",
       "       'D1658', 'D1891', 'D2009', 'D2042', 'D1954', 'D1682', 'D1905',\n",
       "       'D1630', 'D1597', 'D1669', 'D1532', 'D2025', 'D2084', 'D2027',\n",
       "       'D1486', 'D1520', 'D1919', 'D1535', 'D2023', 'D1569', 'D1652',\n",
       "       'D2067', 'D2096', 'D1918', 'D1471', 'D2296', 'D1511', 'D1999',\n",
       "       'D1761', 'D2037', 'D1890', 'D2176', 'D1850', 'D1554', 'D1563',\n",
       "       'D1868', 'D1477', 'D1849', 'D1552', 'D2060', 'D2017', 'D1514',\n",
       "       'D1583', 'D1544', 'D1550', 'D1602', 'D1775', 'D1993', 'D1636',\n",
       "       'D1963', 'D2005', 'D1967', 'D1618', 'D1576', 'D1737', 'D1821',\n",
       "       'D2094', 'D2338', 'D1650', 'D1691', 'D1645', 'D1783', 'D1957',\n",
       "       'D1707', 'D2004', 'D1478', 'D1534', 'D1556', 'D1897', 'D1959',\n",
       "       'D1677', 'D1908', 'D1875', 'D2054', 'D1887', 'D2003', 'D1961',\n",
       "       'D1488', 'D1814', 'D1914', 'D1480', 'D1831', 'D1614', 'D1571',\n",
       "       'D1502', 'D1481', 'D1741', 'D2217', 'D1796', 'D2028', 'D1592',\n",
       "       'D1577', 'D2085', 'D1939', 'D1820', 'D1482', 'D1925', 'D1838',\n",
       "       'D1462', 'D1559', 'D1525', 'D1710', 'D1828', 'D1942', 'D1599',\n",
       "       'D1660', 'D1927', 'D1776', 'D2477', 'D2059', 'D1581', 'D2075',\n",
       "       'D1661', 'D1955', 'D2065', 'D1439', 'D1784', 'D2064', 'D1766',\n",
       "       'D1960', 'D1923', 'D1726', 'D1463', 'D1629', 'D2040', 'D1639',\n",
       "       'D1649', 'D1475', 'D1899', 'D1562', 'D1729', 'D2014', 'D1844',\n",
       "       'D1491', 'D1472', 'D1943', 'D1572', 'D1673', 'D1593', 'D1800',\n",
       "       'D2336', 'D1998', 'D1801', 'D2046', 'D2196', 'D1561', 'D1867',\n",
       "       'D1906', 'D1657', 'D1654', 'D1711', 'D1981', 'D1771', 'D1702',\n",
       "       'D2034', 'D1505', 'D1604', 'D1704', 'D1879', 'D1558', 'D1727',\n",
       "       'D1785', 'D1676', 'D1523', 'D1883', 'D1885', 'D1948', 'D1479',\n",
       "       'D1637', 'D1601', 'D1695', 'D2716', 'D3016', 'D1840', 'D1542',\n",
       "       'D1950', 'D1579', 'D2236', 'D1487', 'D1846', 'D2776', 'D2777',\n",
       "       'D2058', 'D1489', 'D1665', 'D1466', 'D1830', 'D1448', 'D1600',\n",
       "       'D2177', 'D1811', 'D1607', 'D2002', 'D1557', 'D1644', 'D1722',\n",
       "       'D2041', 'D1664', 'D1634', 'D1787', 'D2319', 'D1528', 'D1545',\n",
       "       'D1454', 'D1699', 'D1450', 'D1929', 'D2717', 'D1709', 'D2601',\n",
       "       'D1786', 'D1667', 'D2337', 'D1671', 'D1694', 'D1841', 'D1584',\n",
       "       'D1767', 'D1674', 'D1866', 'D1705', 'D1812', 'D2537', 'D2029',\n",
       "       'D2077', 'D2080', 'D1969', 'D1700', 'D1540', 'D1819', 'D1446',\n",
       "       'D2317', 'D2057', 'D1594', 'D1590', 'D1679', 'D1847', 'D1974',\n",
       "       'D3258', 'D2356', 'D2043', 'D1591', 'D2416', 'D2000', 'D1515',\n",
       "       'D2022', 'D1803', 'D2021', 'D1915', 'D1633', 'D1499', 'D1485',\n",
       "       'D2068', 'D1901', 'D1762', 'D1662', 'D1720', 'D1617', 'D2457',\n",
       "       'D1857', 'D1904', 'D1518', 'D1881', 'D1585', 'D1596', 'D1437',\n",
       "       'D2436', 'D1493', 'D1522', 'D1984', 'D1619', 'D1501', 'D1853',\n",
       "       'D1947', 'D1718', 'D2056', 'D1467', 'D1451', 'D2050', 'D1966',\n",
       "       'D1944', 'D1543', 'D1678', 'D2074', 'D1903', 'D1683', 'D1843',\n",
       "       'D1668', 'D1749', 'D2536', 'D1964', 'D1945', 'D1747', 'D1553',\n",
       "       'D2062', 'D1975', 'D1603', 'D1555', 'D1994', 'D1526', 'D2008',\n",
       "       'D2417', 'D1934', 'D1723', 'D2051', 'D1548', 'D1453', 'D2093',\n",
       "       'D1958', 'D1816', 'D1810', 'D1541', 'D2516', 'D1659', 'D2069',\n",
       "       'D1734', 'D1793', 'D1504', 'D1688', 'D1789', 'D1714', 'D1708',\n",
       "       'D1611', 'D1732', 'D1754', 'D1940', 'D1952', 'D1624', 'D2038',\n",
       "       'D1438', 'D1470', 'D1621', 'D1731', 'D2602', 'D1598', 'D1595',\n",
       "       'D1978', 'D2019', 'D1449', 'D1935', 'D1457', 'D2053', 'D1524',\n",
       "       'D1861', 'D1872', 'D1996', 'D1894', 'D1779', 'D1956', 'D1612',\n",
       "       'D2024', 'D2496', 'D1648', 'D1871', 'D1982', 'D1878', 'D1758',\n",
       "       'D1902', 'D1549', 'D1877', 'D1968', 'D1979', 'D1845', 'D2478',\n",
       "       'D2018', 'D1924', 'D1743', 'D2092', 'D2660', 'D1442', 'D2556',\n",
       "       'D1521', 'D2079', 'D1716', 'D1725', 'D1474', 'D1834', 'D2658',\n",
       "       'D1842', 'D1578', 'D2055', 'D2396', 'D1910', 'D1836', 'D2616',\n",
       "       'D2599', 'D1907', 'D2078', 'D1672', 'D2600', 'D1778', 'D2697',\n",
       "       'D2596', 'D1508', 'D1529', 'D1697', 'D2636', 'D1773', 'D1780',\n",
       "       'D1804', 'D2013', 'D2659', 'D1632', 'D1631', 'D2030', 'D1496',\n",
       "       'D1873', 'D1447', 'D1765', 'D1970', 'D2696', 'D2476', 'D1980',\n",
       "       'D1655', 'D1781', 'D1782', 'D1736', 'D1931', 'D1500', 'D1930',\n",
       "       'D1503', 'D1772', 'D2136', 'D1721', 'D1798', 'D1566', 'D1622',\n",
       "       'D1889', 'D1922', 'D2318', 'D1920', 'D1777', 'D1735', 'D2736',\n",
       "       'D1827', 'D1547', 'D1941', 'D1839', 'D1757', 'D1892', 'D1973',\n",
       "       'D1685', 'D1976', 'D1461', 'D1497', 'D1926', 'D1898', 'D1606',\n",
       "       'D1995', 'D2597', 'D2598', 'D2816', 'D2011', 'D1860', 'D2576',\n",
       "       'D1971', 'D1937', 'D2086', 'D1882', 'D1456', 'D1835', 'D1742',\n",
       "       'D1815', 'D2044', 'D1615', 'D2007', 'D1580', 'D2456', 'D2459',\n",
       "       'D1519', 'D1792', 'D1706', 'D2016', 'D2156', 'D1715', 'D1586',\n",
       "       'D1494', 'D1533', 'D1483', 'D1788', 'D2956', 'D2076', 'D2896',\n",
       "       'D1589', 'D3036', 'D3037', 'D2049', 'D2757', 'D1546', 'D1888',\n",
       "       'D1517', 'D2996', 'D1531', 'D2976', 'D1805', 'D3077', 'D1638',\n",
       "       'D2116', 'D3079', 'D1647', 'D2095', 'D3130', 'D1916', 'D3116',\n",
       "       'D1763', 'D3128', 'D2756', 'D1687', 'D3123', 'D2070', 'D3118',\n",
       "       'D1666', 'D3166', 'D3119', 'D3121', 'D3081', 'D3127', 'D1609',\n",
       "       'D1962', 'D3120', 'D3080', 'D3078', 'D3082', 'D1983', 'D3257',\n",
       "       'D3083', 'D1492', 'D3084', 'D2376', 'D3196', 'D2316', 'D3167',\n",
       "       'D1452', 'D3174', 'D3125', 'D3163', 'D1946', 'D1701', 'D3173',\n",
       "       'D3178', 'D3129', 'D1770', 'D3117', 'D3171', 'D3197', 'D3177',\n",
       "       'D1768', 'D3183', 'D3162', 'D3170'], dtype=object)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_declaration_mrv['TDY_ID'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76954\n",
      "75248\n",
      "CPU times: user 29.7 s, sys: 260 ms, total: 30 s\n",
      "Wall time: 30 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#On complète les effets vide comme étant sans effets\n",
    "df_declaration_mrv['TYPE_EFFET']  = df_declaration_mrv['TYPE_EFFET'].fillna(\"PAS D'EFFET NEFASTE DECLARE\")\n",
    "df_declaration_mrv['TEF_ID']= df_declaration_mrv['TEF_ID'].fillna('E1213')\n",
    "\n",
    "#\n",
    "df_declaration_mrv['TYPE_DYSFONCTIONNEMENT']  = df_declaration_mrv['TYPE_DYSFONCTIONNEMENT'].fillna(\"PAS DE DYSFONCTIONNEMENT\")\n",
    "df_declaration_mrv['TDY_ID']  = df_declaration_mrv['TDY_ID'].fillna(\"D0\")\n",
    "\n",
    "#on selectionne les colonnes avec des effets\n",
    "df = df_declaration_mrv[['DESCRIPTION_INCIDENT','TYPE_VIGILANCE','LIBELLE_COMMERCIAL',\n",
    "                         'REFERENCE_COMMERCIALE','ETAT_PATIENT','FABRICANT','DCO_ID',\n",
    "                         'ACTION_PATIENT','CLASSIFICATION','TDY_ID']]#[df_declaration_mrv['TEF_ID']!='E1213']\n",
    "\n",
    "# On complète les NaN avec du vide\n",
    "df['ETAT_PATIENT'] = df['ETAT_PATIENT'].fillna(\"\")\n",
    "df['DESCRIPTION_INCIDENT'] = df['DESCRIPTION_INCIDENT'].fillna(\"\")\n",
    "df['LIBELLE_COMMERCIAL'] = df['LIBELLE_COMMERCIAL'].fillna(\"\")\n",
    "df['FABRICANT'] = df['FABRICANT'].fillna(\"\")\n",
    "df[\"REFERENCE_COMMERCIALE\"] = df['REFERENCE_COMMERCIALE'].fillna(\"\")\n",
    "df['TYPE_VIGILANCE'] = df['TYPE_VIGILANCE'].fillna(\"\")\n",
    "df['CLASSIFICATION'] = df['CLASSIFICATION'].fillna('')\n",
    "df['DCO_ID'] = df['DCO_ID'].fillna(-1)\n",
    "#On nettoieles variables textueelles : \n",
    "\n",
    "for col in  ['DESCRIPTION_INCIDENT','LIBELLE_COMMERCIAL','ETAT_PATIENT','FABRICANT','ACTION_PATIENT'] :\n",
    "    df[col] = df[col].map(lambda x: clean_text.preprocess_text(x))\n",
    "\n",
    "\n",
    "print(len(df))\n",
    "n = 15\n",
    "# On filtre pour a voir plus de n observations par classse\n",
    "df = df.groupby(\"TDY_ID\").filter(lambda x: len(x) > n)\n",
    "print(len(df))\n",
    "#le_v = LabelEncoder()\n",
    "#df.TYPE_VIGILANCE = le_v.fit_transform(df.TYPE_VIGILANCE.values)\n",
    "le = LabelEncoder()\n",
    "df.TDY_ID = le.fit_transform(df.TDY_ID.values)\n",
    "\n",
    "df_m = df.groupby('DESCRIPTION_INCIDENT')['TDY_ID'].apply(list).reset_index(name='multilabels')\n",
    "\n",
    "\n",
    "df_ = pd.merge(df,df_m, on = 'DESCRIPTION_INCIDENT')\n",
    "df_['multilabels'] = df_['multilabels'].apply(np.array)\n",
    "df_['multilabels'] = df_['multilabels'].map(np.unique)\n",
    "\n",
    "#df_.to_csv('Multilabel_dataset.csv')\n",
    "\n",
    "#df_ = df_.drop_duplicates('DESCRIPTION_INCIDENT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.3 Construction du jeu de données d'entrainement et de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selection des train et test set\n",
    "train_index,test_index = next(GroupShuffleSplit(random_state=1029).split(df_, groups=df_['DESCRIPTION_INCIDENT']))\n",
    "df_train, df_test = df_.iloc[train_index], df_.iloc[test_index]\n",
    "y = df_train.multilabels\n",
    "y_test =df_test.multilabels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.4 Encodage multilabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = MultiLabelBinarizer()\n",
    "y_lb =lb.fit_transform(y) \n",
    "y_test_lb = lb.transform(y_test)\n",
    "X = df_train[['FABRICANT','CLASSIFICATION','DESCRIPTION_INCIDENT','TYPE_VIGILANCE','ETAT_PATIENT']]\n",
    "X_test = df_test[['FABRICANT','CLASSIFICATION','DESCRIPTION_INCIDENT','TYPE_VIGILANCE','ETAT_PATIENT']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Construction du pipeline avec une stratégie ONE-VS-REST\n",
    "\n",
    "This strategy, also known as one-vs-all, is implemented in OneVsRestClassifier. The strategy consists in fitting one classifier per class. For each classifier, the class is fitted against all the other classes. In addition to its computational efficiency (only n_classes classifiers are needed), one advantage of this approach is its interpretability. Since each class is represented by one and only one classifier, it is possible to gain knowledge about the class by inspecting its corresponding classifier. This is the most commonly used strategy and is a fair default choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 339 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "preprocess = ColumnTransformer(\n",
    "    [('description_tfidf',TfidfVectorizer(sublinear_tf=True, min_df=3,\n",
    "                            ngram_range=(1, 1),\n",
    "                            stop_words=STOP_WORDS,\n",
    "                            max_features = 10000,norm = 'l2'), 'DESCRIPTION_INCIDENT'),\n",
    "     \n",
    "     ('etat_pat_tfidf', TfidfVectorizer(sublinear_tf=True, min_df=3,ngram_range=(1, 1),\n",
    "                                       stop_words=STOP_WORDS,\n",
    "                                       max_features = 10000,norm = 'l2'), 'ETAT_PATIENT'),\n",
    "     \n",
    "     ('fabricant_tfidf',TfidfVectorizer(sublinear_tf=True, min_df=3,\n",
    "                            ngram_range=(1, 1),\n",
    "                            stop_words=STOP_WORDS,\n",
    "                            max_features = 5000,norm = 'l2'), 'FABRICANT'),\n",
    "    \n",
    "    ('classification_enc', TfidfVectorizer(sublinear_tf=True, min_df=5,\n",
    "                            ngram_range=(1, 1),\n",
    "                            stop_words=STOP_WORDS,\n",
    "                            max_features = 100,norm = 'l2'),'CLASSIFICATION'),\n",
    "     \n",
    "     ('type_vigilance_enc',OneHotEncoder(),'TYPE_VIGILANCE')\n",
    "     ],\n",
    "    \n",
    "    remainder='passthrough')\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', preprocess),\n",
    "    ('clf', OneVsRestClassifier(CalibratedClassifierCV(LinearSVC(class_weight='balanced'),cv=3, method='isotonic'))),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Evaluation du pipeline en cross validation : l'importance de la séparation train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cv = KFold(n_splits=5, random_state=0)\n",
    "result= cross_validate(pipeline, X, y_lb, scoring='f1_samples', cv=cv,n_jobs=-1)\n",
    "result['test_score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cv = ShuffleSplit(n_splits=5, random_state=0)\n",
    "result= cross_validate(pipeline, X, y_lb, scoring='f1_samples', cv=cv,n_jobs=-1)\n",
    "result['test_score'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Si on supprimer les doublons ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score samples :  0.46811272403864995\n"
     ]
    }
   ],
   "source": [
    "## Supression des doublons\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "\n",
    "\n",
    "df_s = df_.drop_duplicates('DESCRIPTION_INCIDENT')\n",
    "\n",
    "lb_s = MultiLabelBinarizer()\n",
    "y_s = df_s.multilabels\n",
    "y_s  = lb_s.fit_transform(y_s)\n",
    "\n",
    "df_s = df_s[['FABRICANT','CLASSIFICATION','TYPE_VIGILANCE','DESCRIPTION_INCIDENT','ETAT_PATIENT']]\n",
    "\n",
    "mskf = MultilabelStratifiedKFold(n_splits=2, random_state=0)\n",
    "\n",
    "train_index_s,test_index_s = next(mskf.split(df_s,y_s))\n",
    "df_train_s, df_test_s = df_s.iloc[train_index_s], df_s.iloc[test_index_s]\n",
    "y_train_s = y_s[train_index_s]\n",
    "y_test_s =y_s[test_index_s]\n",
    "\n",
    "#lb_s = MultiLabelBinarizer()\n",
    "#y_train_lb_s =lb_s.fit_transform(y_train_s) \n",
    "#y_test_lb_s = lb.transform(y_test_s)\n",
    "X_train_s = df_train_s[['FABRICANT','CLASSIFICATION','TYPE_VIGILANCE','DESCRIPTION_INCIDENT','ETAT_PATIENT']]\n",
    "X_test_s = df_test_s[['FABRICANT','CLASSIFICATION','TYPE_VIGILANCE','DESCRIPTION_INCIDENT','ETAT_PATIENT']]\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', preprocess),\n",
    "    ('clf', OneVsRestClassifier(LinearSVC(class_weight='balanced'))),\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train_s,y_train_s)\n",
    "#y_test_lb = lb.transform(y_test)\n",
    "Y_pred_ovr_s = pipeline.predict(X_test_s)\n",
    "f1 = f1_score(y_test_s , Y_pred_ovr_s,average='samples')\n",
    "print('f1_score samples : ',f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.74      0.73      3057\n",
      "           1       0.00      0.00      0.00         9\n",
      "           2       0.00      0.00      0.00         7\n",
      "           3       1.00      0.10      0.18        10\n",
      "           4       1.00      0.25      0.40         4\n",
      "           5       0.67      0.40      0.50         5\n",
      "           6       1.00      0.22      0.36         9\n",
      "           7       0.64      0.26      0.37        27\n",
      "           8       0.46      0.22      0.30        27\n",
      "           9       0.50      0.14      0.22         7\n",
      "          10       0.45      0.26      0.33        38\n",
      "          11       0.50      0.07      0.12        14\n",
      "          12       1.00      0.29      0.44         7\n",
      "          13       0.41      0.36      0.38        36\n",
      "          14       0.00      0.00      0.00        11\n",
      "          15       0.45      0.16      0.23        57\n",
      "          16       0.76      0.65      0.70        65\n",
      "          17       0.50      0.18      0.27        11\n",
      "          18       0.00      0.00      0.00        14\n",
      "          19       0.69      0.35      0.46        26\n",
      "          20       0.00      0.00      0.00         8\n",
      "          21       0.00      0.00      0.00        15\n",
      "          22       0.50      0.10      0.17        10\n",
      "          23       0.00      0.00      0.00        16\n",
      "          24       0.60      0.56      0.58       193\n",
      "          25       0.25      0.06      0.10        17\n",
      "          26       0.00      0.00      0.00        16\n",
      "          27       0.82      0.62      0.71        45\n",
      "          28       0.75      0.40      0.52        15\n",
      "          29       0.40      0.24      0.30        17\n",
      "          30       1.00      0.22      0.36         9\n",
      "          31       0.50      0.12      0.20         8\n",
      "          32       0.89      0.35      0.50        23\n",
      "          33       0.83      0.45      0.59        11\n",
      "          34       0.00      0.00      0.00         8\n",
      "          35       0.00      0.00      0.00         9\n",
      "          36       0.89      0.57      0.70        28\n",
      "          37       0.75      0.25      0.38        12\n",
      "          38       0.47      0.17      0.25        46\n",
      "          39       1.00      0.18      0.31        11\n",
      "          40       0.66      0.57      0.61        77\n",
      "          41       1.00      0.14      0.25         7\n",
      "          42       0.48      0.43      0.46        37\n",
      "          43       0.57      0.32      0.41        65\n",
      "          44       0.62      0.28      0.38        18\n",
      "          45       0.69      0.55      0.61        62\n",
      "          46       0.62      0.42      0.50        57\n",
      "          47       0.54      0.27      0.36        51\n",
      "          48       0.55      0.64      0.59        28\n",
      "          49       0.70      0.57      0.63        65\n",
      "          50       0.70      0.35      0.47        20\n",
      "          51       0.33      0.08      0.13        12\n",
      "          52       0.75      0.25      0.38        36\n",
      "          53       1.00      0.50      0.67        16\n",
      "          54       0.50      0.08      0.14        12\n",
      "          55       0.60      0.18      0.27        17\n",
      "          56       1.00      0.11      0.20         9\n",
      "          57       1.00      0.20      0.33         5\n",
      "          58       0.73      0.57      0.64        42\n",
      "          59       0.80      0.57      0.67        14\n",
      "          60       0.47      0.22      0.30        69\n",
      "          61       0.83      0.48      0.61        21\n",
      "          62       0.75      0.25      0.37        61\n",
      "          63       0.51      0.52      0.51        83\n",
      "          64       0.25      0.08      0.12        13\n",
      "          65       1.00      0.14      0.25        14\n",
      "          66       1.00      0.17      0.29         6\n",
      "          67       0.62      0.29      0.39        28\n",
      "          68       0.00      0.00      0.00         6\n",
      "          69       0.66      0.42      0.52        59\n",
      "          70       0.67      0.17      0.27        12\n",
      "          71       0.00      0.00      0.00         8\n",
      "          72       0.00      0.00      0.00         5\n",
      "          73       0.67      0.36      0.47        22\n",
      "          74       1.00      0.14      0.25         7\n",
      "          75       0.00      0.00      0.00         5\n",
      "          76       0.89      0.67      0.76        12\n",
      "          77       1.00      0.08      0.14        13\n",
      "          78       0.48      0.22      0.30        51\n",
      "          79       0.30      0.08      0.13        37\n",
      "          80       0.00      0.00      0.00        11\n",
      "          81       0.00      0.00      0.00         7\n",
      "          82       0.30      0.19      0.23       251\n",
      "          83       0.53      0.26      0.35        34\n",
      "          84       0.71      0.24      0.36        21\n",
      "          85       0.60      0.33      0.43        18\n",
      "          86       0.60      0.11      0.19        27\n",
      "          87       0.82      0.92      0.86       933\n",
      "          88       1.00      0.20      0.33         5\n",
      "          89       0.40      0.25      0.31        16\n",
      "          90       0.88      0.48      0.62        44\n",
      "          91       0.80      0.16      0.27        25\n",
      "          92       0.50      0.25      0.33        20\n",
      "          93       0.50      0.20      0.29        15\n",
      "          94       0.50      0.03      0.06        29\n",
      "          95       1.00      0.25      0.40         8\n",
      "          96       0.43      0.57      0.49        23\n",
      "          97       1.00      0.08      0.14        26\n",
      "          98       0.55      0.46      0.50       355\n",
      "          99       0.40      0.35      0.37        49\n",
      "         100       0.44      0.44      0.44        45\n",
      "         101       0.33      0.06      0.11        16\n",
      "         102       0.50      0.25      0.33         8\n",
      "         103       0.50      0.24      0.32        17\n",
      "         104       0.50      0.11      0.18         9\n",
      "         105       0.59      0.62      0.61        16\n",
      "         106       0.00      0.00      0.00        10\n",
      "         107       0.00      0.00      0.00         4\n",
      "         108       0.25      0.05      0.08        20\n",
      "         109       1.00      0.15      0.26        20\n",
      "         110       0.50      0.10      0.17        10\n",
      "         111       0.00      0.00      0.00         8\n",
      "         112       0.50      0.07      0.12        14\n",
      "         113       0.90      0.47      0.62        19\n",
      "         114       1.00      0.62      0.76        13\n",
      "         115       0.19      0.18      0.18       365\n",
      "         116       0.33      0.11      0.17         9\n",
      "         117       0.30      0.29      0.30       181\n",
      "         118       0.29      0.18      0.22       145\n",
      "         119       0.33      0.14      0.20        28\n",
      "         120       0.34      0.27      0.30       260\n",
      "         121       1.00      0.08      0.15        12\n",
      "         122       0.69      0.61      0.65        36\n",
      "         123       0.17      0.06      0.09        16\n",
      "         124       1.00      0.11      0.20        18\n",
      "         125       0.00      0.00      0.00         8\n",
      "         126       1.00      0.42      0.59        19\n",
      "         127       0.50      0.10      0.17        10\n",
      "         128       0.69      0.58      0.63        19\n",
      "         129       0.75      0.21      0.33        14\n",
      "         130       0.75      0.20      0.32        15\n",
      "         131       0.25      0.10      0.14        10\n",
      "         132       0.00      0.00      0.00         6\n",
      "         133       1.00      0.12      0.22         8\n",
      "         134       0.75      0.79      0.77        19\n",
      "         135       1.00      0.17      0.29         6\n",
      "         136       0.50      0.08      0.14        12\n",
      "         137       0.90      0.75      0.82        12\n",
      "         138       0.00      0.00      0.00         8\n",
      "         139       0.61      0.37      0.46        51\n",
      "         140       0.00      0.00      0.00         6\n",
      "         141       0.62      0.43      0.51       182\n",
      "         142       0.67      0.33      0.44        42\n",
      "         143       0.63      0.70      0.67       743\n",
      "         144       1.00      0.33      0.50         9\n",
      "         145       1.00      0.09      0.17        11\n",
      "         146       1.00      0.10      0.17        21\n",
      "         147       0.00      0.00      0.00         4\n",
      "         148       0.50      0.22      0.31        18\n",
      "         149       0.50      0.09      0.15        11\n",
      "         150       0.45      0.49      0.47       550\n",
      "         151       0.58      0.51      0.54       211\n",
      "         152       0.25      0.12      0.16        17\n",
      "         153       0.72      0.49      0.58        43\n",
      "         154       0.08      0.15      0.11        26\n",
      "         155       1.00      0.20      0.33         5\n",
      "         156       0.75      0.18      0.29        17\n",
      "         157       0.25      0.10      0.14        21\n",
      "         158       0.83      0.30      0.44        33\n",
      "         159       0.20      0.08      0.12        12\n",
      "         160       0.43      0.27      0.33        11\n",
      "         161       0.67      0.24      0.36        41\n",
      "         162       0.53      0.22      0.31       131\n",
      "         163       0.62      0.62      0.62       148\n",
      "         164       0.00      0.00      0.00         5\n",
      "         165       0.17      0.06      0.08        18\n",
      "         166       1.00      0.38      0.56        13\n",
      "         167       0.42      0.38      0.40        13\n",
      "         168       0.50      0.29      0.37        17\n",
      "         169       0.45      0.35      0.39        55\n",
      "         170       0.40      0.33      0.36        12\n",
      "         171       1.00      0.14      0.25         7\n",
      "         172       0.90      0.68      0.78        28\n",
      "         173       0.56      0.25      0.34        20\n",
      "         174       0.60      0.15      0.24        20\n",
      "         175       0.45      0.20      0.28        25\n",
      "         176       0.83      0.40      0.54        25\n",
      "         177       0.55      0.45      0.50        77\n",
      "         178       0.52      0.46      0.49       108\n",
      "         179       0.15      0.19      0.17        21\n",
      "         180       0.35      0.44      0.39       803\n",
      "         181       0.44      0.38      0.41        29\n",
      "         182       0.78      0.64      0.70        11\n",
      "         183       0.44      0.36      0.40       335\n",
      "         184       1.00      0.80      0.89         5\n",
      "         185       0.69      0.30      0.42       104\n",
      "         186       1.00      0.33      0.50        12\n",
      "         187       1.00      0.17      0.29         6\n",
      "         188       0.00      0.00      0.00         4\n",
      "         189       0.73      0.69      0.71        84\n",
      "         190       0.50      0.12      0.20         8\n",
      "         191       0.57      0.63      0.60        67\n",
      "         192       0.71      0.42      0.53        12\n",
      "         193       0.61      0.59      0.60        32\n",
      "         194       0.55      0.50      0.52        22\n",
      "         195       0.79      0.34      0.47        98\n",
      "         196       0.78      0.74      0.76        92\n",
      "         197       0.00      0.00      0.00        12\n",
      "         198       0.50      0.19      0.27        16\n",
      "         199       0.50      0.11      0.18         9\n",
      "         200       1.00      0.23      0.38        13\n",
      "         201       0.00      0.00      0.00         8\n",
      "         202       0.73      0.66      0.69        41\n",
      "         203       0.48      0.39      0.43        28\n",
      "         204       1.00      0.11      0.20         9\n",
      "         205       0.70      0.57      0.63        28\n",
      "         206       0.00      0.00      0.00        31\n",
      "         207       0.33      0.10      0.16        39\n",
      "         208       0.83      0.36      0.50        14\n",
      "         209       0.50      0.24      0.32        21\n",
      "         210       1.00      0.07      0.13        14\n",
      "         211       0.25      0.07      0.11        15\n",
      "         212       0.33      0.12      0.18        32\n",
      "         213       0.00      0.00      0.00         3\n",
      "         214       0.25      0.05      0.08        22\n",
      "         215       0.50      0.08      0.14        12\n",
      "         216       0.50      0.06      0.11        17\n",
      "         217       0.33      0.09      0.14        11\n",
      "         218       0.40      0.44      0.42        73\n",
      "         219       0.42      0.23      0.29        22\n",
      "         220       1.00      0.31      0.47        13\n",
      "         221       0.44      0.21      0.29        19\n",
      "         222       0.38      0.44      0.41        81\n",
      "         223       0.52      0.46      0.49        24\n",
      "         224       0.17      0.06      0.09        16\n",
      "         225       0.00      0.00      0.00         2\n",
      "         226       0.40      0.22      0.29         9\n",
      "         227       0.00      0.00      0.00         6\n",
      "         228       0.69      0.41      0.51        44\n",
      "         229       0.80      0.31      0.44        13\n",
      "         230       0.00      0.00      0.00        11\n",
      "         231       0.58      0.41      0.48        27\n",
      "         232       0.56      0.60      0.58      1130\n",
      "         233       0.33      0.13      0.19        15\n",
      "         234       0.53      0.57      0.55        42\n",
      "         235       0.59      0.28      0.38        47\n",
      "         236       0.53      0.62      0.57        37\n",
      "         237       0.62      0.72      0.67        40\n",
      "         238       0.44      0.19      0.27        42\n",
      "         239       0.50      0.33      0.40        15\n",
      "         240       0.60      0.19      0.29        16\n",
      "         241       0.50      0.38      0.43       107\n",
      "         242       0.76      0.75      0.76        56\n",
      "         243       0.40      0.22      0.29         9\n",
      "         244       1.00      0.20      0.33         5\n",
      "         245       0.40      0.13      0.20        30\n",
      "         246       0.50      0.11      0.18         9\n",
      "         247       0.61      0.61      0.61        23\n",
      "         248       1.00      0.10      0.18        10\n",
      "         249       1.00      0.29      0.44         7\n",
      "         250       0.63      0.60      0.61       300\n",
      "         251       0.48      0.31      0.38       138\n",
      "         252       0.00      0.00      0.00        16\n",
      "         253       1.00      0.22      0.36         9\n",
      "         254       0.50      0.09      0.15        11\n",
      "         255       0.56      0.36      0.43        14\n",
      "         256       0.50      0.14      0.22         7\n",
      "         257       0.60      0.18      0.27        17\n",
      "         258       0.00      0.00      0.00        21\n",
      "         259       0.47      0.28      0.35        25\n",
      "         260       1.00      0.43      0.60        14\n",
      "         261       0.00      0.00      0.00         7\n",
      "         262       0.11      0.04      0.05        28\n",
      "         263       0.79      0.38      0.51        29\n",
      "         264       0.67      0.23      0.34        35\n",
      "         265       0.25      0.06      0.10        16\n",
      "         266       0.25      0.05      0.08        21\n",
      "         267       0.71      0.50      0.59        10\n",
      "         268       0.56      0.40      0.47        68\n",
      "         269       0.00      0.00      0.00         3\n",
      "         270       1.00      0.40      0.57        10\n",
      "         271       0.84      0.77      0.80        88\n",
      "         272       0.00      0.00      0.00         7\n",
      "         273       0.40      0.27      0.32        30\n",
      "         274       1.00      0.12      0.22         8\n",
      "         275       0.49      0.51      0.50        39\n",
      "         276       0.50      0.68      0.58        28\n",
      "         277       0.59      0.31      0.40        62\n",
      "         278       0.00      0.00      0.00        14\n",
      "         279       0.00      0.00      0.00        13\n",
      "         280       0.95      0.72      0.82        25\n",
      "         281       0.50      0.50      0.50        18\n",
      "         282       0.57      0.60      0.59        20\n",
      "         283       0.50      0.30      0.37        20\n",
      "         284       0.75      0.20      0.32        15\n",
      "         285       0.24      0.25      0.24       775\n",
      "         286       0.64      0.70      0.67        20\n",
      "         287       0.00      0.00      0.00         2\n",
      "         288       0.72      0.93      0.81        14\n",
      "         289       0.00      0.00      0.00         7\n",
      "         290       0.81      0.86      0.83        72\n",
      "         291       0.88      0.71      0.79        21\n",
      "         292       0.50      0.43      0.46         7\n",
      "         293       0.50      0.06      0.11        17\n",
      "         294       1.00      0.20      0.33        10\n",
      "         295       0.33      0.31      0.32        42\n",
      "         296       1.00      0.19      0.32        21\n",
      "         297       0.00      0.00      0.00         7\n",
      "         298       0.82      0.79      0.80        70\n",
      "         299       0.87      0.87      0.87        15\n",
      "         300       0.00      0.00      0.00        10\n",
      "         301       1.00      0.92      0.96        63\n",
      "         302       0.88      0.58      0.70        12\n",
      "         303       0.00      0.00      0.00         6\n",
      "         304       0.00      0.00      0.00         6\n",
      "         305       1.00      0.45      0.62        11\n",
      "         306       1.00      0.33      0.50         3\n",
      "         307       0.92      0.52      0.67        44\n",
      "         308       0.50      0.71      0.59         7\n",
      "         309       0.17      0.14      0.15         7\n",
      "\n",
      "   micro avg       0.57      0.49      0.53     17924\n",
      "   macro avg       0.54      0.28      0.34     17924\n",
      "weighted avg       0.57      0.49      0.50     17924\n",
      " samples avg       0.46      0.50      0.47     17924\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test_s , Y_pred_ovr_s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Evaluation du pipeline sur les données de test\n",
    "### Avec le SVM probabilisé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "1D data passed to a transformer that expects 2D data. Try to specify the column selection as a list of one item instead of a scalar.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/DGS-env/lib/python3.7/site-packages/sklearn/compose/_column_transformer.py\u001b[0m in \u001b[0;36m_fit_transform\u001b[0;34m(self, X, y, func, fitted)\u001b[0m\n\u001b[1;32m    466\u001b[0m                 for idx, (name, trans, column, weight) in enumerate(\n\u001b[0;32m--> 467\u001b[0;31m                         self._iter(fitted=fitted, replace_strings=True), 1))\n\u001b[0m\u001b[1;32m    468\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DGS-env/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1032\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1033\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DGS-env/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    846\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 847\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    848\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DGS-env/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    764\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    766\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DGS-env/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DGS-env/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DGS-env/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    252\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 253\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DGS-env/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    252\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 253\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DGS-env/lib/python3.7/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[1;32m    739\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit_transform'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DGS-env/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_keywords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 410\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DGS-env/lib/python3.7/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    692\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DGS-env/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_keywords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle_unknown\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_unknown\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_idx_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_drop_idx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DGS-env/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, handle_unknown)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle_unknown\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mX_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_X\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DGS-env/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py\u001b[0m in \u001b[0;36m_check_X\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;31m# if not a dataframe, do normal check_array validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0mX_temp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m             if (not hasattr(X, 'dtype')\n",
      "\u001b[0;32m~/DGS-env/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DGS-env/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    623\u001b[0m                     \u001b[0;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 624\u001b[0;31m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[1;32m    625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=['Matériovigilance' 'Matériovigilance' 'Matériovigilance' ...\n 'Matériovigilance' 'Matériovigilance' 'Matériovigilance'].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/DGS-env/lib/python3.7/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    328\u001b[0m         \"\"\"\n\u001b[1;32m    329\u001b[0m         \u001b[0mfit_params_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_fit_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m         \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m         with _print_elapsed_time('Pipeline',\n\u001b[1;32m    332\u001b[0m                                  self._log_message(len(self.steps) - 1)):\n",
      "\u001b[0;32m~/DGS-env/lib/python3.7/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[1;32m    294\u001b[0m                 \u001b[0mmessage_clsname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Pipeline'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m                 \u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 **fit_params_steps[name])\n\u001b[0m\u001b[1;32m    297\u001b[0m             \u001b[0;31m# Replace the transformer of the step with the fitted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;31m# transformer. This is necessary when loading the transformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DGS-env/lib/python3.7/site-packages/joblib/memory.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DGS-env/lib/python3.7/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[1;32m    738\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit_transform'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DGS-env/lib/python3.7/site-packages/sklearn/compose/_column_transformer.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_remainder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_fit_transform_one\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DGS-env/lib/python3.7/site-packages/sklearn/compose/_column_transformer.py\u001b[0m in \u001b[0;36m_fit_transform\u001b[0;34m(self, X, y, func, fitted)\u001b[0m\n\u001b[1;32m    468\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m\"Expected 2D array, got 1D array instead\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 470\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ERR_MSG_1DCOLUMN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    471\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: 1D data passed to a transformer that expects 2D data. Try to specify the column selection as a list of one item instead of a scalar."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pipeline.fit(X,y_lb)\n",
    "#y_test_lb = lb.transform(y_test)\n",
    "Y_pred_ovr = pipeline.predict(X_test)\n",
    "f1 = f1_score(y_test_lb , Y_pred_ovr,average='samples')\n",
    "print('f1_score samples : ',f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DYSFONCTIONNEMENT_model_0_55.sav']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "filename = 'Encoder_dysfonctionnement.sav'\n",
    "joblib.dump(le, filename)\n",
    "filename = 'DYSFONCTIONNEMENT_model_0_55.sav'\n",
    "joblib.dump(pipeline, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.83      0.85      5117\n",
      "           1       0.00      0.00      0.00        16\n",
      "           2       0.00      0.00      0.00         7\n",
      "           3       1.00      0.25      0.40         4\n",
      "           4       0.00      0.00      0.00         6\n",
      "           5       0.00      0.00      0.00        14\n",
      "           6       0.00      0.00      0.00         0\n",
      "           7       0.60      0.17      0.26        18\n",
      "           8       0.00      0.00      0.00        84\n",
      "           9       0.00      0.00      0.00        12\n",
      "          10       0.56      0.16      0.24        32\n",
      "          11       0.00      0.00      0.00        15\n",
      "          12       1.00      0.92      0.96        24\n",
      "          13       0.14      0.33      0.19         9\n",
      "          14       0.00      0.00      0.00         7\n",
      "          15       0.69      0.21      0.32        43\n",
      "          16       0.90      0.77      0.83        60\n",
      "          17       0.00      0.00      0.00         6\n",
      "          18       0.00      0.00      0.00         6\n",
      "          19       0.57      0.21      0.31        19\n",
      "          20       0.00      0.00      0.00         3\n",
      "          21       0.00      0.00      0.00        18\n",
      "          22       0.00      0.00      0.00         5\n",
      "          23       0.00      0.00      0.00        11\n",
      "          24       0.53      0.58      0.55       217\n",
      "          25       0.14      0.05      0.07        42\n",
      "          26       0.00      0.00      0.00        10\n",
      "          27       0.84      0.59      0.70        27\n",
      "          28       1.00      0.56      0.71         9\n",
      "          29       0.20      0.20      0.20        10\n",
      "          30       1.00      0.17      0.29         6\n",
      "          31       1.00      0.50      0.67         4\n",
      "          32       0.83      0.50      0.62        10\n",
      "          33       1.00      0.33      0.50         6\n",
      "          34       1.00      0.14      0.25         7\n",
      "          35       0.00      0.00      0.00        22\n",
      "          36       0.75      0.17      0.27        18\n",
      "          37       0.00      0.00      0.00         5\n",
      "          38       0.65      0.50      0.57        26\n",
      "          39       1.00      0.33      0.50        15\n",
      "          40       0.81      0.52      0.64        82\n",
      "          41       0.00      0.00      0.00         3\n",
      "          42       0.70      0.55      0.62        29\n",
      "          43       0.33      0.25      0.29        36\n",
      "          44       0.50      0.14      0.22         7\n",
      "          45       0.83      0.43      0.57        93\n",
      "          46       0.81      0.30      0.43       115\n",
      "          47       0.57      0.53      0.55        80\n",
      "          48       0.75      0.95      0.84        19\n",
      "          49       0.49      0.50      0.49        44\n",
      "          50       0.00      0.00      0.00         5\n",
      "          51       0.00      0.00      0.00         3\n",
      "          52       0.50      0.24      0.32        25\n",
      "          53       1.00      0.09      0.16        58\n",
      "          54       1.00      0.07      0.12        15\n",
      "          55       0.40      0.09      0.14        23\n",
      "          56       0.00      0.00      0.00         0\n",
      "          57       0.00      0.00      0.00         2\n",
      "          58       0.62      0.16      0.26        79\n",
      "          59       1.00      0.60      0.75        15\n",
      "          60       0.20      0.13      0.16        55\n",
      "          61       0.50      0.40      0.44         5\n",
      "          62       0.44      0.13      0.20        31\n",
      "          63       0.87      0.67      0.76       112\n",
      "          64       0.00      0.00      0.00         6\n",
      "          65       1.00      0.42      0.59        19\n",
      "          66       0.00      0.00      0.00         3\n",
      "          67       0.47      0.15      0.22        48\n",
      "          68       0.00      0.00      0.00         6\n",
      "          69       0.53      0.43      0.48        53\n",
      "          70       0.00      0.00      0.00        19\n",
      "          71       0.00      0.00      0.00         4\n",
      "          72       0.00      0.00      0.00        10\n",
      "          73       0.74      0.67      0.70        21\n",
      "          74       0.00      0.00      0.00        27\n",
      "          75       0.00      0.00      0.00        11\n",
      "          76       1.00      0.33      0.50        12\n",
      "          77       0.00      0.00      0.00         5\n",
      "          78       0.81      0.46      0.59        28\n",
      "          79       0.80      0.30      0.43        27\n",
      "          80       1.00      0.22      0.36         9\n",
      "          81       0.00      0.00      0.00         6\n",
      "          82       0.34      0.22      0.27       230\n",
      "          83       0.36      0.42      0.38        12\n",
      "          84       0.50      0.06      0.11        64\n",
      "          85       0.44      0.57      0.50         7\n",
      "          86       1.00      0.24      0.38        21\n",
      "          87       0.81      0.86      0.84       764\n",
      "          88       0.00      0.00      0.00        30\n",
      "          89       1.00      0.71      0.83        14\n",
      "          90       0.82      0.70      0.75        33\n",
      "          91       0.15      0.11      0.13        18\n",
      "          92       0.00      0.00      0.00         8\n",
      "          93       0.50      0.15      0.24        13\n",
      "          94       0.80      0.24      0.37        33\n",
      "          95       1.00      0.57      0.73         7\n",
      "          96       0.14      0.47      0.22        17\n",
      "          97       0.33      0.07      0.12        14\n",
      "          98       0.59      0.50      0.54       297\n",
      "          99       0.34      0.33      0.33        43\n",
      "         100       0.37      0.46      0.41        35\n",
      "         101       0.50      0.27      0.35        15\n",
      "         102       0.31      0.67      0.42         6\n",
      "         103       0.50      0.40      0.44        15\n",
      "         104       0.00      0.00      0.00         4\n",
      "         105       0.90      0.50      0.64        18\n",
      "         106       0.00      0.00      0.00         6\n",
      "         107       0.00      0.00      0.00         6\n",
      "         108       0.00      0.00      0.00        21\n",
      "         109       0.50      0.33      0.40        12\n",
      "         110       0.00      0.00      0.00         4\n",
      "         111       0.00      0.00      0.00         1\n",
      "         112       0.00      0.00      0.00        36\n",
      "         113       0.75      0.75      0.75        12\n",
      "         114       1.00      0.62      0.77         8\n",
      "         115       0.22      0.18      0.19       340\n",
      "         116       0.00      0.00      0.00         6\n",
      "         117       0.24      0.19      0.21       108\n",
      "         118       0.27      0.22      0.24        79\n",
      "         119       0.33      0.09      0.14        22\n",
      "         120       0.36      0.24      0.29       209\n",
      "         121       0.00      0.00      0.00        17\n",
      "         122       0.63      0.71      0.67        56\n",
      "         123       0.67      0.22      0.33         9\n",
      "         124       1.00      0.25      0.40         8\n",
      "         125       0.00      0.00      0.00         2\n",
      "         126       1.00      0.50      0.67        12\n",
      "         127       0.00      0.00      0.00         6\n",
      "         128       1.00      0.34      0.51        29\n",
      "         129       0.92      0.71      0.80        17\n",
      "         130       0.75      0.16      0.26        19\n",
      "         131       0.33      0.17      0.22         6\n",
      "         132       1.00      0.07      0.13        14\n",
      "         133       0.67      1.00      0.80         4\n",
      "         134       0.67      0.33      0.44        24\n",
      "         135       0.00      0.00      0.00         3\n",
      "         136       0.00      0.00      0.00         5\n",
      "         137       1.00      0.29      0.44         7\n",
      "         138       0.00      0.00      0.00         9\n",
      "         139       0.42      0.36      0.39        22\n",
      "         140       0.00      0.00      0.00        10\n",
      "         141       0.64      0.46      0.54       157\n",
      "         142       0.61      0.34      0.44        32\n",
      "         143       0.67      0.71      0.69       596\n",
      "         144       0.00      0.00      0.00         1\n",
      "         145       0.00      0.00      0.00        43\n",
      "         146       1.00      0.19      0.32        31\n",
      "         147       0.00      0.00      0.00         0\n",
      "         148       0.29      0.11      0.16        18\n",
      "         149       1.00      0.13      0.24        15\n",
      "         150       0.49      0.53      0.51       469\n",
      "         151       0.48      0.44      0.46       250\n",
      "         152       0.07      0.20      0.10         5\n",
      "         153       0.88      0.68      0.77        65\n",
      "         154       0.17      0.23      0.19        30\n",
      "         155       0.00      0.00      0.00         0\n",
      "         156       1.00      0.43      0.60        14\n",
      "         157       0.00      0.00      0.00        88\n",
      "         158       0.58      0.16      0.25       114\n",
      "         159       0.00      0.00      0.00         6\n",
      "         160       0.00      0.00      0.00         6\n",
      "         161       0.83      0.29      0.43        34\n",
      "         162       0.70      0.12      0.21       411\n",
      "         163       0.58      0.57      0.57        86\n",
      "         164       1.00      0.50      0.67         4\n",
      "         165       1.00      0.19      0.32        21\n",
      "         166       0.00      0.00      0.00         6\n",
      "         167       0.00      0.00      0.00         2\n",
      "         168       0.85      0.35      0.49        81\n",
      "         169       0.56      0.39      0.46        92\n",
      "         170       1.00      0.17      0.29        36\n",
      "         171       0.00      0.00      0.00         3\n",
      "         172       0.74      0.93      0.82        15\n",
      "         173       0.33      0.21      0.26        14\n",
      "         174       0.00      0.00      0.00        11\n",
      "         175       0.43      0.18      0.25        17\n",
      "         176       1.00      0.65      0.79        17\n",
      "         177       0.55      0.23      0.33        90\n",
      "         178       0.72      0.65      0.68        60\n",
      "         179       0.47      0.27      0.34        26\n",
      "         180       0.32      0.39      0.35       644\n",
      "         181       0.43      0.24      0.31        25\n",
      "         182       0.30      0.60      0.40         5\n",
      "         183       0.35      0.28      0.31       315\n",
      "         184       1.00      1.00      1.00        16\n",
      "         185       0.56      0.25      0.34       113\n",
      "         186       0.00      0.00      0.00         2\n",
      "         187       0.00      0.00      0.00         2\n",
      "         188       0.00      0.00      0.00         3\n",
      "         189       0.81      0.72      0.76       148\n",
      "         190       1.00      0.67      0.80         6\n",
      "         191       0.34      0.61      0.44        28\n",
      "         192       0.86      0.75      0.80        16\n",
      "         193       0.90      0.68      0.78        38\n",
      "         194       0.60      0.21      0.32        28\n",
      "         195       0.85      0.36      0.50        92\n",
      "         196       0.85      0.72      0.78       132\n",
      "         197       0.00      0.00      0.00         4\n",
      "         198       1.00      0.03      0.06        61\n",
      "         199       0.50      0.50      0.50         4\n",
      "         200       0.33      0.33      0.33         3\n",
      "         201       0.00      0.00      0.00         5\n",
      "         202       0.78      0.74      0.76        19\n",
      "         203       0.52      0.42      0.46        38\n",
      "         204       0.33      0.33      0.33         6\n",
      "         205       0.64      0.45      0.53        20\n",
      "         206       0.13      0.11      0.12        19\n",
      "         207       0.35      0.07      0.11        88\n",
      "         208       1.00      0.17      0.29        12\n",
      "         209       1.00      0.37      0.54        27\n",
      "         210       0.00      0.00      0.00        14\n",
      "         211       0.00      0.00      0.00        17\n",
      "         212       0.38      0.04      0.07        74\n",
      "         213       1.00      1.00      1.00         4\n",
      "         214       0.00      0.00      0.00        15\n",
      "         215       0.33      0.11      0.17         9\n",
      "         216       0.00      0.00      0.00        17\n",
      "         217       0.67      0.67      0.67        12\n",
      "         218       0.51      0.34      0.41       134\n",
      "         219       0.57      0.22      0.32        18\n",
      "         220       1.00      0.73      0.84        11\n",
      "         221       0.50      0.46      0.48        13\n",
      "         222       0.52      0.40      0.45       114\n",
      "         223       0.90      0.13      0.23        69\n",
      "         224       0.29      0.15      0.20        26\n",
      "         225       0.00      0.00      0.00         0\n",
      "         226       0.00      0.00      0.00         7\n",
      "         227       0.00      0.00      0.00         2\n",
      "         228       0.86      0.28      0.42        69\n",
      "         229       0.00      0.00      0.00         6\n",
      "         230       0.00      0.00      0.00         7\n",
      "         231       0.57      0.50      0.53        32\n",
      "         232       0.54      0.60      0.57       937\n",
      "         233       0.00      0.00      0.00         6\n",
      "         234       0.90      0.42      0.57        43\n",
      "         235       0.86      0.37      0.51        49\n",
      "         236       0.61      0.70      0.65        20\n",
      "         237       0.68      0.78      0.72        27\n",
      "         238       0.73      0.24      0.36        33\n",
      "         239       0.60      0.40      0.48        15\n",
      "         240       0.00      0.00      0.00         6\n",
      "         241       0.54      0.65      0.59        51\n",
      "         242       0.95      0.95      0.95        40\n",
      "         243       0.83      0.56      0.67         9\n",
      "         244       0.00      0.00      0.00         7\n",
      "         245       0.40      0.11      0.17        36\n",
      "         246       0.00      0.00      0.00        18\n",
      "         247       0.67      0.53      0.59        19\n",
      "         248       0.00      0.00      0.00         2\n",
      "         249       0.00      0.00      0.00         3\n",
      "         250       0.51      0.63      0.57       151\n",
      "         251       0.63      0.28      0.39       165\n",
      "         252       0.33      0.22      0.27         9\n",
      "         253       0.00      0.00      0.00         0\n",
      "         254       0.00      0.00      0.00        16\n",
      "         255       0.67      0.62      0.65        16\n",
      "         256       0.60      0.60      0.60         5\n",
      "         257       1.00      0.10      0.17        21\n",
      "         258       0.00      0.00      0.00        47\n",
      "         259       0.35      0.35      0.35        17\n",
      "         260       1.00      0.30      0.46        10\n",
      "         261       0.00      0.00      0.00         2\n",
      "         262       0.16      0.09      0.11        35\n",
      "         263       1.00      0.21      0.35        14\n",
      "         264       1.00      0.41      0.58        17\n",
      "         265       0.80      0.36      0.50        22\n",
      "         266       0.67      0.24      0.35        34\n",
      "         267       1.00      0.22      0.36         9\n",
      "         268       0.31      0.28      0.29        29\n",
      "         269       0.00      0.00      0.00         4\n",
      "         270       1.00      0.09      0.17        33\n",
      "         271       0.84      0.52      0.64        69\n",
      "         272       0.00      0.00      0.00         6\n",
      "         273       0.50      0.27      0.35        22\n",
      "         274       0.22      0.29      0.25         7\n",
      "         275       1.00      0.29      0.44        28\n",
      "         276       0.87      0.46      0.61       114\n",
      "         277       0.70      0.34      0.46        61\n",
      "         278       0.00      0.00      0.00        42\n",
      "         279       0.00      0.00      0.00         3\n",
      "         280       1.00      0.96      0.98        25\n",
      "         281       0.25      0.04      0.07        23\n",
      "         282       1.00      0.65      0.79        26\n",
      "         283       0.80      0.14      0.24        28\n",
      "         284       0.60      0.30      0.40        20\n",
      "         285       0.21      0.28      0.24       516\n",
      "         286       1.00      0.50      0.67         8\n",
      "         287       0.00      0.00      0.00         2\n",
      "         288       0.60      0.60      0.60         5\n",
      "         289       0.00      0.00      0.00         5\n",
      "         290       0.83      0.82      0.82        65\n",
      "         291       1.00      0.91      0.95        11\n",
      "         292       0.50      1.00      0.67         1\n",
      "         293       0.67      0.22      0.33         9\n",
      "         294       1.00      0.17      0.29         6\n",
      "         295       0.20      0.30      0.24        30\n",
      "         296       0.86      0.33      0.48        18\n",
      "         297       0.00      0.00      0.00         5\n",
      "         298       0.80      0.65      0.72        37\n",
      "         299       1.00      1.00      1.00        10\n",
      "         300       0.00      0.00      0.00         5\n",
      "         301       0.92      0.88      0.90        25\n",
      "         302       0.88      0.79      0.83        19\n",
      "         303       0.00      0.00      0.00         4\n",
      "         304       0.00      0.00      0.00         4\n",
      "         305       0.57      0.36      0.44        11\n",
      "         306       1.00      1.00      1.00        60\n",
      "         307       0.98      0.39      0.56       159\n",
      "         308       0.00      0.00      0.00         5\n",
      "         309       0.17      0.50      0.25         2\n",
      "\n",
      "   micro avg       0.65      0.52      0.58     19115\n",
      "   macro avg       0.46      0.28      0.32     19115\n",
      "weighted avg       0.63      0.52      0.55     19115\n",
      " samples avg       0.56      0.57      0.55     19115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test_lb , Y_pred_ovr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sans le SVM probabilisé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect', preprocess),\n",
    "    ('clf', OneVsRestClassifier(LinearSVC(class_weight='balanced'))),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score samples :  0.5521267539039895\n",
      "CPU times: user 2min 35s, sys: 380 ms, total: 2min 36s\n",
      "Wall time: 2min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pipeline.fit(X,y_lb)\n",
    "Y_pred_ovr = pipeline.predict(X_test)\n",
    "f1 = f1_score(y_test_lb , Y_pred_ovr,average='samples')\n",
    "print('f1_score samples : ',f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score samples :  0.5521267539039895\n"
     ]
    }
   ],
   "source": [
    "print('f1_score samples : ',f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.83      0.85      5117\n",
      "           1       0.00      0.00      0.00        16\n",
      "           2       0.00      0.00      0.00         7\n",
      "           3       1.00      0.25      0.40         4\n",
      "           4       0.00      0.00      0.00         6\n",
      "           5       0.00      0.00      0.00        14\n",
      "           6       0.00      0.00      0.00         0\n",
      "           7       0.60      0.17      0.26        18\n",
      "           8       0.00      0.00      0.00        84\n",
      "           9       0.00      0.00      0.00        12\n",
      "          10       0.56      0.16      0.24        32\n",
      "          11       0.00      0.00      0.00        15\n",
      "          12       1.00      0.92      0.96        24\n",
      "          13       0.14      0.33      0.19         9\n",
      "          14       0.00      0.00      0.00         7\n",
      "          15       0.69      0.21      0.32        43\n",
      "          16       0.90      0.77      0.83        60\n",
      "          17       0.00      0.00      0.00         6\n",
      "          18       0.00      0.00      0.00         6\n",
      "          19       0.50      0.16      0.24        19\n",
      "          20       0.00      0.00      0.00         3\n",
      "          21       0.00      0.00      0.00        18\n",
      "          22       0.00      0.00      0.00         5\n",
      "          23       0.00      0.00      0.00        11\n",
      "          24       0.53      0.58      0.55       217\n",
      "          25       0.14      0.05      0.07        42\n",
      "          26       0.00      0.00      0.00        10\n",
      "          27       0.84      0.59      0.70        27\n",
      "          28       1.00      0.56      0.71         9\n",
      "          29       0.20      0.20      0.20        10\n",
      "          30       1.00      0.17      0.29         6\n",
      "          31       1.00      0.50      0.67         4\n",
      "          32       0.83      0.50      0.62        10\n",
      "          33       1.00      0.33      0.50         6\n",
      "          34       1.00      0.14      0.25         7\n",
      "          35       0.00      0.00      0.00        22\n",
      "          36       0.75      0.17      0.27        18\n",
      "          37       0.00      0.00      0.00         5\n",
      "          38       0.65      0.50      0.57        26\n",
      "          39       1.00      0.33      0.50        15\n",
      "          40       0.81      0.52      0.64        82\n",
      "          41       0.00      0.00      0.00         3\n",
      "          42       0.70      0.55      0.62        29\n",
      "          43       0.33      0.25      0.29        36\n",
      "          44       0.50      0.14      0.22         7\n",
      "          45       0.83      0.43      0.57        93\n",
      "          46       0.81      0.30      0.43       115\n",
      "          47       0.57      0.53      0.55        80\n",
      "          48       0.75      0.95      0.84        19\n",
      "          49       0.49      0.50      0.49        44\n",
      "          50       0.00      0.00      0.00         5\n",
      "          51       0.00      0.00      0.00         3\n",
      "          52       0.50      0.24      0.32        25\n",
      "          53       1.00      0.09      0.16        58\n",
      "          54       1.00      0.07      0.12        15\n",
      "          55       0.40      0.09      0.14        23\n",
      "          56       0.00      0.00      0.00         0\n",
      "          57       0.00      0.00      0.00         2\n",
      "          58       0.62      0.16      0.26        79\n",
      "          59       1.00      0.60      0.75        15\n",
      "          60       0.20      0.13      0.16        55\n",
      "          61       0.50      0.40      0.44         5\n",
      "          62       0.44      0.13      0.20        31\n",
      "          63       0.87      0.67      0.76       112\n",
      "          64       0.00      0.00      0.00         6\n",
      "          65       1.00      0.42      0.59        19\n",
      "          66       0.00      0.00      0.00         3\n",
      "          67       0.47      0.15      0.22        48\n",
      "          68       0.00      0.00      0.00         6\n",
      "          69       0.53      0.43      0.48        53\n",
      "          70       0.00      0.00      0.00        19\n",
      "          71       0.00      0.00      0.00         4\n",
      "          72       0.00      0.00      0.00        10\n",
      "          73       0.74      0.67      0.70        21\n",
      "          74       0.00      0.00      0.00        27\n",
      "          75       0.00      0.00      0.00        11\n",
      "          76       1.00      0.33      0.50        12\n",
      "          77       0.00      0.00      0.00         5\n",
      "          78       0.81      0.46      0.59        28\n",
      "          79       0.80      0.30      0.43        27\n",
      "          80       1.00      0.22      0.36         9\n",
      "          81       0.00      0.00      0.00         6\n",
      "          82       0.34      0.22      0.27       230\n",
      "          83       0.36      0.42      0.38        12\n",
      "          84       0.50      0.06      0.11        64\n",
      "          85       0.44      0.57      0.50         7\n",
      "          86       1.00      0.24      0.38        21\n",
      "          87       0.81      0.86      0.84       764\n",
      "          88       0.00      0.00      0.00        30\n",
      "          89       1.00      0.71      0.83        14\n",
      "          90       0.82      0.70      0.75        33\n",
      "          91       0.15      0.11      0.13        18\n",
      "          92       0.00      0.00      0.00         8\n",
      "          93       0.50      0.15      0.24        13\n",
      "          94       0.80      0.24      0.37        33\n",
      "          95       1.00      0.57      0.73         7\n",
      "          96       0.14      0.47      0.22        17\n",
      "          97       0.33      0.07      0.12        14\n",
      "          98       0.59      0.50      0.54       297\n",
      "          99       0.34      0.33      0.33        43\n",
      "         100       0.37      0.46      0.41        35\n",
      "         101       0.50      0.27      0.35        15\n",
      "         102       0.31      0.67      0.42         6\n",
      "         103       0.50      0.40      0.44        15\n",
      "         104       0.00      0.00      0.00         4\n",
      "         105       0.90      0.50      0.64        18\n",
      "         106       0.00      0.00      0.00         6\n",
      "         107       0.00      0.00      0.00         6\n",
      "         108       0.00      0.00      0.00        21\n",
      "         109       0.50      0.33      0.40        12\n",
      "         110       0.00      0.00      0.00         4\n",
      "         111       0.00      0.00      0.00         1\n",
      "         112       0.00      0.00      0.00        36\n",
      "         113       0.75      0.75      0.75        12\n",
      "         114       1.00      0.62      0.77         8\n",
      "         115       0.22      0.18      0.19       340\n",
      "         116       0.00      0.00      0.00         6\n",
      "         117       0.24      0.19      0.21       108\n",
      "         118       0.27      0.22      0.24        79\n",
      "         119       0.33      0.09      0.14        22\n",
      "         120       0.36      0.24      0.29       209\n",
      "         121       0.00      0.00      0.00        17\n",
      "         122       0.63      0.71      0.67        56\n",
      "         123       0.67      0.22      0.33         9\n",
      "         124       1.00      0.25      0.40         8\n",
      "         125       0.00      0.00      0.00         2\n",
      "         126       1.00      0.50      0.67        12\n",
      "         127       0.00      0.00      0.00         6\n",
      "         128       1.00      0.34      0.51        29\n",
      "         129       0.92      0.71      0.80        17\n",
      "         130       0.75      0.16      0.26        19\n",
      "         131       0.33      0.17      0.22         6\n",
      "         132       1.00      0.07      0.13        14\n",
      "         133       0.67      1.00      0.80         4\n",
      "         134       0.67      0.33      0.44        24\n",
      "         135       0.00      0.00      0.00         3\n",
      "         136       0.00      0.00      0.00         5\n",
      "         137       1.00      0.29      0.44         7\n",
      "         138       0.00      0.00      0.00         9\n",
      "         139       0.42      0.36      0.39        22\n",
      "         140       0.00      0.00      0.00        10\n",
      "         141       0.64      0.46      0.54       157\n",
      "         142       0.61      0.34      0.44        32\n",
      "         143       0.67      0.71      0.69       596\n",
      "         144       0.00      0.00      0.00         1\n",
      "         145       0.00      0.00      0.00        43\n",
      "         146       1.00      0.19      0.32        31\n",
      "         147       0.00      0.00      0.00         0\n",
      "         148       0.29      0.11      0.16        18\n",
      "         149       1.00      0.13      0.24        15\n",
      "         150       0.49      0.53      0.51       469\n",
      "         151       0.48      0.44      0.46       250\n",
      "         152       0.07      0.20      0.10         5\n",
      "         153       0.88      0.68      0.77        65\n",
      "         154       0.17      0.23      0.19        30\n",
      "         155       0.00      0.00      0.00         0\n",
      "         156       1.00      0.43      0.60        14\n",
      "         157       0.00      0.00      0.00        88\n",
      "         158       0.58      0.16      0.25       114\n",
      "         159       0.00      0.00      0.00         6\n",
      "         160       0.00      0.00      0.00         6\n",
      "         161       0.83      0.29      0.43        34\n",
      "         162       0.70      0.12      0.21       411\n",
      "         163       0.58      0.57      0.57        86\n",
      "         164       1.00      0.50      0.67         4\n",
      "         165       1.00      0.19      0.32        21\n",
      "         166       0.00      0.00      0.00         6\n",
      "         167       0.00      0.00      0.00         2\n",
      "         168       0.85      0.35      0.49        81\n",
      "         169       0.56      0.39      0.46        92\n",
      "         170       1.00      0.17      0.29        36\n",
      "         171       0.00      0.00      0.00         3\n",
      "         172       0.74      0.93      0.82        15\n",
      "         173       0.33      0.21      0.26        14\n",
      "         174       0.00      0.00      0.00        11\n",
      "         175       0.43      0.18      0.25        17\n",
      "         176       1.00      0.65      0.79        17\n",
      "         177       0.55      0.23      0.33        90\n",
      "         178       0.72      0.65      0.68        60\n",
      "         179       0.47      0.27      0.34        26\n",
      "         180       0.32      0.39      0.35       644\n",
      "         181       0.43      0.24      0.31        25\n",
      "         182       0.30      0.60      0.40         5\n",
      "         183       0.35      0.28      0.31       315\n",
      "         184       1.00      1.00      1.00        16\n",
      "         185       0.56      0.25      0.34       113\n",
      "         186       0.00      0.00      0.00         2\n",
      "         187       0.00      0.00      0.00         2\n",
      "         188       0.00      0.00      0.00         3\n",
      "         189       0.81      0.72      0.76       148\n",
      "         190       1.00      0.67      0.80         6\n",
      "         191       0.34      0.61      0.44        28\n",
      "         192       0.86      0.75      0.80        16\n",
      "         193       0.90      0.68      0.78        38\n",
      "         194       0.60      0.21      0.32        28\n",
      "         195       0.85      0.36      0.50        92\n",
      "         196       0.85      0.72      0.78       132\n",
      "         197       0.00      0.00      0.00         4\n",
      "         198       1.00      0.03      0.06        61\n",
      "         199       0.50      0.50      0.50         4\n",
      "         200       0.33      0.33      0.33         3\n",
      "         201       0.00      0.00      0.00         5\n",
      "         202       0.78      0.74      0.76        19\n",
      "         203       0.52      0.42      0.46        38\n",
      "         204       0.33      0.33      0.33         6\n",
      "         205       0.64      0.45      0.53        20\n",
      "         206       0.13      0.11      0.12        19\n",
      "         207       0.35      0.07      0.11        88\n",
      "         208       1.00      0.17      0.29        12\n",
      "         209       1.00      0.37      0.54        27\n",
      "         210       0.00      0.00      0.00        14\n",
      "         211       0.00      0.00      0.00        17\n",
      "         212       0.38      0.04      0.07        74\n",
      "         213       1.00      1.00      1.00         4\n",
      "         214       0.00      0.00      0.00        15\n",
      "         215       0.33      0.11      0.17         9\n",
      "         216       0.00      0.00      0.00        17\n",
      "         217       0.67      0.67      0.67        12\n",
      "         218       0.51      0.34      0.41       134\n",
      "         219       0.57      0.22      0.32        18\n",
      "         220       1.00      0.73      0.84        11\n",
      "         221       0.50      0.46      0.48        13\n",
      "         222       0.52      0.40      0.45       114\n",
      "         223       0.90      0.13      0.23        69\n",
      "         224       0.29      0.15      0.20        26\n",
      "         225       0.00      0.00      0.00         0\n",
      "         226       0.00      0.00      0.00         7\n",
      "         227       0.00      0.00      0.00         2\n",
      "         228       0.86      0.28      0.42        69\n",
      "         229       0.00      0.00      0.00         6\n",
      "         230       0.00      0.00      0.00         7\n",
      "         231       0.57      0.50      0.53        32\n",
      "         232       0.54      0.60      0.57       937\n",
      "         233       0.00      0.00      0.00         6\n",
      "         234       0.90      0.42      0.57        43\n",
      "         235       0.86      0.37      0.51        49\n",
      "         236       0.61      0.70      0.65        20\n",
      "         237       0.68      0.78      0.72        27\n",
      "         238       0.73      0.24      0.36        33\n",
      "         239       0.60      0.40      0.48        15\n",
      "         240       0.00      0.00      0.00         6\n",
      "         241       0.54      0.65      0.59        51\n",
      "         242       0.95      0.95      0.95        40\n",
      "         243       0.83      0.56      0.67         9\n",
      "         244       0.00      0.00      0.00         7\n",
      "         245       0.40      0.11      0.17        36\n",
      "         246       0.00      0.00      0.00        18\n",
      "         247       0.67      0.53      0.59        19\n",
      "         248       0.00      0.00      0.00         2\n",
      "         249       0.00      0.00      0.00         3\n",
      "         250       0.51      0.63      0.57       151\n",
      "         251       0.63      0.28      0.39       165\n",
      "         252       0.33      0.22      0.27         9\n",
      "         253       0.00      0.00      0.00         0\n",
      "         254       0.00      0.00      0.00        16\n",
      "         255       0.67      0.62      0.65        16\n",
      "         256       0.60      0.60      0.60         5\n",
      "         257       1.00      0.10      0.17        21\n",
      "         258       0.00      0.00      0.00        47\n",
      "         259       0.35      0.35      0.35        17\n",
      "         260       1.00      0.30      0.46        10\n",
      "         261       0.00      0.00      0.00         2\n",
      "         262       0.16      0.09      0.11        35\n",
      "         263       1.00      0.21      0.35        14\n",
      "         264       1.00      0.41      0.58        17\n",
      "         265       0.80      0.36      0.50        22\n",
      "         266       0.67      0.24      0.35        34\n",
      "         267       1.00      0.22      0.36         9\n",
      "         268       0.31      0.28      0.29        29\n",
      "         269       0.00      0.00      0.00         4\n",
      "         270       1.00      0.09      0.17        33\n",
      "         271       0.84      0.52      0.64        69\n",
      "         272       0.00      0.00      0.00         6\n",
      "         273       0.50      0.27      0.35        22\n",
      "         274       0.22      0.29      0.25         7\n",
      "         275       1.00      0.29      0.44        28\n",
      "         276       0.87      0.46      0.61       114\n",
      "         277       0.70      0.34      0.46        61\n",
      "         278       0.00      0.00      0.00        42\n",
      "         279       0.00      0.00      0.00         3\n",
      "         280       1.00      0.96      0.98        25\n",
      "         281       0.25      0.04      0.07        23\n",
      "         282       1.00      0.65      0.79        26\n",
      "         283       0.80      0.14      0.24        28\n",
      "         284       0.60      0.30      0.40        20\n",
      "         285       0.21      0.28      0.24       516\n",
      "         286       1.00      0.50      0.67         8\n",
      "         287       0.00      0.00      0.00         2\n",
      "         288       0.60      0.60      0.60         5\n",
      "         289       0.00      0.00      0.00         5\n",
      "         290       0.83      0.82      0.82        65\n",
      "         291       1.00      0.91      0.95        11\n",
      "         292       0.50      1.00      0.67         1\n",
      "         293       0.67      0.22      0.33         9\n",
      "         294       1.00      0.17      0.29         6\n",
      "         295       0.20      0.30      0.24        30\n",
      "         296       0.86      0.33      0.48        18\n",
      "         297       0.00      0.00      0.00         5\n",
      "         298       0.80      0.65      0.72        37\n",
      "         299       1.00      1.00      1.00        10\n",
      "         300       0.00      0.00      0.00         5\n",
      "         301       0.92      0.88      0.90        25\n",
      "         302       0.88      0.79      0.83        19\n",
      "         303       0.00      0.00      0.00         4\n",
      "         304       0.00      0.00      0.00         4\n",
      "         305       0.57      0.36      0.44        11\n",
      "         306       1.00      1.00      1.00        60\n",
      "         307       0.98      0.39      0.56       159\n",
      "         308       0.00      0.00      0.00         5\n",
      "         309       0.17      0.50      0.25         2\n",
      "\n",
      "   micro avg       0.65      0.52      0.58     19115\n",
      "   macro avg       0.46      0.28      0.32     19115\n",
      "weighted avg       0.63      0.52      0.55     19115\n",
      " samples avg       0.56      0.57      0.55     19115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test_lb , Y_pred_ovr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commentaire : \n",
    "#### **Les principaux enseignements sont les suivants :**\n",
    "1)  L'approche multilabels est pertinente : Nous obtenons enfin un score \"correct\" 0.55 qui se rapproche de la précision à 4 du model simple Label. Ce qui est assez cohérent car il y a en moyenne 4 classes par evénement. Cela signifie que nous avons simplement tirer partie de la métrique mais que notre modèle n'apprend pas mieux... Dommage ! C'est en fait assez logique car le SVM en multiclasse applique déja une stratégie One vs REST\n",
    "\n",
    "2)  Elle est pertinente.. Mais moins qu'on pourrait le penser avec la shuffle validation : comme le montre la différence entre le Kfold et le ShuffleSplit, les doublons joue un rôle important. Cette différence s'explique apr le fait que les doublons joue un rôle important dans notre base de données car ils ne sont pas exactement des \"doublons\". De plus ils nous permettent, de tenir compte de la proportion des classes dans notre evaluation.\n",
    "\n",
    "3) Cette approche multilabels combinée avec la probalisation du SVM est couteuse en temps de construction de modèle et nous devrons le prendre en compte dans notre approche de finetuning.\n",
    "\n",
    "#### **Les pistes d'améliorations sont :**\n",
    "\n",
    "1) Finetuner le modèle pour vérifier que notre jeu de paramètre n'est pas un cas particulier de performances \n",
    "\n",
    "\n",
    "2) Essayer une approche\n",
    "- MultiOutputClassifier\n",
    "- ClassifierChain https://scikit-learn.org/stable/auto_examples/multioutput/plot_classifier_chain_yeast.html\n",
    "\n",
    "3) Essayer le one-sht learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Optune et la marge de progression du modèle\n",
    "Pour le finetunning, nous enlevons la probabilisation du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect', preprocess),\n",
    "    ('clf', OneVsRestClassifier(LinearSVC(class_weight='balanced'))),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna import Trial\n",
    "\n",
    "def objective(trial):    \n",
    "    \n",
    "    train_index,test_index = next(GroupShuffleSplit(random_state=1029).split(X, groups=X['DESCRIPTION_INCIDENT']))\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    y_train_lb = lb.transform(y_train)\n",
    "    y_test_lb = lb.transform(y_test)\n",
    "    \n",
    "    #Etat Patient\n",
    "    #vect__etat_pat_tfidf__analyzer = trial.suggest_categorical('vect__etat_pat_tfidf__analyzer', ['word', 'char', 'char_wb']) \n",
    "    vect__etat_pat_tfidf__max_features = trial.suggest_int('vect__etat_pat_tfidf__max_features', 500, 20_000)\n",
    "    vect__etat_pat_tfidf__min_df =  trial.suggest_int('vect__etat_pat_tfidf__min_df', 1,5)\n",
    "    vect__etat_pat_tfidf__norm = trial.suggest_categorical('vect__etat_pat_tfidf__norm', ('l1', 'l2'))\n",
    "    #Description\n",
    "    #vect__description_tfidf__analyzer = trial.suggest_categorical('vect__description_tfidf__analyzer', ['word', 'char', 'char_wb']) \n",
    "    vect__description_tfidf__max_features = trial.suggest_int('vect__description_tfidf__max_features', 1500, 60_000)\n",
    "    vect__description_tfidf__min_df =  trial.suggest_int('vect__description_tfidf__min_df', 1,5)\n",
    "    vect__description_tfidf__norm = trial.suggest_categorical('vect__description_tfidf__norm', ('l1', 'l2'))\n",
    "    #Fabricant\n",
    "    vect__fabricant_tfidf__analyzer = trial.suggest_categorical('vect__fabricant_tfidf__analyzer', ['word', 'char', 'char_wb']) \n",
    "    vect__fabricant_tfidf__max_features = trial.suggest_int('vect__fabricant_tfidf__max_features', 500, 10_000)\n",
    "    vect__fabricant_tfidf__min_df =  trial.suggest_int('vect__fabricant_tfidf__min_df', 1,5)\n",
    "    vect__fabricant_tfidf__norm = trial.suggest_categorical('vect__fabricant_tfidf__norm', ('l1', 'l2'))\n",
    "    #action patient\n",
    "    \n",
    "    #Classification\n",
    "    vect__classification_enc__analyzer = trial.suggest_categorical('vect__classification_enc__analyzer', ['word', 'char', 'char_wb']) \n",
    "    vect__classification_enc__max_features = trial.suggest_int('vect__classification_enc__max_features', 500, 5000)\n",
    "    vect__classification_enc__min_df =  trial.suggest_int('vect__classification_enc__min_df', 1,5)\n",
    "    vect__classification_enc__norm = trial.suggest_categorical('vect__classification_enc__norm', ('l1', 'l2'))\n",
    "    \n",
    "    #clf__C =trial.suggest_loguniform('svr_c', 1e-5, 1e5)\n",
    "    \n",
    "\n",
    "    \n",
    "    params = {\n",
    "        #'vect__etat_pat_tfidf__analyzer':vect__etat_pat_tfidf__analyzer,\n",
    "        'vect__etat_pat_tfidf__max_features': vect__etat_pat_tfidf__max_features,\n",
    "        'vect__etat_pat_tfidf__min_df':vect__etat_pat_tfidf__min_df,\n",
    "        'vect__etat_pat_tfidf__norm':vect__etat_pat_tfidf__norm,\n",
    "        \n",
    "        #'vect__description_tfidf__analyzer':vect__description_tfidf__analyzer,\n",
    "        'vect__description_tfidf__max_features': vect__description_tfidf__max_features,\n",
    "        'vect__description_tfidf__min_df':vect__description_tfidf__min_df,\n",
    "        'vect__description_tfidf__norm':vect__description_tfidf__norm,\n",
    "        \n",
    "        'vect__fabricant_tfidf__analyzer':vect__fabricant_tfidf__analyzer,\n",
    "        'vect__fabricant_tfidf__max_features': vect__fabricant_tfidf__max_features,\n",
    "        'vect__fabricant_tfidf__min_df':vect__fabricant_tfidf__min_df,\n",
    "        'vect__fabricant_tfidf__norm':vect__fabricant_tfidf__norm,\n",
    "        \n",
    "        \n",
    "        'vect__classification_enc__analyzer':vect__classification_enc__analyzer,\n",
    "        'vect__classification_enc__max_features': vect__classification_enc__max_features,\n",
    "        'vect__classification_enc__min_df':vect__classification_enc__min_df,\n",
    "        'vect__classification_enc__norm':vect__classification_enc__norm,\n",
    "        \n",
    "        #'clf__C':clf__C\n",
    "    }\n",
    "    \n",
    "    pipeline.set_params(**params)\n",
    "    pipeline.fit(X_train,y_train_lb)\n",
    "    Y_pred_ovr = pipeline.predict(X_test)\n",
    "    score = f1_score(y_test_lb , Y_pred_ovr,average='samples')\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2020-06-12 14:38:20,429]\u001b[0m Finished trial#0 with value: 0.5502198450845048 with parameters: {'vect__etat_pat_tfidf__max_features': 2402, 'vect__etat_pat_tfidf__min_df': 2, 'vect__etat_pat_tfidf__norm': 'l2', 'vect__description_tfidf__max_features': 9720, 'vect__description_tfidf__min_df': 2, 'vect__description_tfidf__norm': 'l2', 'vect__fabricant_tfidf__analyzer': 'word', 'vect__fabricant_tfidf__max_features': 4471, 'vect__fabricant_tfidf__min_df': 1, 'vect__fabricant_tfidf__norm': 'l1', 'vect__classification_enc__analyzer': 'char_wb', 'vect__classification_enc__max_features': 1779, 'vect__classification_enc__min_df': 2, 'vect__classification_enc__norm': 'l2'}. Best is trial#0 with value: 0.5502198450845048.\u001b[0m\n",
      "\u001b[32m[I 2020-06-12 14:49:14,466]\u001b[0m Finished trial#1 with value: 0.5283789553711131 with parameters: {'vect__etat_pat_tfidf__max_features': 18618, 'vect__etat_pat_tfidf__min_df': 1, 'vect__etat_pat_tfidf__norm': 'l1', 'vect__description_tfidf__max_features': 47325, 'vect__description_tfidf__min_df': 4, 'vect__description_tfidf__norm': 'l1', 'vect__fabricant_tfidf__analyzer': 'char_wb', 'vect__fabricant_tfidf__max_features': 6361, 'vect__fabricant_tfidf__min_df': 5, 'vect__fabricant_tfidf__norm': 'l2', 'vect__classification_enc__analyzer': 'char_wb', 'vect__classification_enc__max_features': 4307, 'vect__classification_enc__min_df': 1, 'vect__classification_enc__norm': 'l2'}. Best is trial#0 with value: 0.5502198450845048.\u001b[0m\n",
      "\u001b[32m[I 2020-06-12 14:57:30,013]\u001b[0m Finished trial#2 with value: 0.5204642075373166 with parameters: {'vect__etat_pat_tfidf__max_features': 2517, 'vect__etat_pat_tfidf__min_df': 5, 'vect__etat_pat_tfidf__norm': 'l1', 'vect__description_tfidf__max_features': 31998, 'vect__description_tfidf__min_df': 5, 'vect__description_tfidf__norm': 'l1', 'vect__fabricant_tfidf__analyzer': 'char', 'vect__fabricant_tfidf__max_features': 2721, 'vect__fabricant_tfidf__min_df': 5, 'vect__fabricant_tfidf__norm': 'l2', 'vect__classification_enc__analyzer': 'char', 'vect__classification_enc__max_features': 4433, 'vect__classification_enc__min_df': 2, 'vect__classification_enc__norm': 'l1'}. Best is trial#0 with value: 0.5502198450845048.\u001b[0m\n",
      "\u001b[32m[I 2020-06-12 14:59:49,700]\u001b[0m Finished trial#3 with value: 0.513357011017016 with parameters: {'vect__etat_pat_tfidf__max_features': 18875, 'vect__etat_pat_tfidf__min_df': 3, 'vect__etat_pat_tfidf__norm': 'l2', 'vect__description_tfidf__max_features': 1623, 'vect__description_tfidf__min_df': 1, 'vect__description_tfidf__norm': 'l2', 'vect__fabricant_tfidf__analyzer': 'char', 'vect__fabricant_tfidf__max_features': 5586, 'vect__fabricant_tfidf__min_df': 3, 'vect__fabricant_tfidf__norm': 'l1', 'vect__classification_enc__analyzer': 'word', 'vect__classification_enc__max_features': 1973, 'vect__classification_enc__min_df': 5, 'vect__classification_enc__norm': 'l2'}. Best is trial#0 with value: 0.5502198450845048.\u001b[0m\n",
      "\u001b[32m[I 2020-06-12 15:09:01,475]\u001b[0m Finished trial#4 with value: 0.5180607649745014 with parameters: {'vect__etat_pat_tfidf__max_features': 1531, 'vect__etat_pat_tfidf__min_df': 1, 'vect__etat_pat_tfidf__norm': 'l1', 'vect__description_tfidf__max_features': 20841, 'vect__description_tfidf__min_df': 1, 'vect__description_tfidf__norm': 'l1', 'vect__fabricant_tfidf__analyzer': 'char_wb', 'vect__fabricant_tfidf__max_features': 5303, 'vect__fabricant_tfidf__min_df': 3, 'vect__fabricant_tfidf__norm': 'l2', 'vect__classification_enc__analyzer': 'char_wb', 'vect__classification_enc__max_features': 1389, 'vect__classification_enc__min_df': 2, 'vect__classification_enc__norm': 'l2'}. Best is trial#0 with value: 0.5502198450845048.\u001b[0m\n",
      "\u001b[32m[I 2020-06-12 15:14:29,428]\u001b[0m Finished trial#5 with value: 0.5406124124818787 with parameters: {'vect__etat_pat_tfidf__max_features': 8827, 'vect__etat_pat_tfidf__min_df': 4, 'vect__etat_pat_tfidf__norm': 'l1', 'vect__description_tfidf__max_features': 11007, 'vect__description_tfidf__min_df': 3, 'vect__description_tfidf__norm': 'l1', 'vect__fabricant_tfidf__analyzer': 'word', 'vect__fabricant_tfidf__max_features': 2165, 'vect__fabricant_tfidf__min_df': 2, 'vect__fabricant_tfidf__norm': 'l2', 'vect__classification_enc__analyzer': 'char_wb', 'vect__classification_enc__max_features': 4289, 'vect__classification_enc__min_df': 3, 'vect__classification_enc__norm': 'l1'}. Best is trial#0 with value: 0.5502198450845048.\u001b[0m\n",
      "\u001b[32m[I 2020-06-12 15:17:57,079]\u001b[0m Finished trial#6 with value: 0.5441750087335718 with parameters: {'vect__etat_pat_tfidf__max_features': 14691, 'vect__etat_pat_tfidf__min_df': 1, 'vect__etat_pat_tfidf__norm': 'l2', 'vect__description_tfidf__max_features': 25536, 'vect__description_tfidf__min_df': 4, 'vect__description_tfidf__norm': 'l2', 'vect__fabricant_tfidf__analyzer': 'char', 'vect__fabricant_tfidf__max_features': 4372, 'vect__fabricant_tfidf__min_df': 2, 'vect__fabricant_tfidf__norm': 'l2', 'vect__classification_enc__analyzer': 'char_wb', 'vect__classification_enc__max_features': 1049, 'vect__classification_enc__min_df': 3, 'vect__classification_enc__norm': 'l2'}. Best is trial#0 with value: 0.5502198450845048.\u001b[0m\n",
      "\u001b[32m[I 2020-06-12 15:23:12,528]\u001b[0m Finished trial#7 with value: 0.5219928739407617 with parameters: {'vect__etat_pat_tfidf__max_features': 15486, 'vect__etat_pat_tfidf__min_df': 2, 'vect__etat_pat_tfidf__norm': 'l2', 'vect__description_tfidf__max_features': 49874, 'vect__description_tfidf__min_df': 3, 'vect__description_tfidf__norm': 'l1', 'vect__fabricant_tfidf__analyzer': 'word', 'vect__fabricant_tfidf__max_features': 3539, 'vect__fabricant_tfidf__min_df': 1, 'vect__fabricant_tfidf__norm': 'l2', 'vect__classification_enc__analyzer': 'word', 'vect__classification_enc__max_features': 4214, 'vect__classification_enc__min_df': 3, 'vect__classification_enc__norm': 'l2'}. Best is trial#0 with value: 0.5502198450845048.\u001b[0m\n",
      "\u001b[32m[I 2020-06-12 15:32:11,852]\u001b[0m Finished trial#8 with value: 0.5254190417990063 with parameters: {'vect__etat_pat_tfidf__max_features': 5257, 'vect__etat_pat_tfidf__min_df': 5, 'vect__etat_pat_tfidf__norm': 'l1', 'vect__description_tfidf__max_features': 29847, 'vect__description_tfidf__min_df': 1, 'vect__description_tfidf__norm': 'l1', 'vect__fabricant_tfidf__analyzer': 'char', 'vect__fabricant_tfidf__max_features': 1559, 'vect__fabricant_tfidf__min_df': 2, 'vect__fabricant_tfidf__norm': 'l2', 'vect__classification_enc__analyzer': 'char_wb', 'vect__classification_enc__max_features': 2804, 'vect__classification_enc__min_df': 2, 'vect__classification_enc__norm': 'l2'}. Best is trial#0 with value: 0.5502198450845048.\u001b[0m\n",
      "\u001b[32m[I 2020-06-12 15:34:18,249]\u001b[0m Finished trial#9 with value: 0.5335405297965379 with parameters: {'vect__etat_pat_tfidf__max_features': 6338, 'vect__etat_pat_tfidf__min_df': 3, 'vect__etat_pat_tfidf__norm': 'l2', 'vect__description_tfidf__max_features': 4529, 'vect__description_tfidf__min_df': 1, 'vect__description_tfidf__norm': 'l2', 'vect__fabricant_tfidf__analyzer': 'char', 'vect__fabricant_tfidf__max_features': 9234, 'vect__fabricant_tfidf__min_df': 2, 'vect__fabricant_tfidf__norm': 'l1', 'vect__classification_enc__analyzer': 'char_wb', 'vect__classification_enc__max_features': 3682, 'vect__classification_enc__min_df': 5, 'vect__classification_enc__norm': 'l1'}. Best is trial#0 with value: 0.5502198450845048.\u001b[0m\n",
      "\u001b[32m[I 2020-06-12 15:36:50,930]\u001b[0m Finished trial#10 with value: 0.5577568172053373 with parameters: {'vect__etat_pat_tfidf__max_features': 605, 'vect__etat_pat_tfidf__min_df': 2, 'vect__etat_pat_tfidf__norm': 'l2', 'vect__description_tfidf__max_features': 12411, 'vect__description_tfidf__min_df': 2, 'vect__description_tfidf__norm': 'l2', 'vect__fabricant_tfidf__analyzer': 'word', 'vect__fabricant_tfidf__max_features': 7770, 'vect__fabricant_tfidf__min_df': 1, 'vect__fabricant_tfidf__norm': 'l1', 'vect__classification_enc__analyzer': 'char', 'vect__classification_enc__max_features': 525, 'vect__classification_enc__min_df': 1, 'vect__classification_enc__norm': 'l2'}. Best is trial#10 with value: 0.5577568172053373.\u001b[0m\n",
      "\u001b[32m[I 2020-06-12 15:39:20,953]\u001b[0m Finished trial#11 with value: 0.5558374391159617 with parameters: {'vect__etat_pat_tfidf__max_features': 921, 'vect__etat_pat_tfidf__min_df': 2, 'vect__etat_pat_tfidf__norm': 'l2', 'vect__description_tfidf__max_features': 14654, 'vect__description_tfidf__min_df': 2, 'vect__description_tfidf__norm': 'l2', 'vect__fabricant_tfidf__analyzer': 'word', 'vect__fabricant_tfidf__max_features': 8467, 'vect__fabricant_tfidf__min_df': 1, 'vect__fabricant_tfidf__norm': 'l1', 'vect__classification_enc__analyzer': 'char', 'vect__classification_enc__max_features': 632, 'vect__classification_enc__min_df': 1, 'vect__classification_enc__norm': 'l2'}. Best is trial#10 with value: 0.5577568172053373.\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-f862972c0b03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mstudyName\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'EFFET_ml_optimisation_svm'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"maximize\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#Suvegarde du resultat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DGS-env/lib/python3.7/site-packages/optuna/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    332\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m                 self._optimize_sequential(\n\u001b[0;32m--> 334\u001b[0;31m                     \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgc_after_trial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m                 )\n\u001b[1;32m    336\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DGS-env/lib/python3.7/site-packages/optuna/study.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(self, func, n_trials, timeout, catch, callbacks, gc_after_trial, time_start)\u001b[0m\n\u001b[1;32m    646\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_trial_and_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgc_after_trial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_progress_bar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime_start\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_seconds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DGS-env/lib/python3.7/site-packages/optuna/study.py\u001b[0m in \u001b[0;36m_run_trial_and_callbacks\u001b[0;34m(self, func, catch, callbacks, gc_after_trial)\u001b[0m\n\u001b[1;32m    676\u001b[0m         \u001b[0;31m# type: (...) -> None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0mtrial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgc_after_trial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DGS-env/lib/python3.7/site-packages/optuna/study.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(self, func, catch, gc_after_trial)\u001b[0m\n\u001b[1;32m    697\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             message = \"Setting status of trial#{} as {}. {}\".format(\n",
      "\u001b[0;32m<ipython-input-38-39f0253d94b9>\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train_lb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m     \u001b[0mY_pred_ovr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test_lb\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mY_pred_ovr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'samples'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DGS-env/lib/python3.7/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'passthrough'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m                 \u001b[0mfit_params_last_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_params_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_last_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DGS-env/lib/python3.7/site-packages/sklearn/multiclass.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0;34m\"not %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_binarizer_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 self.label_binarizer_.classes_[i]])\n\u001b[0;32m--> 245\u001b[0;31m             for i, column in enumerate(columns))\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DGS-env/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1030\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1032\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1033\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DGS-env/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    845\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 847\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    848\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DGS-env/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    766\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DGS-env/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DGS-env/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    568\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DGS-env/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 253\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DGS-env/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 253\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DGS-env/lib/python3.7/site-packages/sklearn/multiclass.py\u001b[0m in \u001b[0;36m_fit_binary\u001b[0;34m(estimator, X, y, classes)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mestimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DGS-env/lib/python3.7/site-packages/sklearn/svm/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    235\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulti_class\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m             self.loss, sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulti_class\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"crammer_singer\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DGS-env/lib/python3.7/site-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36m_fit_liblinear\u001b[0;34m(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon, sample_weight)\u001b[0m\n\u001b[1;32m    965\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_ind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misspmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m         \u001b[0mclass_weight_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 967\u001b[0;31m         epsilon, sample_weight)\n\u001b[0m\u001b[1;32m    968\u001b[0m     \u001b[0;31m# Regarding rnd.randint(..) in the above signature:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    969\u001b[0m     \u001b[0;31m# seed for srand in range [0..INT_MAX); due to limitations in Numpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "time_out = 75*100\n",
    "studyName = 'EFFET_ml_optimisation_svm'\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, timeout=time_out)\n",
    "\n",
    "#Suvegarde du resultat\n",
    "df = study.trials_dataframe()\n",
    "df.to_json(studyName+'.json')\n",
    "print(study.best_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score samples :  0.5491033504163741\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Param = {'vect__etat_pat_tfidf__max_features': 19700, 'vect__etat_pat_tfidf__min_df': 4, 'vect__etat_pat_tfidf__norm': 'l2', \n",
    "         'vect__description_tfidf__max_features': 20401, 'vect__description_tfidf__min_df': 2, 'vect__description_tfidf__norm': 'l2', \n",
    "         'vect__fabricant_tfidf__analyzer': 'word', 'vect__fabricant_tfidf__max_features': 9992, 'vect__fabricant_tfidf__min_df': 3, \n",
    "         'vect__fabricant_tfidf__norm': 'l1', 'vect__classification_enc_analyzer': 'word', 'vect__classification_enc__max_features': 2062,\n",
    "         'vect__classification_enc__min_df': 4, 'vect__classification_enc__norm': 'l1'}\n",
    "\n",
    "preprocess2 = ColumnTransformer(\n",
    "    [('description_tfidf',TfidfVectorizer(sublinear_tf=True, min_df=2,\n",
    "                            ngram_range=(1, 1),\n",
    "                            stop_words=STOP_WORDS,\n",
    "                            max_features = 20401,norm = 'l2'), 'DESCRIPTION_INCIDENT'),\n",
    "     \n",
    "     ('etat_pat_tfidf', TfidfVectorizer(sublinear_tf=True, min_df=4,\n",
    "                                        ngram_range=(1, 1),\n",
    "                                        stop_words=STOP_WORDS,\n",
    "                                        max_features = 19700,\n",
    "                                        norm = 'l2'), 'ETAT_PATIENT'),\n",
    "     \n",
    "     ('fabricant_tfidf',TfidfVectorizer(sublinear_tf=True, min_df=3,\n",
    "                            ngram_range=(1, 1),\n",
    "                            stop_words=STOP_WORDS,\n",
    "                            max_features = 9992,norm = 'l1'), 'FABRICANT'),\n",
    "    \n",
    "    ('classification_enc', TfidfVectorizer(sublinear_tf=True, min_df=4,\n",
    "                            ngram_range=(1, 1),\n",
    "                            stop_words=STOP_WORDS,\n",
    "                            max_features = 2062,norm = 'l2'),'CLASSIFICATION')\n",
    "     ],\n",
    "    \n",
    "    remainder='passthrough')\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', preprocess2),\n",
    "    ('clf', OneVsRestClassifier(CalibratedClassifierCV(LinearSVC(class_weight='balanced'),cv=3, method='isotonic'))),\n",
    "])\n",
    "\n",
    "\n",
    "pipeline.fit(X,y_lb)\n",
    "Y_pred_ovr = pipeline.predict(X_test)\n",
    "f1 = f1_score(y_test_lb , Y_pred_ovr,average='samples')\n",
    "print('f1_score samples : ',f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.20      0.26        49\n",
      "           1       0.00      0.00      0.00         4\n",
      "           2       0.00      0.00      0.00        28\n",
      "           3       0.78      0.31      0.44        68\n",
      "           4       0.00      0.00      0.00        34\n",
      "           5       0.45      0.37      0.41        68\n",
      "           6       0.00      0.00      0.00       353\n",
      "           7       1.00      0.76      0.86        21\n",
      "           8       1.00      0.26      0.41        23\n",
      "           9       0.00      0.00      0.00        34\n",
      "          10       0.00      0.00      0.00        25\n",
      "          11       0.00      0.00      0.00        22\n",
      "          12       0.67      0.12      0.21        16\n",
      "          13       0.00      0.00      0.00        11\n",
      "          14       0.00      0.00      0.00        39\n",
      "          15       0.00      0.00      0.00        18\n",
      "          16       0.00      0.00      0.00         1\n",
      "          17       0.00      0.00      0.00        14\n",
      "          18       0.76      0.65      0.70       179\n",
      "          19       0.88      0.14      0.24        50\n",
      "          20       0.00      0.00      0.00         4\n",
      "          21       0.56      0.69      0.62        13\n",
      "          22       0.72      0.54      0.62        61\n",
      "          23       1.00      0.62      0.76        26\n",
      "          24       0.88      0.24      0.38        29\n",
      "          25       1.00      0.13      0.24        15\n",
      "          26       0.84      0.22      0.35        95\n",
      "          27       0.53      0.29      0.37        80\n",
      "          28       0.73      0.62      0.67       106\n",
      "          29       0.90      0.93      0.92       116\n",
      "          30       0.83      0.56      0.66       209\n",
      "          31       0.92      0.40      0.56        57\n",
      "          32       0.00      0.00      0.00         8\n",
      "          33       0.00      0.00      0.00         8\n",
      "          34       0.00      0.00      0.00        35\n",
      "          35       0.71      0.48      0.57       650\n",
      "          36       0.00      0.00      0.00        62\n",
      "          37       0.00      0.00      0.00        34\n",
      "          38       0.00      0.00      0.00        82\n",
      "          39       0.00      0.00      0.00        10\n",
      "          40       0.82      0.68      0.74        34\n",
      "          41       0.00      0.00      0.00         2\n",
      "          42       0.75      0.25      0.37       189\n",
      "          43       0.44      0.29      0.35        14\n",
      "          44       0.00      0.00      0.00        13\n",
      "          45       1.00      0.75      0.86        12\n",
      "          46       0.56      0.06      0.12       155\n",
      "          47       0.00      0.00      0.00        15\n",
      "          48       0.00      0.00      0.00         2\n",
      "          49       0.69      0.53      0.60        17\n",
      "          50       1.00      0.75      0.86        40\n",
      "          51       1.00      1.00      1.00        62\n",
      "          52       0.00      0.00      0.00         2\n",
      "          53       0.66      0.67      0.66        81\n",
      "          54       0.21      0.33      0.26        18\n",
      "          55       0.87      0.59      0.70        56\n",
      "          56       1.00      0.50      0.67         2\n",
      "          57       1.00      0.92      0.96        26\n",
      "          58       0.00      0.00      0.00        24\n",
      "          59       0.78      0.17      0.28       289\n",
      "          60       0.73      0.24      0.36       770\n",
      "          61       1.00      0.02      0.03        61\n",
      "          62       0.00      0.00      0.00        33\n",
      "          63       1.00      0.56      0.71        18\n",
      "          64       1.00      0.37      0.54        46\n",
      "          65       1.00      0.09      0.17        64\n",
      "          66       0.00      0.00      0.00         2\n",
      "          67       1.00      0.41      0.58        27\n",
      "          68       0.67      0.10      0.17        42\n",
      "          69       0.00      0.00      0.00        24\n",
      "          70       0.00      0.00      0.00         4\n",
      "          71       0.00      0.00      0.00       106\n",
      "          72       0.91      0.42      0.58        92\n",
      "          73       0.67      0.06      0.11        33\n",
      "          74       1.00      0.67      0.80         6\n",
      "          75       1.00      0.75      0.86         4\n",
      "          76       0.00      0.00      0.00         3\n",
      "          77       0.00      0.00      0.00         0\n",
      "          78       0.82      0.64      0.72       166\n",
      "          79       0.00      0.00      0.00        20\n",
      "          80       0.54      0.29      0.38        24\n",
      "          81       0.84      0.79      0.81       239\n",
      "          82       0.00      0.00      0.00        17\n",
      "          83       0.00      0.00      0.00         5\n",
      "          84       0.00      0.00      0.00        63\n",
      "          85       0.78      0.35      0.49        82\n",
      "          86       0.74      0.38      0.50       546\n",
      "          87       0.18      0.07      0.10        45\n",
      "          88       0.86      0.79      0.82      2852\n",
      "          89       0.00      0.00      0.00         4\n",
      "          90       0.62      0.71      0.66        55\n",
      "          91       0.67      0.57      0.62        14\n",
      "          92       0.57      0.08      0.15        48\n",
      "          93       0.00      0.00      0.00         2\n",
      "          94       1.00      0.57      0.73        28\n",
      "          95       1.00      0.89      0.94        18\n",
      "          96       0.75      0.06      0.11        99\n",
      "          97       0.00      0.00      0.00         5\n",
      "          98       0.50      0.50      0.50         2\n",
      "          99       0.00      0.00      0.00         3\n",
      "         100       0.00      0.00      0.00        18\n",
      "         101       1.00      0.40      0.57        15\n",
      "         102       0.17      0.05      0.07        86\n",
      "         103       0.97      0.99      0.98      4380\n",
      "         104       0.00      0.00      0.00        18\n",
      "         105       0.65      0.35      0.46        74\n",
      "         106       0.90      0.41      0.57        46\n",
      "         107       0.84      0.50      0.63        32\n",
      "         108       0.00      0.00      0.00         4\n",
      "         109       0.71      0.03      0.05       560\n",
      "         110       0.85      0.34      0.48        98\n",
      "         111       0.00      0.00      0.00         5\n",
      "         112       0.94      0.69      0.80      1359\n",
      "         113       0.76      0.81      0.78      2635\n",
      "         114       0.88      0.97      0.92      3918\n",
      "         115       0.90      0.77      0.83      2084\n",
      "         116       0.86      0.76      0.81      2302\n",
      "         117       0.74      0.48      0.58      2488\n",
      "         118       0.98      0.24      0.38      1008\n",
      "         119       0.76      0.31      0.44      2250\n",
      "         120       0.83      0.95      0.89      3494\n",
      "         121       0.78      0.04      0.07      1199\n",
      "         122       0.00      0.00      0.00       177\n",
      "         123       0.73      0.78      0.76      2437\n",
      "         124       0.76      0.18      0.28       177\n",
      "         125       0.80      0.45      0.58       238\n",
      "         126       1.00      0.02      0.05       168\n",
      "         127       1.00      0.11      0.19       294\n",
      "         128       0.00      0.00      0.00        93\n",
      "         129       0.71      0.41      0.52       920\n",
      "         130       0.83      0.35      0.49      1064\n",
      "         131       0.54      0.11      0.18       628\n",
      "         132       0.36      0.04      0.07       358\n",
      "         133       1.00      0.16      0.27        57\n",
      "         134       0.95      0.87      0.91       916\n",
      "         135       0.00      0.00      0.00        39\n",
      "         136       0.96      0.81      0.88        57\n",
      "         137       0.00      0.00      0.00         6\n",
      "         138       0.00      0.00      0.00        61\n",
      "         139       0.59      0.12      0.20        82\n",
      "         140       0.80      0.51      0.62       818\n",
      "         141       0.88      0.44      0.59       212\n",
      "         142       1.00      0.43      0.60         7\n",
      "         143       0.76      0.12      0.21       233\n",
      "         144       0.64      0.26      0.37        27\n",
      "         145       1.00      0.25      0.40         4\n",
      "         146       0.78      0.42      0.55        93\n",
      "         147       0.93      0.30      0.45        44\n",
      "         148       0.59      0.36      0.45        47\n",
      "         149       0.85      0.46      0.60        37\n",
      "         150       1.00      0.08      0.14        13\n",
      "         151       0.00      0.00      0.00         3\n",
      "         152       0.00      0.00      0.00         6\n",
      "         153       0.00      0.00      0.00         0\n",
      "         154       0.00      0.00      0.00         0\n",
      "         155       0.00      0.00      0.00         3\n",
      "         156       0.00      0.00      0.00         0\n",
      "         157       0.59      0.18      0.27        57\n",
      "         158       0.43      0.38      0.40         8\n",
      "         159       0.00      0.00      0.00         9\n",
      "         160       0.00      0.00      0.00        10\n",
      "         161       0.00      0.00      0.00         0\n",
      "         162       0.00      0.00      0.00         4\n",
      "         163       0.00      0.00      0.00         9\n",
      "         164       1.00      0.50      0.67         4\n",
      "         165       0.00      0.00      0.00        13\n",
      "         166       1.00      0.25      0.40         8\n",
      "         167       1.00      0.14      0.25        21\n",
      "         168       0.00      0.00      0.00         4\n",
      "         169       0.00      0.00      0.00        19\n",
      "         170       0.00      0.00      0.00        23\n",
      "         171       0.00      0.00      0.00        32\n",
      "\n",
      "   micro avg       0.84      0.62      0.71     47095\n",
      "   macro avg       0.49      0.26      0.31     47095\n",
      "weighted avg       0.79      0.62      0.66     47095\n",
      " samples avg       0.63      0.53      0.55     47095\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test_lb , Y_pred_ovr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commantaire\n",
    " Notre première approximation des performances était la bonne. En effet, le finetunning des paramètres des TFIDF n'a pas permis d'augmenter significativmement les résultats\n",
    " \n",
    "## 3.0 L'approche Multioutput\n",
    "\n",
    "> Multioutput classification support can be added to any classifier with MultiOutputClassifier. This strategy consists of fitting one classifier per target. This allows multiple target variable classifications. The purpose of this class is to extend estimators to be able to estimate a series of target functions (f1,f2,f3…,fn) that are trained on a single X predictor matrix to predict a series of responses (y1,y2,y3…,yn).\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score samples :  0.5589696469390759\n"
     ]
    }
   ],
   "source": [
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', preprocess),\n",
    "    ('clf', MultiOutputClassifier(LinearSVC(class_weight='balanced'))),\n",
    "])\n",
    "#### prédiction \n",
    "pipeline.fit(X,y_lb)\n",
    "Y_pred_ovr = pipeline.predict(X_test)\n",
    "f1 = f1_score(y_test_lb , Y_pred_ovr,average='samples')\n",
    "print('f1_score samples : ',f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.83      0.85      5117\n",
      "           1       0.00      0.00      0.00        16\n",
      "           2       0.00      0.00      0.00         7\n",
      "           3       0.50      0.25      0.33         4\n",
      "           4       0.00      0.00      0.00         6\n",
      "           5       0.00      0.00      0.00        14\n",
      "           6       0.00      0.00      0.00         0\n",
      "           7       0.67      0.22      0.33        18\n",
      "           8       0.40      0.10      0.15        84\n",
      "           9       0.00      0.00      0.00        12\n",
      "          10       0.60      0.19      0.29        32\n",
      "          11       0.00      0.00      0.00        15\n",
      "          12       1.00      0.75      0.86        24\n",
      "          13       0.11      0.22      0.14         9\n",
      "          14       0.00      0.00      0.00         7\n",
      "          15       0.52      0.30      0.38        43\n",
      "          16       0.90      0.73      0.81        60\n",
      "          17       0.00      0.00      0.00         6\n",
      "          18       0.00      0.00      0.00         6\n",
      "          19       0.60      0.32      0.41        19\n",
      "          20       0.00      0.00      0.00         3\n",
      "          21       0.00      0.00      0.00        18\n",
      "          22       0.00      0.00      0.00         5\n",
      "          23       0.00      0.00      0.00        11\n",
      "          24       0.57      0.61      0.59       217\n",
      "          25       0.25      0.10      0.14        42\n",
      "          26       0.00      0.00      0.00        10\n",
      "          27       0.71      0.56      0.63        27\n",
      "          28       0.83      0.56      0.67         9\n",
      "          29       0.18      0.20      0.19        10\n",
      "          30       0.00      0.00      0.00         6\n",
      "          31       0.67      0.50      0.57         4\n",
      "          32       0.00      0.00      0.00        10\n",
      "          33       0.75      1.00      0.86         6\n",
      "          34       1.00      0.14      0.25         7\n",
      "          35       1.00      0.18      0.31        22\n",
      "          36       0.75      0.17      0.27        18\n",
      "          37       0.00      0.00      0.00         5\n",
      "          38       0.93      0.54      0.68        26\n",
      "          39       1.00      0.13      0.24        15\n",
      "          40       0.81      0.52      0.64        82\n",
      "          41       0.00      0.00      0.00         3\n",
      "          42       0.70      0.72      0.71        29\n",
      "          43       0.35      0.22      0.27        36\n",
      "          44       1.00      0.14      0.25         7\n",
      "          45       0.79      0.49      0.61        93\n",
      "          46       0.75      0.31      0.44       115\n",
      "          47       0.49      0.53      0.51        80\n",
      "          48       0.70      1.00      0.83        19\n",
      "          49       0.45      0.57      0.50        44\n",
      "          50       0.00      0.00      0.00         5\n",
      "          51       0.00      0.00      0.00         3\n",
      "          52       0.74      0.68      0.71        25\n",
      "          53       1.00      0.10      0.19        58\n",
      "          54       0.00      0.00      0.00        15\n",
      "          55       0.50      0.09      0.15        23\n",
      "          56       0.00      0.00      0.00         0\n",
      "          57       1.00      0.50      0.67         2\n",
      "          58       0.68      0.22      0.33        79\n",
      "          59       1.00      0.60      0.75        15\n",
      "          60       0.41      0.24      0.30        55\n",
      "          61       0.40      0.40      0.40         5\n",
      "          62       0.50      0.13      0.21        31\n",
      "          63       0.84      0.64      0.73       112\n",
      "          64       0.40      0.33      0.36         6\n",
      "          65       1.00      0.58      0.73        19\n",
      "          66       0.00      0.00      0.00         3\n",
      "          67       0.44      0.15      0.22        48\n",
      "          68       0.00      0.00      0.00         6\n",
      "          69       0.47      0.36      0.41        53\n",
      "          70       0.00      0.00      0.00        19\n",
      "          71       0.00      0.00      0.00         4\n",
      "          72       0.00      0.00      0.00        10\n",
      "          73       0.78      0.67      0.72        21\n",
      "          74       0.00      0.00      0.00        27\n",
      "          75       0.00      0.00      0.00        11\n",
      "          76       1.00      0.33      0.50        12\n",
      "          77       0.00      0.00      0.00         5\n",
      "          78       0.54      0.46      0.50        28\n",
      "          79       0.67      0.30      0.41        27\n",
      "          80       0.00      0.00      0.00         9\n",
      "          81       0.00      0.00      0.00         6\n",
      "          82       0.34      0.23      0.28       230\n",
      "          83       0.26      0.42      0.32        12\n",
      "          84       0.33      0.03      0.06        64\n",
      "          85       0.57      0.57      0.57         7\n",
      "          86       0.67      0.29      0.40        21\n",
      "          87       0.80      0.86      0.83       764\n",
      "          88       0.00      0.00      0.00        30\n",
      "          89       0.91      0.71      0.80        14\n",
      "          90       0.83      0.73      0.77        33\n",
      "          91       0.25      0.11      0.15        18\n",
      "          92       1.00      0.12      0.22         8\n",
      "          93       0.50      0.15      0.24        13\n",
      "          94       0.80      0.24      0.37        33\n",
      "          95       1.00      0.14      0.25         7\n",
      "          96       0.11      0.35      0.16        17\n",
      "          97       0.33      0.07      0.12        14\n",
      "          98       0.54      0.46      0.50       297\n",
      "          99       0.24      0.33      0.27        43\n",
      "         100       0.43      0.46      0.44        35\n",
      "         101       0.67      0.13      0.22        15\n",
      "         102       0.04      0.33      0.07         6\n",
      "         103       0.69      0.60      0.64        15\n",
      "         104       0.00      0.00      0.00         4\n",
      "         105       0.79      0.61      0.69        18\n",
      "         106       0.00      0.00      0.00         6\n",
      "         107       0.00      0.00      0.00         6\n",
      "         108       0.00      0.00      0.00        21\n",
      "         109       0.00      0.00      0.00        12\n",
      "         110       0.00      0.00      0.00         4\n",
      "         111       0.00      0.00      0.00         1\n",
      "         112       0.00      0.00      0.00        36\n",
      "         113       1.00      0.75      0.86        12\n",
      "         114       0.83      0.62      0.71         8\n",
      "         115       0.23      0.20      0.21       340\n",
      "         116       0.00      0.00      0.00         6\n",
      "         117       0.32      0.28      0.30       108\n",
      "         118       0.27      0.18      0.22        79\n",
      "         119       0.33      0.09      0.14        22\n",
      "         120       0.32      0.22      0.26       209\n",
      "         121       0.00      0.00      0.00        17\n",
      "         122       0.63      0.71      0.67        56\n",
      "         123       0.00      0.00      0.00         9\n",
      "         124       1.00      0.25      0.40         8\n",
      "         125       0.00      0.00      0.00         2\n",
      "         126       1.00      0.58      0.74        12\n",
      "         127       0.00      0.00      0.00         6\n",
      "         128       1.00      0.34      0.51        29\n",
      "         129       0.50      0.24      0.32        17\n",
      "         130       0.75      0.16      0.26        19\n",
      "         131       0.33      0.17      0.22         6\n",
      "         132       1.00      0.07      0.13        14\n",
      "         133       1.00      1.00      1.00         4\n",
      "         134       0.56      0.42      0.48        24\n",
      "         135       1.00      0.67      0.80         3\n",
      "         136       1.00      0.20      0.33         5\n",
      "         137       1.00      0.29      0.44         7\n",
      "         138       0.00      0.00      0.00         9\n",
      "         139       0.43      0.27      0.33        22\n",
      "         140       0.00      0.00      0.00        10\n",
      "         141       0.70      0.52      0.60       157\n",
      "         142       0.68      0.47      0.56        32\n",
      "         143       0.62      0.73      0.67       596\n",
      "         144       0.00      0.00      0.00         1\n",
      "         145       0.00      0.00      0.00        43\n",
      "         146       0.80      0.26      0.39        31\n",
      "         147       0.00      0.00      0.00         0\n",
      "         148       0.25      0.11      0.15        18\n",
      "         149       1.00      0.13      0.24        15\n",
      "         150       0.49      0.60      0.54       469\n",
      "         151       0.46      0.48      0.47       250\n",
      "         152       0.25      0.20      0.22         5\n",
      "         153       0.90      0.71      0.79        65\n",
      "         154       0.16      0.17      0.16        30\n",
      "         155       0.00      0.00      0.00         0\n",
      "         156       1.00      0.43      0.60        14\n",
      "         157       1.00      0.07      0.13        88\n",
      "         158       0.45      0.12      0.19       114\n",
      "         159       0.00      0.00      0.00         6\n",
      "         160       0.00      0.00      0.00         6\n",
      "         161       0.86      0.35      0.50        34\n",
      "         162       0.46      0.14      0.22       411\n",
      "         163       0.55      0.55      0.55        86\n",
      "         164       1.00      0.50      0.67         4\n",
      "         165       0.44      0.19      0.27        21\n",
      "         166       0.00      0.00      0.00         6\n",
      "         167       0.00      0.00      0.00         2\n",
      "         168       0.94      0.79      0.86        81\n",
      "         169       0.48      0.33      0.39        92\n",
      "         170       0.55      0.17      0.26        36\n",
      "         171       0.00      0.00      0.00         3\n",
      "         172       0.71      1.00      0.83        15\n",
      "         173       0.33      0.36      0.34        14\n",
      "         174       0.00      0.00      0.00        11\n",
      "         175       0.43      0.18      0.25        17\n",
      "         176       1.00      0.88      0.94        17\n",
      "         177       0.68      0.23      0.35        90\n",
      "         178       0.59      0.60      0.60        60\n",
      "         179       0.15      0.12      0.13        26\n",
      "         180       0.28      0.37      0.32       644\n",
      "         181       0.38      0.24      0.29        25\n",
      "         182       0.42      1.00      0.59         5\n",
      "         183       0.31      0.28      0.29       315\n",
      "         184       1.00      1.00      1.00        16\n",
      "         185       0.57      0.27      0.36       113\n",
      "         186       0.00      0.00      0.00         2\n",
      "         187       0.00      0.00      0.00         2\n",
      "         188       0.00      0.00      0.00         3\n",
      "         189       0.65      0.68      0.67       148\n",
      "         190       1.00      0.83      0.91         6\n",
      "         191       0.41      0.61      0.49        28\n",
      "         192       0.86      0.75      0.80        16\n",
      "         193       0.89      0.63      0.74        38\n",
      "         194       0.75      0.21      0.33        28\n",
      "         195       0.86      0.40      0.55        92\n",
      "         196       0.86      0.73      0.79       132\n",
      "         197       0.00      0.00      0.00         4\n",
      "         198       1.00      0.03      0.06        61\n",
      "         199       0.50      0.50      0.50         4\n",
      "         200       0.33      0.67      0.44         3\n",
      "         201       0.60      0.60      0.60         5\n",
      "         202       0.80      0.63      0.71        19\n",
      "         203       0.55      0.47      0.51        38\n",
      "         204       0.33      0.33      0.33         6\n",
      "         205       0.71      0.60      0.65        20\n",
      "         206       0.14      0.11      0.12        19\n",
      "         207       0.46      0.07      0.12        88\n",
      "         208       1.00      0.17      0.29        12\n",
      "         209       0.91      0.37      0.53        27\n",
      "         210       0.40      0.14      0.21        14\n",
      "         211       0.00      0.00      0.00        17\n",
      "         212       0.50      0.03      0.05        74\n",
      "         213       1.00      1.00      1.00         4\n",
      "         214       0.00      0.00      0.00        15\n",
      "         215       0.33      0.11      0.17         9\n",
      "         216       0.00      0.00      0.00        17\n",
      "         217       1.00      0.67      0.80        12\n",
      "         218       0.51      0.34      0.41       134\n",
      "         219       0.44      0.22      0.30        18\n",
      "         220       1.00      0.73      0.84        11\n",
      "         221       0.38      0.46      0.41        13\n",
      "         222       0.49      0.54      0.52       114\n",
      "         223       1.00      0.14      0.25        69\n",
      "         224       0.25      0.15      0.19        26\n",
      "         225       0.00      0.00      0.00         0\n",
      "         226       0.00      0.00      0.00         7\n",
      "         227       0.00      0.00      0.00         2\n",
      "         228       0.79      0.28      0.41        69\n",
      "         229       0.00      0.00      0.00         6\n",
      "         230       0.00      0.00      0.00         7\n",
      "         231       0.50      0.44      0.47        32\n",
      "         232       0.53      0.64      0.58       937\n",
      "         233       0.00      0.00      0.00         6\n",
      "         234       0.81      0.40      0.53        43\n",
      "         235       0.84      0.33      0.47        49\n",
      "         236       0.41      0.55      0.47        20\n",
      "         237       0.73      0.81      0.77        27\n",
      "         238       0.89      0.24      0.38        33\n",
      "         239       0.70      0.47      0.56        15\n",
      "         240       0.00      0.00      0.00         6\n",
      "         241       0.52      0.65      0.57        51\n",
      "         242       0.88      0.72      0.79        40\n",
      "         243       1.00      0.56      0.71         9\n",
      "         244       0.00      0.00      0.00         7\n",
      "         245       0.05      0.06      0.05        36\n",
      "         246       0.00      0.00      0.00        18\n",
      "         247       0.83      0.53      0.65        19\n",
      "         248       0.00      0.00      0.00         2\n",
      "         249       0.00      0.00      0.00         3\n",
      "         250       0.55      0.60      0.58       151\n",
      "         251       0.55      0.38      0.45       165\n",
      "         252       0.43      0.33      0.38         9\n",
      "         253       0.00      0.00      0.00         0\n",
      "         254       0.00      0.00      0.00        16\n",
      "         255       0.67      0.62      0.65        16\n",
      "         256       0.00      0.00      0.00         5\n",
      "         257       0.20      0.14      0.17        21\n",
      "         258       0.00      0.00      0.00        47\n",
      "         259       0.40      0.47      0.43        17\n",
      "         260       1.00      0.30      0.46        10\n",
      "         261       0.00      0.00      0.00         2\n",
      "         262       0.42      0.14      0.21        35\n",
      "         263       0.75      0.21      0.33        14\n",
      "         264       0.50      0.18      0.26        17\n",
      "         265       0.00      0.00      0.00        22\n",
      "         266       0.57      0.24      0.33        34\n",
      "         267       0.67      0.22      0.33         9\n",
      "         268       0.28      0.28      0.28        29\n",
      "         269       0.00      0.00      0.00         4\n",
      "         270       1.00      0.09      0.17        33\n",
      "         271       0.80      0.52      0.63        69\n",
      "         272       0.00      0.00      0.00         6\n",
      "         273       0.60      0.41      0.49        22\n",
      "         274       0.22      0.29      0.25         7\n",
      "         275       0.92      0.43      0.59        28\n",
      "         276       0.87      0.48      0.62       114\n",
      "         277       0.61      0.31      0.41        61\n",
      "         278       0.00      0.00      0.00        42\n",
      "         279       0.00      0.00      0.00         3\n",
      "         280       1.00      0.96      0.98        25\n",
      "         281       0.25      0.04      0.07        23\n",
      "         282       0.94      0.65      0.77        26\n",
      "         283       1.00      0.14      0.25        28\n",
      "         284       0.47      0.40      0.43        20\n",
      "         285       0.21      0.30      0.25       516\n",
      "         286       1.00      0.62      0.77         8\n",
      "         287       0.00      0.00      0.00         2\n",
      "         288       0.60      0.60      0.60         5\n",
      "         289       0.00      0.00      0.00         5\n",
      "         290       0.82      0.85      0.83        65\n",
      "         291       1.00      0.91      0.95        11\n",
      "         292       0.50      1.00      0.67         1\n",
      "         293       0.67      0.22      0.33         9\n",
      "         294       0.50      0.17      0.25         6\n",
      "         295       0.15      0.27      0.19        30\n",
      "         296       0.86      0.33      0.48        18\n",
      "         297       0.00      0.00      0.00         5\n",
      "         298       0.86      0.65      0.74        37\n",
      "         299       1.00      1.00      1.00        10\n",
      "         300       0.00      0.00      0.00         5\n",
      "         301       0.92      0.88      0.90        25\n",
      "         302       0.85      0.58      0.69        19\n",
      "         303       0.00      0.00      0.00         4\n",
      "         304       0.00      0.00      0.00         4\n",
      "         305       0.67      0.36      0.47        11\n",
      "         306       1.00      1.00      1.00        60\n",
      "         307       0.99      0.42      0.58       159\n",
      "         308       0.00      0.00      0.00         5\n",
      "         309       0.17      0.50      0.25         2\n",
      "\n",
      "   micro avg       0.63      0.53      0.58     19115\n",
      "   macro avg       0.45      0.29      0.32     19115\n",
      "weighted avg       0.63      0.53      0.55     19115\n",
      " samples avg       0.56      0.59      0.56     19115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test_lb , Y_pred_ovr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commentaire\n",
    "Comme attendu, nous n'observons pas de grande différence car les deux approches sont très similaires\n",
    "\n",
    "## 3.1 Approche One vs One\n",
    "\n",
    ">This strategy consists in fitting one classifier per class pair. At prediction time, the class which received the most votes is selected. Since it requires to fit n_classes * (n_classes - 1) / 2 classifiers, this method is usually slower than one-vs-the-rest, due to its O(n_classes^2) complexity. However, this method may be advantageous for algorithms such as kernel algorithms which don’t scale well with n_samples. This is because each individual learning problem only involves a small subset of the data whereas, with one-vs-the-rest, the complete dataset is used n_classes times.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score samples :  0.5589696469390759\n",
      "CPU times: user 3min 28s, sys: 8.17 s, total: 3min 37s\n",
      "Wall time: 3min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "pipeline = Pipeline([\n",
    "    ('vect', preprocess),\n",
    "    ('clf', MultiOutputClassifier(OneVsOneClassifier(LinearSVC(class_weight='balanced')))),\n",
    "])\n",
    "#### prédiction \n",
    "pipeline.fit(X,y_lb)\n",
    "Y_pred_ovr = pipeline.predict(X_test)\n",
    "f1 = f1_score(y_test_lb , Y_pred_ovr,average='samples')\n",
    "print('f1_score samples : ',f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commentaire\n",
    "Nous n'oservons pas de changement de performances, seulement une hausse du temps de calcul\n",
    "## 3.2 l'approche ClassifierChain\n",
    ">A multi-label model that arranges binary classifiers into a chain.\n",
    "Each model makes a prediction in the order specified by the chain using all of the available features provided to the model plus the predictions of models that are earlier in the chain.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multioutput import ClassifierChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index_,test_index_ = next(GroupShuffleSplit(random_state=1029).split(X, groups=X['DESCRIPTION_INCIDENT']))\n",
    "X_train_, X_test_ = X.iloc[train_index_], X.iloc[test_index_]\n",
    "y_train_, y_test_ = y.iloc[train_index_], y.iloc[test_index_]\n",
    "\n",
    "X_train_, X_test_ =preprocess.fit_transform(X_train_),preprocess.transform(X_test_)\n",
    "y_train_lb_ = lb.transform(y_train_)\n",
    "y_test_lb_ = lb.transform(y_test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5904407086021436\n",
      "CPU times: user 34min 53s, sys: 2min 34s, total: 37min 27s\n",
      "Wall time: 37min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train, X_test_ =preprocess.fit_transform(X),preprocess.transform(X_test)\n",
    "clf = LinearSVC(class_weight='balanced')\n",
    "\n",
    "\n",
    "chains = [ClassifierChain(clf, order='random', random_state=i) for i in range(10)]\n",
    "\n",
    "for chain in chains:\n",
    "    chain.fit(X_train, y_lb)\n",
    "    \n",
    "y_pred_chains = np.array([chain.predict(X_test_) for chain in chains])\n",
    "\n",
    "chain_f1_scores = [f1_score(y_test_lb, y_pred_chain, average='samples') for y_pred_chain in y_pred_chains]\n",
    "\n",
    "y_pred_ensemble = y_pred_chains.mean(axis=0)\n",
    "\n",
    "y_e = y_pred_ensemble>=0.4\n",
    "\n",
    "ensemble_f1_score = f1_score(y_test_lb,y_e, average='samples')\n",
    "\n",
    "print(ensemble_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.83      0.85      5117\n",
      "           1       0.00      0.00      0.00        16\n",
      "           2       0.00      0.00      0.00         7\n",
      "           3       0.50      0.25      0.33         4\n",
      "           4       0.00      0.00      0.00         6\n",
      "           5       0.00      0.00      0.00        14\n",
      "           6       0.00      0.00      0.00         0\n",
      "           7       0.67      0.22      0.33        18\n",
      "           8       0.40      0.10      0.15        84\n",
      "           9       0.00      0.00      0.00        12\n",
      "          10       0.60      0.19      0.29        32\n",
      "          11       0.00      0.00      0.00        15\n",
      "          12       1.00      0.75      0.86        24\n",
      "          13       0.11      0.22      0.14         9\n",
      "          14       0.00      0.00      0.00         7\n",
      "          15       0.52      0.30      0.38        43\n",
      "          16       0.90      0.73      0.81        60\n",
      "          17       0.00      0.00      0.00         6\n",
      "          18       0.00      0.00      0.00         6\n",
      "          19       0.60      0.32      0.41        19\n",
      "          20       0.00      0.00      0.00         3\n",
      "          21       0.00      0.00      0.00        18\n",
      "          22       0.00      0.00      0.00         5\n",
      "          23       0.00      0.00      0.00        11\n",
      "          24       0.57      0.61      0.59       217\n",
      "          25       0.25      0.10      0.14        42\n",
      "          26       0.00      0.00      0.00        10\n",
      "          27       0.71      0.56      0.63        27\n",
      "          28       0.83      0.56      0.67         9\n",
      "          29       0.18      0.20      0.19        10\n",
      "          30       0.00      0.00      0.00         6\n",
      "          31       0.67      0.50      0.57         4\n",
      "          32       0.00      0.00      0.00        10\n",
      "          33       0.75      1.00      0.86         6\n",
      "          34       1.00      0.14      0.25         7\n",
      "          35       1.00      0.18      0.31        22\n",
      "          36       0.75      0.17      0.27        18\n",
      "          37       0.00      0.00      0.00         5\n",
      "          38       0.93      0.54      0.68        26\n",
      "          39       1.00      0.13      0.24        15\n",
      "          40       0.81      0.52      0.64        82\n",
      "          41       0.00      0.00      0.00         3\n",
      "          42       0.70      0.72      0.71        29\n",
      "          43       0.35      0.22      0.27        36\n",
      "          44       1.00      0.14      0.25         7\n",
      "          45       0.79      0.49      0.61        93\n",
      "          46       0.75      0.31      0.44       115\n",
      "          47       0.49      0.53      0.51        80\n",
      "          48       0.70      1.00      0.83        19\n",
      "          49       0.45      0.57      0.50        44\n",
      "          50       0.00      0.00      0.00         5\n",
      "          51       0.00      0.00      0.00         3\n",
      "          52       0.74      0.68      0.71        25\n",
      "          53       1.00      0.10      0.19        58\n",
      "          54       0.00      0.00      0.00        15\n",
      "          55       0.50      0.09      0.15        23\n",
      "          56       0.00      0.00      0.00         0\n",
      "          57       1.00      0.50      0.67         2\n",
      "          58       0.68      0.22      0.33        79\n",
      "          59       1.00      0.60      0.75        15\n",
      "          60       0.41      0.24      0.30        55\n",
      "          61       0.40      0.40      0.40         5\n",
      "          62       0.50      0.13      0.21        31\n",
      "          63       0.84      0.64      0.73       112\n",
      "          64       0.40      0.33      0.36         6\n",
      "          65       1.00      0.58      0.73        19\n",
      "          66       0.00      0.00      0.00         3\n",
      "          67       0.44      0.15      0.22        48\n",
      "          68       0.00      0.00      0.00         6\n",
      "          69       0.47      0.36      0.41        53\n",
      "          70       0.00      0.00      0.00        19\n",
      "          71       0.00      0.00      0.00         4\n",
      "          72       0.00      0.00      0.00        10\n",
      "          73       0.78      0.67      0.72        21\n",
      "          74       0.00      0.00      0.00        27\n",
      "          75       0.00      0.00      0.00        11\n",
      "          76       1.00      0.33      0.50        12\n",
      "          77       0.00      0.00      0.00         5\n",
      "          78       0.54      0.46      0.50        28\n",
      "          79       0.67      0.30      0.41        27\n",
      "          80       0.00      0.00      0.00         9\n",
      "          81       0.00      0.00      0.00         6\n",
      "          82       0.34      0.23      0.28       230\n",
      "          83       0.26      0.42      0.32        12\n",
      "          84       0.33      0.03      0.06        64\n",
      "          85       0.57      0.57      0.57         7\n",
      "          86       0.67      0.29      0.40        21\n",
      "          87       0.80      0.86      0.83       764\n",
      "          88       0.00      0.00      0.00        30\n",
      "          89       0.91      0.71      0.80        14\n",
      "          90       0.83      0.73      0.77        33\n",
      "          91       0.25      0.11      0.15        18\n",
      "          92       1.00      0.12      0.22         8\n",
      "          93       0.50      0.15      0.24        13\n",
      "          94       0.80      0.24      0.37        33\n",
      "          95       1.00      0.14      0.25         7\n",
      "          96       0.11      0.35      0.16        17\n",
      "          97       0.33      0.07      0.12        14\n",
      "          98       0.54      0.46      0.50       297\n",
      "          99       0.24      0.33      0.27        43\n",
      "         100       0.43      0.46      0.44        35\n",
      "         101       0.67      0.13      0.22        15\n",
      "         102       0.04      0.33      0.07         6\n",
      "         103       0.69      0.60      0.64        15\n",
      "         104       0.00      0.00      0.00         4\n",
      "         105       0.79      0.61      0.69        18\n",
      "         106       0.00      0.00      0.00         6\n",
      "         107       0.00      0.00      0.00         6\n",
      "         108       0.00      0.00      0.00        21\n",
      "         109       0.00      0.00      0.00        12\n",
      "         110       0.00      0.00      0.00         4\n",
      "         111       0.00      0.00      0.00         1\n",
      "         112       0.00      0.00      0.00        36\n",
      "         113       1.00      0.75      0.86        12\n",
      "         114       0.83      0.62      0.71         8\n",
      "         115       0.23      0.20      0.21       340\n",
      "         116       0.00      0.00      0.00         6\n",
      "         117       0.32      0.28      0.30       108\n",
      "         118       0.27      0.18      0.22        79\n",
      "         119       0.33      0.09      0.14        22\n",
      "         120       0.32      0.22      0.26       209\n",
      "         121       0.00      0.00      0.00        17\n",
      "         122       0.63      0.71      0.67        56\n",
      "         123       0.00      0.00      0.00         9\n",
      "         124       1.00      0.25      0.40         8\n",
      "         125       0.00      0.00      0.00         2\n",
      "         126       1.00      0.58      0.74        12\n",
      "         127       0.00      0.00      0.00         6\n",
      "         128       1.00      0.34      0.51        29\n",
      "         129       0.50      0.24      0.32        17\n",
      "         130       0.75      0.16      0.26        19\n",
      "         131       0.33      0.17      0.22         6\n",
      "         132       1.00      0.07      0.13        14\n",
      "         133       1.00      1.00      1.00         4\n",
      "         134       0.56      0.42      0.48        24\n",
      "         135       1.00      0.67      0.80         3\n",
      "         136       1.00      0.20      0.33         5\n",
      "         137       1.00      0.29      0.44         7\n",
      "         138       0.00      0.00      0.00         9\n",
      "         139       0.43      0.27      0.33        22\n",
      "         140       0.00      0.00      0.00        10\n",
      "         141       0.70      0.52      0.60       157\n",
      "         142       0.68      0.47      0.56        32\n",
      "         143       0.62      0.73      0.67       596\n",
      "         144       0.00      0.00      0.00         1\n",
      "         145       0.00      0.00      0.00        43\n",
      "         146       0.80      0.26      0.39        31\n",
      "         147       0.00      0.00      0.00         0\n",
      "         148       0.25      0.11      0.15        18\n",
      "         149       1.00      0.13      0.24        15\n",
      "         150       0.49      0.60      0.54       469\n",
      "         151       0.46      0.48      0.47       250\n",
      "         152       0.25      0.20      0.22         5\n",
      "         153       0.90      0.71      0.79        65\n",
      "         154       0.16      0.17      0.16        30\n",
      "         155       0.00      0.00      0.00         0\n",
      "         156       1.00      0.43      0.60        14\n",
      "         157       1.00      0.07      0.13        88\n",
      "         158       0.45      0.12      0.19       114\n",
      "         159       0.00      0.00      0.00         6\n",
      "         160       0.00      0.00      0.00         6\n",
      "         161       0.86      0.35      0.50        34\n",
      "         162       0.46      0.14      0.22       411\n",
      "         163       0.55      0.55      0.55        86\n",
      "         164       1.00      0.50      0.67         4\n",
      "         165       0.44      0.19      0.27        21\n",
      "         166       0.00      0.00      0.00         6\n",
      "         167       0.00      0.00      0.00         2\n",
      "         168       0.94      0.79      0.86        81\n",
      "         169       0.48      0.33      0.39        92\n",
      "         170       0.55      0.17      0.26        36\n",
      "         171       0.00      0.00      0.00         3\n",
      "         172       0.71      1.00      0.83        15\n",
      "         173       0.33      0.36      0.34        14\n",
      "         174       0.00      0.00      0.00        11\n",
      "         175       0.43      0.18      0.25        17\n",
      "         176       1.00      0.88      0.94        17\n",
      "         177       0.68      0.23      0.35        90\n",
      "         178       0.59      0.60      0.60        60\n",
      "         179       0.15      0.12      0.13        26\n",
      "         180       0.28      0.37      0.32       644\n",
      "         181       0.38      0.24      0.29        25\n",
      "         182       0.42      1.00      0.59         5\n",
      "         183       0.31      0.28      0.29       315\n",
      "         184       1.00      1.00      1.00        16\n",
      "         185       0.57      0.27      0.36       113\n",
      "         186       0.00      0.00      0.00         2\n",
      "         187       0.00      0.00      0.00         2\n",
      "         188       0.00      0.00      0.00         3\n",
      "         189       0.65      0.68      0.67       148\n",
      "         190       1.00      0.83      0.91         6\n",
      "         191       0.41      0.61      0.49        28\n",
      "         192       0.86      0.75      0.80        16\n",
      "         193       0.89      0.63      0.74        38\n",
      "         194       0.75      0.21      0.33        28\n",
      "         195       0.86      0.40      0.55        92\n",
      "         196       0.86      0.73      0.79       132\n",
      "         197       0.00      0.00      0.00         4\n",
      "         198       1.00      0.03      0.06        61\n",
      "         199       0.50      0.50      0.50         4\n",
      "         200       0.33      0.67      0.44         3\n",
      "         201       0.60      0.60      0.60         5\n",
      "         202       0.80      0.63      0.71        19\n",
      "         203       0.55      0.47      0.51        38\n",
      "         204       0.33      0.33      0.33         6\n",
      "         205       0.71      0.60      0.65        20\n",
      "         206       0.14      0.11      0.12        19\n",
      "         207       0.46      0.07      0.12        88\n",
      "         208       1.00      0.17      0.29        12\n",
      "         209       0.91      0.37      0.53        27\n",
      "         210       0.40      0.14      0.21        14\n",
      "         211       0.00      0.00      0.00        17\n",
      "         212       0.50      0.03      0.05        74\n",
      "         213       1.00      1.00      1.00         4\n",
      "         214       0.00      0.00      0.00        15\n",
      "         215       0.33      0.11      0.17         9\n",
      "         216       0.00      0.00      0.00        17\n",
      "         217       1.00      0.67      0.80        12\n",
      "         218       0.51      0.34      0.41       134\n",
      "         219       0.44      0.22      0.30        18\n",
      "         220       1.00      0.73      0.84        11\n",
      "         221       0.38      0.46      0.41        13\n",
      "         222       0.49      0.54      0.52       114\n",
      "         223       1.00      0.14      0.25        69\n",
      "         224       0.25      0.15      0.19        26\n",
      "         225       0.00      0.00      0.00         0\n",
      "         226       0.00      0.00      0.00         7\n",
      "         227       0.00      0.00      0.00         2\n",
      "         228       0.79      0.28      0.41        69\n",
      "         229       0.00      0.00      0.00         6\n",
      "         230       0.00      0.00      0.00         7\n",
      "         231       0.50      0.44      0.47        32\n",
      "         232       0.53      0.64      0.58       937\n",
      "         233       0.00      0.00      0.00         6\n",
      "         234       0.81      0.40      0.53        43\n",
      "         235       0.84      0.33      0.47        49\n",
      "         236       0.41      0.55      0.47        20\n",
      "         237       0.73      0.81      0.77        27\n",
      "         238       0.89      0.24      0.38        33\n",
      "         239       0.70      0.47      0.56        15\n",
      "         240       0.00      0.00      0.00         6\n",
      "         241       0.52      0.65      0.57        51\n",
      "         242       0.88      0.72      0.79        40\n",
      "         243       1.00      0.56      0.71         9\n",
      "         244       0.00      0.00      0.00         7\n",
      "         245       0.05      0.06      0.05        36\n",
      "         246       0.00      0.00      0.00        18\n",
      "         247       0.83      0.53      0.65        19\n",
      "         248       0.00      0.00      0.00         2\n",
      "         249       0.00      0.00      0.00         3\n",
      "         250       0.55      0.60      0.58       151\n",
      "         251       0.55      0.38      0.45       165\n",
      "         252       0.43      0.33      0.38         9\n",
      "         253       0.00      0.00      0.00         0\n",
      "         254       0.00      0.00      0.00        16\n",
      "         255       0.67      0.62      0.65        16\n",
      "         256       0.00      0.00      0.00         5\n",
      "         257       0.20      0.14      0.17        21\n",
      "         258       0.00      0.00      0.00        47\n",
      "         259       0.40      0.47      0.43        17\n",
      "         260       1.00      0.30      0.46        10\n",
      "         261       0.00      0.00      0.00         2\n",
      "         262       0.42      0.14      0.21        35\n",
      "         263       0.75      0.21      0.33        14\n",
      "         264       0.50      0.18      0.26        17\n",
      "         265       0.00      0.00      0.00        22\n",
      "         266       0.57      0.24      0.33        34\n",
      "         267       0.67      0.22      0.33         9\n",
      "         268       0.28      0.28      0.28        29\n",
      "         269       0.00      0.00      0.00         4\n",
      "         270       1.00      0.09      0.17        33\n",
      "         271       0.80      0.52      0.63        69\n",
      "         272       0.00      0.00      0.00         6\n",
      "         273       0.60      0.41      0.49        22\n",
      "         274       0.22      0.29      0.25         7\n",
      "         275       0.92      0.43      0.59        28\n",
      "         276       0.87      0.48      0.62       114\n",
      "         277       0.61      0.31      0.41        61\n",
      "         278       0.00      0.00      0.00        42\n",
      "         279       0.00      0.00      0.00         3\n",
      "         280       1.00      0.96      0.98        25\n",
      "         281       0.25      0.04      0.07        23\n",
      "         282       0.94      0.65      0.77        26\n",
      "         283       1.00      0.14      0.25        28\n",
      "         284       0.47      0.40      0.43        20\n",
      "         285       0.21      0.30      0.25       516\n",
      "         286       1.00      0.62      0.77         8\n",
      "         287       0.00      0.00      0.00         2\n",
      "         288       0.60      0.60      0.60         5\n",
      "         289       0.00      0.00      0.00         5\n",
      "         290       0.82      0.85      0.83        65\n",
      "         291       1.00      0.91      0.95        11\n",
      "         292       0.50      1.00      0.67         1\n",
      "         293       0.67      0.22      0.33         9\n",
      "         294       0.50      0.17      0.25         6\n",
      "         295       0.15      0.27      0.19        30\n",
      "         296       0.86      0.33      0.48        18\n",
      "         297       0.00      0.00      0.00         5\n",
      "         298       0.86      0.65      0.74        37\n",
      "         299       1.00      1.00      1.00        10\n",
      "         300       0.00      0.00      0.00         5\n",
      "         301       0.92      0.88      0.90        25\n",
      "         302       0.85      0.58      0.69        19\n",
      "         303       0.00      0.00      0.00         4\n",
      "         304       0.00      0.00      0.00         4\n",
      "         305       0.67      0.36      0.47        11\n",
      "         306       1.00      1.00      1.00        60\n",
      "         307       0.99      0.42      0.58       159\n",
      "         308       0.00      0.00      0.00         5\n",
      "         309       0.17      0.50      0.25         2\n",
      "\n",
      "   micro avg       0.63      0.53      0.58     19115\n",
      "   macro avg       0.45      0.29      0.32     19115\n",
      "weighted avg       0.63      0.53      0.55     19115\n",
      " samples avg       0.56      0.59      0.56     19115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test_lb , Y_pred_ovr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random test\n",
    "test = np.random.rand(y_e.shape[0],y_e.shape[1])>=0.5\n",
    "f1_score(y_test_lb_,test, average='samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commentaire : \n",
    "L'approche chain combiner à une méthode ensemblise améliore les performaces de quelques pourcent mais l'ordre de grandeur reste le même.\n",
    "\n",
    "L'approche multilabel est une piste à suivre mais elle n'améliore pas ou peu la qualité de notre apprentissage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = sk.metrics.multilabel_confusion_matrix(y_test_lb,y_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_re = pd.DataFrame(classification_report(y_test_lb , Y_pred_ovr,output_dict=True)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'micro avg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-273-9b06eafa88de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_re\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Class'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_re\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/DGS-env/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, mapper, na_action)\u001b[0m\n\u001b[1;32m   4562\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindexes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulti\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMultiIndex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4564\u001b[0;31m         \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4566\u001b[0m         \u001b[0mattributes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_attributes_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DGS-env/lib/python3.7/site-packages/pandas/core/base.py\u001b[0m in \u001b[0;36m_map_values\u001b[0;34m(self, mapper, na_action)\u001b[0m\n\u001b[1;32m   1143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m         \u001b[0;31m# mapper is a function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1145\u001b[0;31m         \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnew_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: 'micro avg'"
     ]
    }
   ],
   "source": [
    "df_re['Class'] = le.inverse_transform(df_re.index.map(int).values[:-5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "L =df_re.index.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'map'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-271-3a35fc6d70ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'map'"
     ]
    }
   ],
   "source": [
    "L.map(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DGS-env",
   "language": "python",
   "name": "dgs-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
