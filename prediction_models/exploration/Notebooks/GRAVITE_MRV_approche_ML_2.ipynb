{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inférence de la GRAVITE à partir des données de MRV (Stratégie ML 2)\n",
    "La variable GRAVITE représente la grvité de l'évenement avec 5 echelons\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Stratégie ML 2**\n",
    "\n",
    "Classifier Ordinal et XGboost\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 0) Chargement des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "import logging\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "lang ='french'\n",
    "\n",
    "import clean_text\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,HashingVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder,OrdinalEncoder,OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.feature_extraction.text import TfidfTransformer,CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, balanced_accuracy_score,f1_score\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.decomposition import TruncatedSVD,IncrementalPCA,SparsePCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "import spacy\n",
    "nlp =spacy.load('fr')\n",
    "from spacy.lang.fr.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 Chargement et exploration des données\n",
    "### 0.1.1 voir le notebook exploration pour l'analyse du champs de gravité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 9.78 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "df_declaration_mrv = pd.read_csv(\"data/data_mrv/declaration_mrv_complet.csv\")#delimiter=';',encoding='ISO-8859-1')\n",
    "id_to_dco = pd.read_csv(\"data/ref_MRV/referentiel_dispositif.csv\",delimiter=';',encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Constructions du jeux d'évaluation\n",
    "On met de coté environ 20% du dataset pour l'évaluation et on ne garde pour l'entrainement seulement les classes avec plus de 10 observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 37.7 s, sys: 252 ms, total: 37.9 s\n",
      "Wall time: 37.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_declaration_mrv = pd.read_csv(\"data/data_mrv/declaration_mrv_complet.csv\")#delimiter=';',encoding='ISO-8859-1')\n",
    "id_to_dco = pd.read_csv(\"data/ref_MRV/referentiel_dispositif.csv\",delimiter=';',encoding='ISO-8859-1')\n",
    "\n",
    "#Supression des NaN\n",
    "\n",
    "#Charegement des colonnes utiles et suppression des NaN\n",
    "\n",
    "df = df_declaration_mrv[['DESCRIPTION_INCIDENT','TYPE_VIGILANCE','LIBELLE_COMMERCIAL','DCO_ID',\n",
    "                         'REFERENCE_COMMERCIALE','ETAT_PATIENT','ACTION_PATIENT','FABRICANT',\n",
    "                          'GRAVITE','CLASSIFICATION']][df_declaration_mrv['GRAVITE'].notna()]\n",
    "\n",
    "# On complète les NaN avec du vide\n",
    "df['ETAT_PATIENT'] = df['ETAT_PATIENT'].fillna(\"\")\n",
    "df['DESCRIPTION_INCIDENT'] = df['DESCRIPTION_INCIDENT'].fillna(\"\")\n",
    "df['LIBELLE_COMMERCIAL'] = df['LIBELLE_COMMERCIAL'].fillna(\"\")\n",
    "df['FABRICANT'] = df['FABRICANT'].fillna(\"\")\n",
    "df[\"REFERENCE_COMMERCIALE\"] = df['REFERENCE_COMMERCIALE'].fillna(\"\")\n",
    "df['TYPE_VIGILANCE'] = df['TYPE_VIGILANCE'].fillna(\"\")\n",
    "df['CLASSIFICATION'] = df['CLASSIFICATION'].fillna('')\n",
    "df['ACTION_PATIENT'] = df['ACTION_PATIENT'].fillna('')\n",
    "df['DCO_ID'] = df['DCO_ID'].fillna(-1)\n",
    "\n",
    "# On ajoute des collones pertinentes\n",
    "df['des_lib'] = df['LIBELLE_COMMERCIAL']+ ' ' + df['DESCRIPTION_INCIDENT']\n",
    "df['fab_lib'] = df['LIBELLE_COMMERCIAL']+ ' ' + df['FABRICANT']\n",
    "df['com'] = df['LIBELLE_COMMERCIAL']+ ' ' + df['REFERENCE_COMMERCIALE']\n",
    "df['Text'] = df['LIBELLE_COMMERCIAL']+ ' ' + df['FABRICANT'] + \"\" + df['DESCRIPTION_INCIDENT']\n",
    "\n",
    "# On nettoie les données :\n",
    "for col in  ['DESCRIPTION_INCIDENT','LIBELLE_COMMERCIAL','ETAT_PATIENT','Text',\"des_lib\",\"fab_lib\"] :\n",
    "        df[col] = df[col].map(lambda x: clean_text.preprocess_text(x))\n",
    "\n",
    "n = 15\n",
    "# On filtre pour a voir plus de n observations par classse\n",
    "df_n = df.groupby(\"GRAVITE\").filter(lambda x: len(x) > n)\n",
    "\n",
    "# On encode les labels\n",
    "def GRAVITE_ENC(x):\n",
    "    if x =='NULLE':\n",
    "        return 0\n",
    "    elif x == 'MINEU':\n",
    "        return 1\n",
    "    elif x == 'MOYEN':\n",
    "        return 2\n",
    "    elif x== 'SEVER':\n",
    "        return 3\n",
    "    elif x== 'CRITI':\n",
    "        return 4\n",
    "df_n.GRAVITE = df_n.GRAVITE.map(lambda x:GRAVITE_ENC(x))\n",
    "\n",
    "#OnEncode les autres varibles\n",
    "le = LabelEncoder()\n",
    "#On encode le type de vigilance\n",
    "#df_n.TYPE_VIGILANCE = le.fit_transform(df_n.TYPE_VIGILANCE.values)\n",
    "#On encode la classifcation \n",
    "#df_n.CLASSIFICATION = le.fit_transform(df_n.CLASSIFICATION.values)\n",
    "#on encode le DCO\n",
    "#df_n.DCO_ID = le.fit_transform(df_n.DCO_ID.values)\n",
    "\n",
    "# On selection les variables de test en faisant attention aux doublons\n",
    "train_index,test_index = next(GroupShuffleSplit(random_state=1029,test_size=0.2).split(df_n, groups=df_n['DESCRIPTION_INCIDENT']))\n",
    "\n",
    "\n",
    "df_train, df_test = df_n.iloc[train_index], df_n.iloc[test_index]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Construction du pipeline  pour la gravité ordonnée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.78 s, sys: 44 ms, total: 7.83 s\n",
      "Wall time: 7.83 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "preprocess = ColumnTransformer(\n",
    "    [('etat_pat_tfidf', TfidfVectorizer(sublinear_tf=True, min_df=3,ngram_range=(1, 1),\n",
    "                                       stop_words=STOP_WORDS,\n",
    "                                       max_features = 10000,norm = 'l2'), 'ETAT_PATIENT'),\n",
    "     \n",
    "     ('description_tfidf',TfidfVectorizer(sublinear_tf=True, min_df=5,\n",
    "                            ngram_range=(1, 1),\n",
    "                            stop_words=STOP_WORDS,\n",
    "                            max_features = 50000,norm = 'l2'), 'DESCRIPTION_INCIDENT'),\n",
    "     \n",
    "     ('action_pat_tfidf',TfidfVectorizer(sublinear_tf=True, min_df=3,\n",
    "                           ngram_range=(1, 1),\n",
    "                           stop_words=STOP_WORDS,\n",
    "                           max_features = 10000,norm = 'l2'), 'ACTION_PATIENT'),\n",
    "     \n",
    "     ('fabricant_tfidf',TfidfVectorizer(sublinear_tf=True, min_df=3,\n",
    "                           ngram_range=(1, 1),\n",
    "                           stop_words=STOP_WORDS,\n",
    "                           max_features = 5000,norm = 'l2'), 'FABRICANT'),\n",
    "     ('classification_enc', TfidfVectorizer(sublinear_tf=True, min_df=5,\n",
    "                            ngram_range=(1, 1),\n",
    "                            stop_words=STOP_WORDS,\n",
    "                            max_features = 100,norm = 'l2'),'CLASSIFICATION')\n",
    "     \n",
    "     #('vigilance_enc', OneHotEncoder(handle_unknown='ignore'),'TYPE_VIGILANCE')\n",
    "     \n",
    "    ],\n",
    "    #\n",
    "    remainder='passthrough')\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', preprocess),\n",
    "    ('clf', CalibratedClassifierCV(LinearSVC(class_weight='balanced'),cv=3, method='isotonic')),\n",
    "])\n",
    "\n",
    "X = df_train[['DESCRIPTION_INCIDENT','ETAT_PATIENT','ACTION_PATIENT','FABRICANT','CLASSIFICATION']] # \n",
    "y = df_train.GRAVITE\n",
    "CV = 5\n",
    "\n",
    "X_ = preprocess.fit_transform(X)\n",
    "\n",
    "#result= cross_validate(pipeline, X, y, scoring=['accuracy','balanced_accuracy','f1_weighted' ], cv=CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "DataFrame.dtypes for data must be int, float or bool.\n                Did not expect the data types in fields DESCRIPTION_INCIDENT, ETAT_PATIENT, ACTION_PATIENT, FABRICANT, CLASSIFICATION",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-aaefdaf2873c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m         X, y, test_size=0.1, random_state=23)\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0md_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0md_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DGS-env/lib/python3.7/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, label, weight, base_margin, missing, silent, feature_names, feature_types, nthread)\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m         data, feature_names, feature_types = _convert_dataframes(\n\u001b[0;32m--> 520\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_types\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    521\u001b[0m         )\n\u001b[1;32m    522\u001b[0m         \u001b[0mmissing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmissing\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmissing\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DGS-env/lib/python3.7/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36m_convert_dataframes\u001b[0;34m(data, feature_names, feature_types, meta, meta_type)\u001b[0m\n\u001b[1;32m    418\u001b[0m                                                             \u001b[0mfeature_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m                                                             \u001b[0mmeta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m                                                             meta_type)\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     data, feature_names, feature_types = _maybe_dt_data(data,\n",
      "\u001b[0;32m~/DGS-env/lib/python3.7/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36m_maybe_pandas_data\u001b[0;34m(data, feature_names, feature_types, meta, meta_type)\u001b[0m\n\u001b[1;32m    292\u001b[0m         msg = \"\"\"DataFrame.dtypes for data must be int, float or bool.\n\u001b[1;32m    293\u001b[0m                 Did not expect the data types in fields \"\"\"\n\u001b[0;32m--> 294\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbad_fields\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfeature_names\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmeta\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: DataFrame.dtypes for data must be int, float or bool.\n                Did not expect the data types in fields DESCRIPTION_INCIDENT, ETAT_PATIENT, ACTION_PATIENT, FABRICANT, CLASSIFICATION"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier, DMatrix\n",
    "import xgboost as xgb\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "        X, y, test_size=0.1, random_state=23)\n",
    "\n",
    "d_valid = DMatrix(X_valid, y_valid)\n",
    "d_train = xgb.DMatrix(X_train, y_train)\n",
    "\n",
    "watchlist = [(d_valid, 'valid')]\n",
    "\n",
    "\n",
    "xgb_params = {'eta': 0.3, \n",
    "            'max_depth': 6, \n",
    "            'objective': 'multi:softmax', \n",
    "            'eval_metric': 'mlogloss', \n",
    "            'seed': 23,\n",
    "            'num_class':5\n",
    "            }\n",
    "\n",
    "\n",
    "xgb_model = xgb.train(xgb_params, d_train, 15, watchlist, verbose_eval=True, early_stopping_rounds=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DGS-env",
   "language": "python",
   "name": "dgs-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
