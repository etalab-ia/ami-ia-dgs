{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approche ML 2.1 : XGboost & Optuna\n",
    "Nous souhaitons dans ce notebook comprendre comment se comporte l'algorithme Xgboost sur le problème de classification des DCO. Sachant tout de même que lalimite dumodèle XGboost et son temps d'execution lorsque le nombre de classes est important (ici 449).\n",
    "\n",
    "Les methodes traditionnelles pour optimiser les hyperparamètres  sont :\n",
    "- Grid Search (mise en oeuvre pour le SVM dans l'approche 2.)\n",
    "- Randomized Search : https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html\n",
    "\n",
    "Ces méthodes sont trop longues pour notre problème car un entrainement nécessite environ 2h de calcul. Afin de sélectionner de manière pertinente les hyperparamèytres nous allons utliser l'optimisation bayesienne. Détailler dans cette article de manière intéressante : https://towardsdatascience.com/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f\n",
    "\n",
    "En python, il existe différenteframeword pour mettre en place cette optimisation bayesienne :\n",
    "- Optuna\n",
    "- Hyperopt\n",
    "\n",
    "Après la lecture de cet article (https://towardsdatascience.com/optuna-vs-hyperopt-which-hyperparameter-optimization-library-should-you-choose-ed8564618151) et les conseils de Boris nous avons choisis optuna.\n",
    "\n",
    "Pour écrire le code, nous nous sommes inspirés de ces examples : \n",
    "- https://machinelearningapplied.com/hyperparameter-search-with-optuna-part-2-xgboost-classification-and-ensembling/\n",
    "- https://machinelearningapplied.com/hyperparameter-search-and-pruning-with-optuna-part-4-xgboost-classification-and-ensembling/\n",
    "- https://www.kaggle.com/kst6690/make-your-xgboost-model-awesome-with-optuna\n",
    "- https://www.kaggle.com/snakayama/xgboost-using-optuna\n",
    "- https://medium.com/optuna/using-optuna-to-optimize-xgboost-hyperparameters-63bfcdfd3407\n",
    "\n",
    "et utilisé la doc: \n",
    "https://optuna.readthedocs.io/en/latest/tutorial/rdb.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "import clean_text\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, balanced_accuracy_score,f1_score\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,HashingVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "import spacy\n",
    "nlp =spacy.load('fr')\n",
    "from spacy.lang.fr.stop_words import STOP_WORDS\n",
    "\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "import optuna\n",
    "from optuna import Trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 48.1 s, sys: 532 ms, total: 48.7 s\n",
      "Wall time: 49 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_declaration_mrv = pd.read_csv(\"data/data_mrv/declaration_mrv_complet.csv\")#delimiter=';',encoding='ISO-8859-1')\n",
    "id_to_dco = pd.read_csv(\"data/ref_MRV/referentiel_dispositif.csv\",delimiter=';',encoding='ISO-8859-1')\n",
    "\n",
    "#Charegement des colonnes utiles\n",
    "df = df_declaration_mrv[['DESCRIPTION_INCIDENT','TYPE_VIGILANCE','LIBELLE_COMMERCIAL',\n",
    "                         'REFERENCE_COMMERCIALE','ETAT_PATIENT','FABRICANT','DCO_ID','CLASSIFICATION']]\n",
    "# On complète les NaN avec du vide\n",
    "df['ETAT_PATIENT'] = df['ETAT_PATIENT'].fillna(\"\")\n",
    "df['DESCRIPTION_INCIDENT'] = df['DESCRIPTION_INCIDENT'].fillna(\"\")\n",
    "df['LIBELLE_COMMERCIAL'] = df['LIBELLE_COMMERCIAL'].fillna(\"\")\n",
    "df['FABRICANT'] = df['FABRICANT'].fillna(\"\")\n",
    "df[\"REFERENCE_COMMERCIALE\"] = df['REFERENCE_COMMERCIALE'].fillna(\"\")\n",
    "df['TYPE_VIGILANCE'] = df['TYPE_VIGILANCE'].fillna(\"\")\n",
    "df['CLASSIFICATION'] = df['CLASSIFICATION'].fillna('')\n",
    "\n",
    "\n",
    "# On ajoute des collones pertinentes\n",
    "df['des_lib'] = df['LIBELLE_COMMERCIAL']+ ' ' + df['DESCRIPTION_INCIDENT']\n",
    "df['fab_lib'] = df['LIBELLE_COMMERCIAL']+ ' ' + df['FABRICANT']\n",
    "df['com'] = df['LIBELLE_COMMERCIAL']+ ' ' + df['REFERENCE_COMMERCIALE']\n",
    "df['Text'] = df['LIBELLE_COMMERCIAL']+ ' ' + df['FABRICANT'] + \"\" + df['DESCRIPTION_INCIDENT']\n",
    "\n",
    "# On nettoie les données :\n",
    "for col in  ['DESCRIPTION_INCIDENT','LIBELLE_COMMERCIAL','ETAT_PATIENT','Text',\"des_lib\",\"fab_lib\"] :\n",
    "    df[col] = df[col].map(lambda x: clean_text.preprocess_text(x))\n",
    "\n",
    "n = 15\n",
    "# On filtre pour a voir plus de n observations par classse\n",
    "df_n = df.groupby(\"DCO_ID\").filter(lambda x: len(x) > n)\n",
    "\n",
    "# On encode les labels\n",
    "le = LabelEncoder()\n",
    "df_n.DCO_ID = le.fit_transform(df_n.DCO_ID.values)\n",
    "#On encode le type de vigilance\n",
    "df_n.TYPE_VIGILANCE = le.fit_transform(df_n.TYPE_VIGILANCE.values)\n",
    "#On encode la classifcation \n",
    "df_n.CLASSIFICATION = le.fit_transform(df_n.CLASSIFICATION.values)\n",
    "\n",
    "# On selection les variables de test en faisant attention aux doublons\n",
    "train_index,test_index = next(GroupShuffleSplit(random_state=1029).split(df_n, groups=df_n['DESCRIPTION_INCIDENT']))\n",
    "df_train, df_test = df_n.iloc[train_index], df_n.iloc[test_index]\n",
    "\n",
    "\n",
    "\n",
    "# On transforme les variables\n",
    "preprocess = ColumnTransformer(\n",
    "    [   \n",
    "     ('libelle_tfidf', TfidfVectorizer(sublinear_tf=True, min_df=3,ngram_range=(1, 1),\n",
    "                                       stop_words=STOP_WORDS,\n",
    "                                       max_features = 10000,norm = 'l2'), 'LIBELLE_COMMERCIAL'),\n",
    "     \n",
    "     ('description_tfidf',TfidfVectorizer(sublinear_tf=True, min_df=5,\n",
    "                            ngram_range=(1, 1),\n",
    "                            stop_words=STOP_WORDS,\n",
    "                            max_features = 10000,norm = 'l2'), 'DESCRIPTION_INCIDENT'),\n",
    "     \n",
    "    ('fabricant_tfidf',TfidfVectorizer(sublinear_tf=True, min_df=3,\n",
    "                            ngram_range=(1, 1),\n",
    "                            stop_words=STOP_WORDS,\n",
    "                            max_features = 10000,norm = 'l2'), 'FABRICANT')],\n",
    "    \n",
    "    remainder='passthrough')\n",
    "\n",
    "X = df_train[['DESCRIPTION_INCIDENT','FABRICANT','LIBELLE_COMMERCIAL']]  \n",
    "X_prep = preprocess.fit_transform(X)\n",
    "X_test = df_test[['DESCRIPTION_INCIDENT','FABRICANT','LIBELLE_COMMERCIAL']]  \n",
    "X_test_prep = preprocess.transform(X_test)\n",
    "\n",
    "y_true = df_test.DCO_ID \n",
    "y = df_train.DCO_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (https://optuna.readthedocs.io/en/stable/faq.html#objective-func-additional-args).\n",
    "def objective(trial:Trial):\n",
    "    data,target = X_prep, y\n",
    "    train_x, valid_x, train_y, valid_y = train_test_split(data, target, test_size=0.1)\n",
    "    dtrain = xgb.DMatrix(train_x, label=train_y)\n",
    "    dvalid = xgb.DMatrix(valid_x, label=valid_y)\n",
    "    \n",
    "    # Liste des paramètres à optimiser\n",
    "    param = {\n",
    "        #\"silent\": 1,\n",
    "        \"verbosity\" :1,\n",
    "        \"objective\":'multi:softmax',\n",
    "        \"eval_metric\":'mlogloss',\n",
    "        \"num_class\":len(y.unique()),\n",
    "        \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\", \"gblinear\", \"dart\"]),#btree and dart use tree based models while gblinear uses linear functions\n",
    "        \"lambda\": trial.suggest_loguniform(\"lambda\", 1e-8, 1.0), #L2 regularization term on weights. Increasing this value will make model more conservative.\n",
    "        \"alpha\": trial.suggest_loguniform(\"alpha\", 1e-8, 1.0), #L1 regularization term on weights. Increasing this value will make model more conservative.\n",
    "        \"nthread\":-1\n",
    "    }\n",
    "\n",
    "    if param[\"booster\"] == \"gbtree\" or param[\"booster\"] == \"dart\":\n",
    "        param[\"max_depth\"] = trial.suggest_int(\"max_depth\", 2, 9) #Maximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit.\n",
    "        param[\"eta\"] = trial.suggest_loguniform(\"eta\", 1e-5, 1.0)#alias: learning_rate\n",
    "        param[\"gamma\"] = trial.suggest_loguniform(\"gamma\", 1e-8, 1.0)#lias: min_split_loss\n",
    "        param[\"grow_policy\"] = trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])#Controls a way new nodes are added to the tree\n",
    "    if param[\"booster\"] == \"dart\":\n",
    "        param[\"sample_type\"] = trial.suggest_categorical(\"sample_type\", [\"uniform\", \"weighted\"])#Type of sampling algorithm\n",
    "        param[\"normalize_type\"] = trial.suggest_categorical(\"normalize_type\", [\"tree\", \"forest\"]) #Type of normalization algorithm\n",
    "        param[\"rate_drop\"] = trial.suggest_loguniform(\"rate_drop\", 1e-8, 1.0) #Dropout rate\n",
    "        param[\"skip_drop\"] = trial.suggest_loguniform(\"skip_drop\", 1e-8, 1.0)#Probability of skipping the dropout procedure during a boosting iteration\n",
    "    \n",
    "    pruning_callback = optuna.integration.XGBoostPruningCallback(trial,'eval-mlogloss')\n",
    "    \n",
    "    bst = xgb.train(param, dtrain,20,evals=[(dvalid, \"eval\")],callbacks=[pruning_callback],early_stopping_rounds=15)\n",
    "    preds = bst.predict(dvalid)\n",
    "    #pred_labels = np.rint(preds)\n",
    "    f1_weighted = f1_score(valid_y, pred_labels, average='weighted')\n",
    "    pprint()\n",
    "    bst.save_model('last_xgb.model')\n",
    "            \n",
    "    return f1_weighted\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\teval-mlogloss:4.48689\n",
      "Will train until eval-mlogloss hasn't improved in 30 rounds.\n",
      "[1]\teval-mlogloss:36.76839\n",
      "[2]\teval-mlogloss:27.88941\n",
      "[3]\teval-mlogloss:35.42197\n",
      "[4]\teval-mlogloss:35.66395\n",
      "[5]\teval-mlogloss:35.80046\n",
      "[6]\teval-mlogloss:27.88941\n",
      "[7]\teval-mlogloss:36.25961\n",
      "[8]\teval-mlogloss:36.32786\n",
      "[9]\teval-mlogloss:36.25340\n",
      "[10]\teval-mlogloss:27.88941\n",
      "[11]\teval-mlogloss:36.24720\n",
      "[12]\teval-mlogloss:36.30925\n",
      "[13]\teval-mlogloss:36.29684\n",
      "[14]\teval-mlogloss:27.88941\n",
      "[15]\teval-mlogloss:36.24720\n",
      "[16]\teval-mlogloss:36.37750\n",
      "[17]\teval-mlogloss:36.45195\n",
      "[18]\teval-mlogloss:27.88941\n",
      "[19]\teval-mlogloss:36.43334\n",
      "[0]\teval-mlogloss:6.10659\n",
      "Will train until eval-mlogloss hasn't improved in 30 rounds.\n",
      "[1]\teval-mlogloss:6.10635\n",
      "[2]\teval-mlogloss:6.10588\n",
      "[3]\teval-mlogloss:6.10561\n",
      "[4]\teval-mlogloss:6.10526\n",
      "[5]\teval-mlogloss:6.10475\n",
      "[6]\teval-mlogloss:6.10458\n",
      "[7]\teval-mlogloss:6.10406\n",
      "[8]\teval-mlogloss:6.10373\n",
      "[9]\teval-mlogloss:6.10322\n",
      "[10]\teval-mlogloss:6.10297\n",
      "[11]\teval-mlogloss:6.10273\n",
      "[12]\teval-mlogloss:6.10227\n",
      "[13]\teval-mlogloss:6.10193\n",
      "[14]\teval-mlogloss:6.10145\n",
      "[15]\teval-mlogloss:6.10120\n",
      "[16]\teval-mlogloss:6.10075\n",
      "[17]\teval-mlogloss:6.10044\n",
      "[18]\teval-mlogloss:6.10012\n",
      "[19]\teval-mlogloss:6.09962\n",
      "[0]\teval-mlogloss:2.89359\n",
      "Will train until eval-mlogloss hasn't improved in 30 rounds.\n",
      "[1]\teval-mlogloss:2.32726\n",
      "[2]\teval-mlogloss:1.90410\n",
      "[3]\teval-mlogloss:1.65215\n",
      "[4]\teval-mlogloss:1.46701\n",
      "[5]\teval-mlogloss:1.34434\n",
      "[6]\teval-mlogloss:1.24030\n",
      "[7]\teval-mlogloss:1.15479\n",
      "[8]\teval-mlogloss:1.08210\n",
      "[9]\teval-mlogloss:1.01906\n",
      "[10]\teval-mlogloss:0.96767\n",
      "[11]\teval-mlogloss:0.92226\n",
      "[12]\teval-mlogloss:0.88117\n",
      "[13]\teval-mlogloss:0.84591\n",
      "[14]\teval-mlogloss:0.81166\n",
      "[15]\teval-mlogloss:0.78241\n",
      "[16]\teval-mlogloss:0.75521\n",
      "[17]\teval-mlogloss:0.73028\n",
      "[18]\teval-mlogloss:0.70742\n",
      "[19]\teval-mlogloss:0.68773\n",
      "[0]\teval-mlogloss:5.53389\n",
      "Will train until eval-mlogloss hasn't improved in 30 rounds.\n",
      "[1]\teval-mlogloss:5.28193\n",
      "[2]\teval-mlogloss:5.09350\n",
      "[3]\teval-mlogloss:4.93975\n",
      "[4]\teval-mlogloss:4.80666\n",
      "[5]\teval-mlogloss:4.69132\n",
      "[6]\teval-mlogloss:4.58749\n",
      "[7]\teval-mlogloss:4.49278\n",
      "[8]\teval-mlogloss:4.40528\n",
      "[9]\teval-mlogloss:4.32480\n",
      "[10]\teval-mlogloss:4.24980\n",
      "[11]\teval-mlogloss:4.17968\n",
      "[12]\teval-mlogloss:4.11393\n",
      "[13]\teval-mlogloss:4.05208\n",
      "[14]\teval-mlogloss:3.99348\n",
      "[15]\teval-mlogloss:3.93682\n",
      "[16]\teval-mlogloss:3.88295\n",
      "[17]\teval-mlogloss:3.83136\n",
      "[18]\teval-mlogloss:3.78208\n",
      "[19]\teval-mlogloss:3.73469\n",
      "[0]\teval-mlogloss:20.04940\n",
      "Will train until eval-mlogloss hasn't improved in 30 rounds.\n",
      "[1]\teval-mlogloss:36.47057\n",
      "[2]\teval-mlogloss:35.46540\n",
      "[3]\teval-mlogloss:35.71979\n",
      "[4]\teval-mlogloss:35.78184\n",
      "[5]\teval-mlogloss:27.67845\n",
      "[6]\teval-mlogloss:35.97419\n",
      "[7]\teval-mlogloss:36.24720\n",
      "[8]\teval-mlogloss:36.29684\n",
      "[9]\teval-mlogloss:27.67845\n",
      "[10]\teval-mlogloss:36.33406\n",
      "[11]\teval-mlogloss:36.34027\n",
      "[12]\teval-mlogloss:36.37129\n",
      "[13]\teval-mlogloss:27.67845\n",
      "[14]\teval-mlogloss:36.36509\n",
      "[15]\teval-mlogloss:36.38370\n",
      "[16]\teval-mlogloss:36.52021\n",
      "[17]\teval-mlogloss:27.67845\n",
      "[18]\teval-mlogloss:36.43334\n",
      "[19]\teval-mlogloss:36.46436\n",
      "[0]\teval-mlogloss:6.10165\n",
      "Will train until eval-mlogloss hasn't improved in 30 rounds.\n",
      "[0]\teval-mlogloss:20.58371\n",
      "Will train until eval-mlogloss hasn't improved in 30 rounds.\n",
      "[1]\teval-mlogloss:36.55123\n",
      "[2]\teval-mlogloss:35.53365\n",
      "[3]\teval-mlogloss:35.68877\n",
      "[4]\teval-mlogloss:35.70739\n",
      "[5]\teval-mlogloss:27.77152\n",
      "[6]\teval-mlogloss:36.03003\n",
      "[7]\teval-mlogloss:36.30304\n",
      "[8]\teval-mlogloss:36.16033\n",
      "[9]\teval-mlogloss:27.77152\n",
      "[10]\teval-mlogloss:36.35888\n",
      "[11]\teval-mlogloss:36.30925\n",
      "[12]\teval-mlogloss:36.24099\n",
      "[13]\teval-mlogloss:27.77152\n",
      "[14]\teval-mlogloss:36.31545\n",
      "[15]\teval-mlogloss:36.36509\n",
      "[16]\teval-mlogloss:36.42713\n",
      "[17]\teval-mlogloss:27.77152\n",
      "[18]\teval-mlogloss:36.42093\n",
      "[19]\teval-mlogloss:36.43334\n",
      "[0]\teval-mlogloss:4.30288\n",
      "Will train until eval-mlogloss hasn't improved in 30 rounds.\n",
      "[0]\teval-mlogloss:6.10368\n",
      "Will train until eval-mlogloss hasn't improved in 30 rounds.\n",
      "FrozenTrial(number=2, value=0.8703358941441853, datetime_start=datetime.datetime(2020, 5, 28, 14, 37, 41, 50529), datetime_complete=datetime.datetime(2020, 5, 28, 15, 27, 6, 984571), params={'booster': 'gbtree', 'lambda': 0.3285133500747008, 'alpha': 0.2400936048140958, 'max_depth': 9, 'eta': 0.13219460009896047, 'gamma': 0.0060702836128487215, 'grow_policy': 'lossguide'}, distributions={'booster': CategoricalDistribution(choices=('gbtree', 'gblinear', 'dart')), 'lambda': LogUniformDistribution(high=1.0, low=1e-08), 'alpha': LogUniformDistribution(high=1.0, low=1e-08), 'max_depth': IntUniformDistribution(high=9, low=1, step=1), 'eta': LogUniformDistribution(high=1.0, low=1e-08), 'gamma': LogUniformDistribution(high=1.0, low=1e-08), 'grow_policy': CategoricalDistribution(choices=('depthwise', 'lossguide'))}, user_attrs={}, system_attrs={}, intermediate_values={0: 2.893587, 1: 2.327263, 2: 1.904098, 3: 1.652153, 4: 1.467014, 5: 1.344341, 6: 1.240303, 7: 1.15479, 8: 1.082102, 9: 1.019059, 10: 0.967668, 11: 0.922255, 12: 0.881166, 13: 0.845914, 14: 0.811658, 15: 0.782406, 16: 0.755214, 17: 0.730278, 18: 0.707422, 19: 0.687734}, trial_id=2, state=TrialState.COMPLETE)\n",
      "CPU times: user 3h 52s, sys: 25.2 s, total: 3h 1min 17s\n",
      "Wall time: 3h 1min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "studyName = 'test_study'\n",
    "maximum_time = 3*60*60#second\n",
    "number_of_random_points = 25\n",
    "#optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "study = optuna.create_study(study_name = studyName,  direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=15, timeout=maximum_time)# On créer 15 points\n",
    "#Alternative\n",
    "\n",
    "#Suvegarde du resultat\n",
    "df = study.trials_dataframe()\n",
    "df.to_json(studyName+'.json')\n",
    "print(study.best_trial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resultat de 14h de run en optimisant la loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('14_h_study.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df.sort_values('value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'number': 4,\n",
       " 'value': 0.703457,\n",
       " 'datetime_start': 1590765186400,\n",
       " 'datetime_complete': 1590770377614,\n",
       " 'duration': 5191213,\n",
       " 'params_alpha': 0.0131112178,\n",
       " 'params_booster': 'dart',\n",
       " 'params_eta': 0.1366919939,\n",
       " 'params_gamma': 0.00010909920000000001,\n",
       " 'params_grow_policy': 'depthwise',\n",
       " 'params_lambda': 2.05748e-05,\n",
       " 'params_max_depth': 7.0,\n",
       " 'params_normalize_type': 'tree',\n",
       " 'params_rate_drop': 4.187e-06,\n",
       " 'params_sample_type': 'weighted',\n",
       " 'params_skip_drop': 8.24e-08,\n",
       " 'state': 'COMPLETE'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[4].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\teval-mlogloss:2.87413\n",
      "[1]\teval-mlogloss:2.24748\n",
      "[2]\teval-mlogloss:1.88546\n",
      "[3]\teval-mlogloss:1.63839\n",
      "[4]\teval-mlogloss:1.48865\n",
      "[5]\teval-mlogloss:1.30299\n",
      "[6]\teval-mlogloss:1.19614\n",
      "[7]\teval-mlogloss:1.11411\n",
      "[8]\teval-mlogloss:1.04723\n",
      "[9]\teval-mlogloss:0.99068\n",
      "[10]\teval-mlogloss:0.93931\n",
      "[11]\teval-mlogloss:0.89266\n",
      "[12]\teval-mlogloss:0.85504\n",
      "[13]\teval-mlogloss:0.81862\n",
      "[14]\teval-mlogloss:0.78683\n",
      "[15]\teval-mlogloss:0.75795\n",
      "[16]\teval-mlogloss:0.73166\n",
      "[17]\teval-mlogloss:0.70775\n",
      "[18]\teval-mlogloss:0.68668\n",
      "[19]\teval-mlogloss:0.66736\n",
      "CPU times: user 2h 55min 14s, sys: 39.1 s, total: 2h 55min 53s\n",
      "Wall time: 12min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "data,target = X_prep, y\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(data, target, test_size=0.1)\n",
    "dtrain = xgb.DMatrix(train_x, label=train_y)\n",
    "dvalid = xgb.DMatrix(valid_x, label=valid_y)\n",
    "\n",
    "xgb_params = {'alpha': 0.013,\n",
    "              'booster': 'dart',\n",
    "              'eta': 0.13,\n",
    "              'gamma': 0.0010,\n",
    "              'grow_policy': 'depthwise',\n",
    "              'lambda':2.05748e-05,\n",
    "              'max_depth': 7,\n",
    "              'normalize_type': 'tree',\n",
    "              'rate_drop': 4.187e-06,\n",
    "              'sample_type': 'weighted',\n",
    "              'skip_drop': 8.24e-08,\n",
    "              'objective': 'multi:softmax', \n",
    "              'eval_metric': 'mlogloss', \n",
    "              'seed': 23,\n",
    "              'num_class':len(y.unique())\n",
    "            }\n",
    "\n",
    "bst = xgb.train(xgb_params, dtrain,20,evals=[(dvalid, \"eval\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = bst.predict( xgb.DMatrix(X_test_prep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "précision: 0.7156460693681796\n",
      "présision pondéré:  0.6228305996165773\n",
      "f1_weighted :  0.7370516663765648\n"
     ]
    }
   ],
   "source": [
    "print('précision:',accuracy_score(y_pred,y_true))\n",
    "print('présision pondéré: ', balanced_accuracy_score(y_pred,y_true))\n",
    "print('f1_weighted : ',f1_score(y_pred,y_true,average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commentaire : \n",
    "Pour interpréter la Loss dans le cas ou nous avons 449 classes équilibrées, il faut savoir que -log (1/449) = 6.10, représente le seuil de \"connaissance\".\n",
    "Nos classes étant fortment déséquilibré, si l'onprend la stratégie Naive de prédire la classe majoritaire à chaque fois, la loss est alors de -log(14638/len(df_train)) =1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'number': 2,\n",
       " 'value': 0.8703358941441853,\n",
       " 'datetime_start': Timestamp('2020-05-28 14:37:41.050529'),\n",
       " 'datetime_complete': Timestamp('2020-05-28 15:27:06.984571'),\n",
       " 'duration': Timedelta('0 days 00:49:25.934042'),\n",
       " 'params_alpha': 0.2400936048140958,\n",
       " 'params_booster': 'gbtree',\n",
       " 'params_eta': 0.13219460009896047,\n",
       " 'params_gamma': 0.0060702836128487215,\n",
       " 'params_grow_policy': 'lossguide',\n",
       " 'params_lambda': 0.3285133500747008,\n",
       " 'params_max_depth': 9.0,\n",
       " 'params_normalize_type': nan,\n",
       " 'params_rate_drop': nan,\n",
       " 'params_sample_type': nan,\n",
       " 'params_skip_drop': nan,\n",
       " 'state': 'COMPLETE'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[2].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\teval-mlogloss:2.88022\n",
      "[1]\teval-mlogloss:2.28407\n",
      "[2]\teval-mlogloss:1.86922\n",
      "[3]\teval-mlogloss:1.63167\n",
      "[4]\teval-mlogloss:1.46906\n",
      "[5]\teval-mlogloss:1.34692\n",
      "[6]\teval-mlogloss:1.24930\n",
      "[7]\teval-mlogloss:1.16259\n",
      "[8]\teval-mlogloss:1.09278\n",
      "[9]\teval-mlogloss:1.03138\n",
      "[10]\teval-mlogloss:0.97851\n",
      "[11]\teval-mlogloss:0.93084\n",
      "[12]\teval-mlogloss:0.88758\n",
      "[13]\teval-mlogloss:0.84909\n",
      "[14]\teval-mlogloss:0.81378\n",
      "[15]\teval-mlogloss:0.78253\n",
      "[16]\teval-mlogloss:0.75379\n",
      "[17]\teval-mlogloss:0.72781\n",
      "[18]\teval-mlogloss:0.70488\n",
      "[19]\teval-mlogloss:0.68442\n"
     ]
    }
   ],
   "source": [
    "data,target = X_prep, y\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(data, target, test_size=0.1)\n",
    "dtrain = xgb.DMatrix(train_x, label=train_y)\n",
    "dvalid = xgb.DMatrix(valid_x, label=valid_y)\n",
    "\n",
    "xgb_params = {'alpha': 0.2400936048140958,\n",
    "              'booster': 'gbtree',\n",
    "              'eta': 0.13219460009896047,\n",
    "              'gamma': 0.0060702836128487215,\n",
    "              'grow_policy': 'lossguide',\n",
    "              'lambda': 0.3285133500747008,\n",
    "              'max_depth': 9,\n",
    "              'objective': 'multi:softmax', \n",
    "              'eval_metric': 'mlogloss', \n",
    "              'seed': 23,\n",
    "              'num_class':len(y.unique())\n",
    "            }\n",
    "\n",
    "bst = xgb.train(xgb_params, dtrain,20,evals=[(dvalid, \"eval\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = bst.predict( xgb.DMatrix(X_test_prep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "précision: 0.7315632167929381\n",
      "présision pondéré:  0.6314742092997577\n",
      "f1_weighted :  0.7547056234173378\n"
     ]
    }
   ],
   "source": [
    "print('précision:',accuracy_score(y_pred,y_true))\n",
    "print('présision pondéré: ', balanced_accuracy_score(y_pred,y_true))\n",
    "print('f1_weighted : ',f1_score(y_pred,y_true,average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00         9\n",
      "         1.0       0.50      1.00      0.67         3\n",
      "         2.0       0.00      0.00      0.00         3\n",
      "         3.0       0.50      0.50      0.50         2\n",
      "         4.0       0.50      0.50      0.50         2\n",
      "         5.0       0.82      0.93      0.87        15\n",
      "         6.0       0.33      1.00      0.50         1\n",
      "         7.0       0.20      0.25      0.22         4\n",
      "         8.0       0.33      0.67      0.44         3\n",
      "         9.0       0.74      0.65      0.69        31\n",
      "        10.0       0.77      0.67      0.71        15\n",
      "        11.0       0.50      1.00      0.67         1\n",
      "        12.0       0.00      0.00      0.00         0\n",
      "        13.0       0.64      0.54      0.58        13\n",
      "        14.0       0.22      1.00      0.36         2\n",
      "        15.0       0.20      1.00      0.33         1\n",
      "        16.0       0.86      0.74      0.80        43\n",
      "        17.0       0.98      0.83      0.90        99\n",
      "        18.0       0.00      0.00      0.00         0\n",
      "        19.0       0.00      0.00      0.00         8\n",
      "        20.0       0.17      0.33      0.22         3\n",
      "        21.0       0.43      0.69      0.53        13\n",
      "        22.0       0.43      0.75      0.55         4\n",
      "        23.0       0.57      0.62      0.59        39\n",
      "        24.0       0.80      0.80      0.80         5\n",
      "        25.0       0.00      0.00      0.00         9\n",
      "        26.0       0.62      0.59      0.60        22\n",
      "        27.0       0.67      0.94      0.78        34\n",
      "        28.0       1.00      1.00      1.00        10\n",
      "        29.0       0.90      0.69      0.78        39\n",
      "        30.0       0.57      0.86      0.69        14\n",
      "        31.0       0.36      0.67      0.47         6\n",
      "        32.0       0.50      0.33      0.40         3\n",
      "        33.0       0.30      0.60      0.40         5\n",
      "        34.0       0.33      1.00      0.50         2\n",
      "        35.0       1.00      0.65      0.79        57\n",
      "        36.0       0.09      1.00      0.17         1\n",
      "        37.0       0.44      0.40      0.42        10\n",
      "        38.0       0.67      0.33      0.44        12\n",
      "        39.0       0.00      0.00      0.00         0\n",
      "        40.0       0.00      0.00      0.00         0\n",
      "        41.0       0.33      1.00      0.50         4\n",
      "        42.0       0.50      0.50      0.50         2\n",
      "        43.0       0.67      0.67      0.67         3\n",
      "        44.0       0.60      0.38      0.46         8\n",
      "        45.0       0.17      1.00      0.29         1\n",
      "        46.0       0.86      0.86      0.86         7\n",
      "        47.0       0.00      0.00      0.00         0\n",
      "        48.0       0.20      1.00      0.33         1\n",
      "        49.0       0.43      1.00      0.60         3\n",
      "        50.0       0.00      0.00      0.00         0\n",
      "        51.0       0.60      1.00      0.75         3\n",
      "        52.0       0.93      0.66      0.77        56\n",
      "        53.0       0.50      0.75      0.60         4\n",
      "        54.0       0.75      1.00      0.86         6\n",
      "        55.0       1.00      0.88      0.93         8\n",
      "        56.0       0.46      1.00      0.63         6\n",
      "        57.0       0.81      0.77      0.79        22\n",
      "        58.0       0.89      1.00      0.94         8\n",
      "        59.0       0.25      1.00      0.40         1\n",
      "        60.0       0.33      0.89      0.48         9\n",
      "        61.0       0.67      0.67      0.67         3\n",
      "        62.0       0.75      0.68      0.71        22\n",
      "        63.0       0.82      0.60      0.69        30\n",
      "        64.0       0.00      0.00      0.00         1\n",
      "        65.0       0.17      0.31      0.22        13\n",
      "        66.0       0.00      0.00      0.00         0\n",
      "        67.0       0.42      0.71      0.53         7\n",
      "        68.0       0.52      0.24      0.33        58\n",
      "        69.0       0.00      0.00      0.00         0\n",
      "        70.0       0.17      0.50      0.25         2\n",
      "        71.0       0.13      1.00      0.24         2\n",
      "        72.0       0.20      0.50      0.29         4\n",
      "        73.0       0.33      0.50      0.40         2\n",
      "        74.0       0.00      0.00      0.00         0\n",
      "        75.0       0.00      0.00      0.00         0\n",
      "        76.0       1.00      1.00      1.00         1\n",
      "        77.0       0.29      0.40      0.33         5\n",
      "        78.0       0.00      0.00      0.00         0\n",
      "        79.0       0.14      0.22      0.17         9\n",
      "        80.0       0.48      0.48      0.48        29\n",
      "        81.0       0.75      0.78      0.76        27\n",
      "        82.0       0.33      1.00      0.50         1\n",
      "        83.0       0.44      0.89      0.59         9\n",
      "        84.0       0.10      1.00      0.18         1\n",
      "        85.0       0.00      0.00      0.00         0\n",
      "        86.0       0.57      1.00      0.73         4\n",
      "        87.0       0.84      0.74      0.78       388\n",
      "        88.0       0.57      0.67      0.62        18\n",
      "        89.0       0.57      0.53      0.55        30\n",
      "        90.0       0.24      0.44      0.31         9\n",
      "        91.0       0.78      0.61      0.69       219\n",
      "        92.0       0.87      0.66      0.75       167\n",
      "        93.0       0.39      0.27      0.32        51\n",
      "        94.0       0.36      0.45      0.40        22\n",
      "        95.0       0.83      1.00      0.91         5\n",
      "        96.0       0.00      0.00      0.00         0\n",
      "        97.0       0.97      0.73      0.83        44\n",
      "        98.0       0.00      0.00      0.00         0\n",
      "        99.0       0.94      1.00      0.97        33\n",
      "       100.0       0.80      0.89      0.84         9\n",
      "       101.0       0.60      0.86      0.71         7\n",
      "       102.0       0.67      1.00      0.80         2\n",
      "       103.0       0.36      1.00      0.53         4\n",
      "       104.0       0.00      0.00      0.00         0\n",
      "       105.0       0.99      0.95      0.97      3703\n",
      "       106.0       0.31      0.56      0.40        18\n",
      "       107.0       0.75      0.86      0.80         7\n",
      "       108.0       0.77      0.25      0.38       118\n",
      "       109.0       0.31      0.67      0.42         6\n",
      "       110.0       0.00      0.00      0.00         0\n",
      "       111.0       0.00      0.00      0.00         4\n",
      "       112.0       0.51      0.39      0.44        79\n",
      "       113.0       0.82      0.41      0.55        34\n",
      "       114.0       0.71      0.83      0.77         6\n",
      "       115.0       0.00      0.00      0.00         0\n",
      "       116.0       0.83      0.78      0.81        32\n",
      "       117.0       0.50      0.75      0.60         4\n",
      "       118.0       0.50      1.00      0.67         3\n",
      "       119.0       0.89      0.87      0.88        45\n",
      "       120.0       0.25      0.42      0.31        12\n",
      "       121.0       0.80      0.65      0.72        37\n",
      "       122.0       0.40      1.00      0.57         2\n",
      "       123.0       0.00      0.00      0.00         0\n",
      "       124.0       0.67      0.56      0.61        39\n",
      "       125.0       0.00      0.00      0.00         4\n",
      "       126.0       0.73      0.83      0.77        58\n",
      "       127.0       0.15      0.29      0.20         7\n",
      "       128.0       0.00      0.00      0.00         0\n",
      "       129.0       0.56      0.29      0.38        17\n",
      "       130.0       0.60      0.90      0.72        10\n",
      "       131.0       0.59      0.61      0.60        44\n",
      "       132.0       0.00      0.00      0.00         0\n",
      "       133.0       0.78      0.72      0.75        43\n",
      "       134.0       0.00      0.00      0.00         1\n",
      "       135.0       0.00      0.00      0.00         1\n",
      "       136.0       0.38      0.75      0.50         8\n",
      "       137.0       0.75      0.75      0.75         8\n",
      "       138.0       0.00      0.00      0.00         0\n",
      "       139.0       1.00      1.00      1.00         2\n",
      "       140.0       0.89      0.62      0.73       106\n",
      "       142.0       0.79      0.65      0.71        23\n",
      "       143.0       0.00      0.00      0.00         2\n",
      "       144.0       0.78      0.84      0.81        76\n",
      "       145.0       0.80      0.62      0.70        13\n",
      "       146.0       0.56      0.53      0.55        17\n",
      "       147.0       0.85      1.00      0.92        22\n",
      "       148.0       1.00      1.00      1.00         6\n",
      "       149.0       0.62      0.83      0.71        12\n",
      "       150.0       0.81      0.38      0.52       116\n",
      "       151.0       0.14      0.20      0.17         5\n",
      "       152.0       0.00      0.00      0.00         0\n",
      "       153.0       0.00      0.00      0.00         3\n",
      "       154.0       0.00      0.00      0.00         0\n",
      "       155.0       0.00      0.00      0.00         1\n",
      "       156.0       0.97      0.74      0.84        39\n",
      "       157.0       0.95      0.95      0.95       241\n",
      "       158.0       0.65      0.50      0.57        60\n",
      "       159.0       0.50      1.00      0.67         1\n",
      "       160.0       0.00      0.00      0.00         0\n",
      "       161.0       0.33      1.00      0.50         5\n",
      "       162.0       0.57      0.40      0.47        10\n",
      "       163.0       1.00      0.71      0.83        17\n",
      "       164.0       0.40      1.00      0.57         2\n",
      "       165.0       0.69      0.67      0.68        33\n",
      "       166.0       0.81      0.61      0.69        28\n",
      "       167.0       0.00      0.00      0.00         0\n",
      "       168.0       0.25      1.00      0.40         1\n",
      "       169.0       0.73      0.63      0.68        30\n",
      "       170.0       1.00      0.89      0.94         9\n",
      "       171.0       0.20      1.00      0.33         1\n",
      "       172.0       0.00      0.00      0.00         0\n",
      "       173.0       0.00      0.00      0.00         0\n",
      "       174.0       0.72      0.95      0.82        22\n",
      "       175.0       0.00      0.00      0.00         1\n",
      "       176.0       0.40      0.50      0.44         4\n",
      "       177.0       0.67      0.91      0.77        11\n",
      "       178.0       0.40      1.00      0.57         2\n",
      "       179.0       0.18      0.25      0.21         8\n",
      "       180.0       0.64      0.37      0.47        43\n",
      "       181.0       0.50      0.50      0.50         4\n",
      "       182.0       0.65      1.00      0.79        11\n",
      "       183.0       0.56      1.00      0.71         5\n",
      "       184.0       0.58      1.00      0.73        15\n",
      "       185.0       0.23      0.43      0.30         7\n",
      "       186.0       0.23      0.33      0.27         9\n",
      "       187.0       0.95      0.64      0.76        55\n",
      "       188.0       0.25      0.50      0.33         6\n",
      "       189.0       0.75      0.43      0.55        21\n",
      "       190.0       0.82      0.64      0.72        36\n",
      "       191.0       0.00      0.00      0.00         0\n",
      "       192.0       0.25      0.17      0.20         6\n",
      "       193.0       0.75      0.75      0.75        16\n",
      "       194.0       0.25      0.20      0.22         5\n",
      "       195.0       0.77      0.78      0.78        64\n",
      "       196.0       0.77      0.45      0.57        73\n",
      "       197.0       0.00      0.00      0.00         0\n",
      "       198.0       0.50      1.00      0.67         1\n",
      "       199.0       0.82      0.75      0.78        68\n",
      "       200.0       0.85      0.56      0.67        70\n",
      "       201.0       0.07      0.20      0.10         5\n",
      "       202.0       0.08      1.00      0.14         1\n",
      "       203.0       0.60      0.60      0.60        10\n",
      "       204.0       0.18      0.67      0.29         3\n",
      "       205.0       0.88      0.79      0.84        29\n",
      "       206.0       0.67      0.50      0.57         4\n",
      "       208.0       0.00      0.00      0.00         0\n",
      "       209.0       1.00      1.00      1.00         1\n",
      "       210.0       0.00      0.00      0.00         0\n",
      "       211.0       0.50      0.71      0.59        14\n",
      "       212.0       0.33      0.75      0.46         4\n",
      "       213.0       0.75      0.43      0.55         7\n",
      "       214.0       0.47      0.47      0.47        17\n",
      "       215.0       0.00      0.00      0.00         0\n",
      "       216.0       0.71      0.77      0.74        13\n",
      "       217.0       0.78      0.70      0.74        40\n",
      "       218.0       0.00      0.00      0.00         0\n",
      "       219.0       0.44      1.00      0.62         4\n",
      "       220.0       0.87      0.92      0.89       161\n",
      "       221.0       1.00      1.00      1.00         6\n",
      "       222.0       0.60      0.86      0.71         7\n",
      "       223.0       0.71      1.00      0.83        10\n",
      "       224.0       0.83      0.88      0.86       109\n",
      "       225.0       0.82      0.66      0.73       185\n",
      "       226.0       0.65      0.32      0.43        41\n",
      "       227.0       0.91      0.77      0.83        78\n",
      "       228.0       0.29      0.50      0.36         4\n",
      "       229.0       0.00      0.00      0.00         1\n",
      "       230.0       0.00      0.00      0.00         1\n",
      "       231.0       0.00      0.00      0.00         8\n",
      "       232.0       0.67      0.80      0.73         5\n",
      "       233.0       0.00      0.00      0.00         1\n",
      "       234.0       0.00      0.00      0.00         2\n",
      "       235.0       0.05      1.00      0.10         1\n",
      "       236.0       0.50      1.00      0.67         1\n",
      "       237.0       0.71      0.58      0.64        26\n",
      "       238.0       0.71      0.83      0.77         6\n",
      "       239.0       0.17      1.00      0.29         1\n",
      "       240.0       0.00      0.00      0.00         0\n",
      "       241.0       0.00      0.00      0.00         9\n",
      "       242.0       0.50      1.00      0.67         2\n",
      "       243.0       0.78      0.54      0.64        26\n",
      "       244.0       1.00      0.59      0.74        17\n",
      "       245.0       0.00      0.00      0.00         0\n",
      "       246.0       0.33      1.00      0.50         2\n",
      "       247.0       0.00      0.00      0.00         2\n",
      "       248.0       0.33      0.50      0.40         2\n",
      "       249.0       0.73      0.70      0.71       123\n",
      "       250.0       0.17      1.00      0.29         1\n",
      "       251.0       0.00      0.00      0.00         0\n",
      "       252.0       0.29      0.50      0.36         8\n",
      "       253.0       0.46      0.33      0.39        36\n",
      "       254.0       0.00      0.00      0.00         0\n",
      "       255.0       0.00      0.00      0.00         0\n",
      "       256.0       0.60      0.75      0.67         8\n",
      "       257.0       0.71      1.00      0.83         5\n",
      "       258.0       0.60      0.43      0.50         7\n",
      "       259.0       0.62      1.00      0.77        10\n",
      "       260.0       0.47      1.00      0.64         7\n",
      "       261.0       0.44      0.40      0.42        10\n",
      "       262.0       0.65      0.45      0.53       304\n",
      "       263.0       0.20      0.50      0.29         4\n",
      "       264.0       0.49      0.54      0.51       123\n",
      "       265.0       0.00      0.00      0.00         3\n",
      "       266.0       0.12      0.67      0.20         3\n",
      "       267.0       0.50      0.59      0.55        91\n",
      "       268.0       0.73      0.62      0.67       571\n",
      "       269.0       0.33      0.50      0.40         2\n",
      "       270.0       0.55      0.54      0.54       194\n",
      "       271.0       0.71      0.57      0.64        87\n",
      "       272.0       0.33      0.82      0.47        17\n",
      "       273.0       0.84      0.61      0.70       339\n",
      "       274.0       0.11      0.25      0.15        16\n",
      "       275.0       0.58      0.57      0.57        67\n",
      "       276.0       0.91      0.88      0.89        88\n",
      "       277.0       0.00      0.00      0.00         2\n",
      "       278.0       0.28      0.77      0.41        13\n",
      "       279.0       0.00      0.00      0.00         0\n",
      "       280.0       0.00      0.00      0.00         0\n",
      "       281.0       0.50      0.80      0.62         5\n",
      "       282.0       0.62      0.65      0.63        49\n",
      "       283.0       0.00      0.00      0.00         2\n",
      "       284.0       0.71      0.80      0.75        15\n",
      "       285.0       0.46      0.48      0.47        25\n",
      "       286.0       0.75      0.60      0.67        10\n",
      "       287.0       0.94      0.89      0.92       287\n",
      "       288.0       0.00      0.00      0.00         2\n",
      "       289.0       0.60      0.38      0.46         8\n",
      "       290.0       0.93      0.78      0.85       252\n",
      "       291.0       0.55      0.86      0.67         7\n",
      "       292.0       0.75      0.72      0.73       128\n",
      "       293.0       1.00      0.60      0.75         5\n",
      "       294.0       0.85      0.67      0.75        51\n",
      "       295.0       0.64      1.00      0.78        18\n",
      "       296.0       0.00      0.00      0.00         2\n",
      "       297.0       0.58      0.67      0.62        42\n",
      "       298.0       0.89      0.74      0.81       130\n",
      "       299.0       0.00      0.00      0.00         0\n",
      "       300.0       0.00      0.00      0.00         4\n",
      "       301.0       0.48      0.92      0.63        24\n",
      "       302.0       0.07      0.17      0.10         6\n",
      "       303.0       0.50      0.56      0.53         9\n",
      "       304.0       0.33      1.00      0.50         2\n",
      "       305.0       0.84      0.62      0.71        34\n",
      "       306.0       0.82      0.93      0.87        30\n",
      "       307.0       0.50      0.50      0.50         4\n",
      "       308.0       0.00      0.00      0.00         0\n",
      "       309.0       0.72      0.69      0.71        26\n",
      "       310.0       0.92      0.73      0.81        15\n",
      "       311.0       0.33      0.67      0.44         3\n",
      "       312.0       0.33      0.60      0.43         5\n",
      "       313.0       0.77      0.35      0.48        78\n",
      "       314.0       0.44      1.00      0.62         4\n",
      "       315.0       0.70      1.00      0.82         7\n",
      "       316.0       0.00      0.00      0.00         0\n",
      "       317.0       1.00      0.83      0.91        29\n",
      "       318.0       0.72      0.78      0.75       110\n",
      "       319.0       0.50      1.00      0.67         1\n",
      "       320.0       0.00      0.00      0.00         6\n",
      "       321.0       0.67      0.35      0.46        46\n",
      "       322.0       0.67      1.00      0.80         4\n",
      "       323.0       0.45      0.82      0.58        11\n",
      "       324.0       0.67      0.69      0.68        42\n",
      "       325.0       1.00      0.85      0.92        13\n",
      "       326.0       0.69      0.12      0.20        92\n",
      "       327.0       1.00      0.60      0.75        25\n",
      "       328.0       0.71      0.83      0.77         6\n",
      "       329.0       1.00      0.78      0.88         9\n",
      "       330.0       0.00      0.00      0.00         0\n",
      "       331.0       0.59      0.80      0.68        20\n",
      "       332.0       0.25      0.67      0.36         3\n",
      "       333.0       0.67      0.33      0.44        12\n",
      "       334.0       0.43      1.00      0.60         3\n",
      "       335.0       0.00      0.00      0.00         0\n",
      "       336.0       0.94      0.89      0.92        19\n",
      "       337.0       0.00      0.00      0.00         4\n",
      "       338.0       0.29      0.42      0.34        19\n",
      "       339.0       0.00      0.00      0.00         1\n",
      "       340.0       0.00      0.00      0.00         0\n",
      "       341.0       0.77      1.00      0.87        20\n",
      "       342.0       0.14      0.33      0.20         3\n",
      "       343.0       0.00      0.00      0.00         0\n",
      "       344.0       0.50      1.00      0.67         5\n",
      "       345.0       0.50      0.20      0.29         5\n",
      "       346.0       0.20      1.00      0.33         1\n",
      "       347.0       0.43      0.75      0.55         4\n",
      "       348.0       1.00      0.33      0.50         3\n",
      "       349.0       0.25      0.50      0.33         2\n",
      "       350.0       0.90      0.64      0.75        14\n",
      "       351.0       0.00      0.00      0.00         1\n",
      "       352.0       0.86      0.82      0.84        22\n",
      "       353.0       0.59      0.61      0.60        28\n",
      "       354.0       0.00      0.00      0.00         0\n",
      "       355.0       0.71      0.74      0.72        23\n",
      "       356.0       0.67      1.00      0.80         8\n",
      "       357.0       0.79      0.69      0.73        32\n",
      "       358.0       1.00      0.57      0.73         7\n",
      "       359.0       0.50      1.00      0.67         4\n",
      "       360.0       0.95      0.83      0.88       258\n",
      "       361.0       0.00      0.00      0.00         1\n",
      "       362.0       0.00      0.00      0.00         1\n",
      "       363.0       0.12      0.12      0.12         8\n",
      "       364.0       0.91      0.94      0.93       106\n",
      "       365.0       0.00      0.00      0.00         1\n",
      "       366.0       0.53      0.72      0.61        32\n",
      "       367.0       0.00      0.00      0.00         3\n",
      "       368.0       0.24      0.24      0.24        21\n",
      "       369.0       0.50      0.60      0.55         5\n",
      "       370.0       0.59      0.83      0.69        12\n",
      "       371.0       0.85      1.00      0.92        11\n",
      "       372.0       0.87      0.81      0.84        16\n",
      "       373.0       0.55      0.67      0.60         9\n",
      "       374.0       0.96      0.80      0.87        56\n",
      "       375.0       0.00      0.00      0.00         1\n",
      "       376.0       0.55      0.73      0.63        15\n",
      "       377.0       1.00      1.00      1.00         3\n",
      "       378.0       0.88      0.75      0.81        48\n",
      "       379.0       0.76      0.81      0.79        27\n",
      "       380.0       0.20      1.00      0.33         1\n",
      "       381.0       0.64      0.50      0.56        14\n",
      "       382.0       0.33      0.50      0.40         2\n",
      "       383.0       0.88      0.75      0.81       130\n",
      "       384.0       0.85      0.92      0.88        12\n",
      "       385.0       0.82      1.00      0.90         9\n",
      "       386.0       0.67      0.80      0.73         5\n",
      "       387.0       0.73      0.63      0.68       118\n",
      "       388.0       0.00      0.00      0.00         1\n",
      "       389.0       0.00      0.00      0.00         4\n",
      "       390.0       0.80      0.89      0.84         9\n",
      "       391.0       0.36      0.82      0.50        11\n",
      "       392.0       0.67      0.64      0.65        44\n",
      "       393.0       0.60      0.60      0.60         5\n",
      "       394.0       0.54      0.88      0.67         8\n",
      "       395.0       0.33      0.33      0.33         3\n",
      "       396.0       0.71      0.80      0.75        25\n",
      "       397.0       1.00      0.89      0.94         9\n",
      "       398.0       0.00      0.00      0.00         3\n",
      "       399.0       0.33      0.50      0.40         6\n",
      "       400.0       0.00      0.00      0.00         5\n",
      "       401.0       0.57      0.44      0.50        18\n",
      "       402.0       0.83      0.50      0.62        10\n",
      "       403.0       0.00      0.00      0.00         1\n",
      "       404.0       0.20      0.22      0.21         9\n",
      "       405.0       1.00      0.12      0.22        16\n",
      "       406.0       0.71      1.00      0.83         5\n",
      "       407.0       1.00      1.00      1.00        11\n",
      "       408.0       0.50      1.00      0.67         2\n",
      "       409.0       0.85      0.79      0.81        14\n",
      "       410.0       1.00      0.80      0.89         5\n",
      "       411.0       1.00      1.00      1.00        18\n",
      "       412.0       0.50      1.00      0.67         3\n",
      "       414.0       1.00      0.50      0.67         2\n",
      "       415.0       0.86      0.86      0.86         7\n",
      "       416.0       1.00      1.00      1.00         2\n",
      "       417.0       0.71      1.00      0.83         5\n",
      "       418.0       0.10      1.00      0.18         1\n",
      "       419.0       1.00      1.00      1.00         5\n",
      "       420.0       0.00      0.00      0.00         0\n",
      "       421.0       0.00      0.00      0.00         5\n",
      "       422.0       0.00      0.00      0.00         8\n",
      "       423.0       0.00      0.00      0.00         0\n",
      "       424.0       0.10      0.12      0.11        17\n",
      "       425.0       0.00      0.00      0.00       119\n",
      "       426.0       0.86      1.00      0.92         6\n",
      "       427.0       0.00      0.00      0.00         0\n",
      "       428.0       0.17      0.50      0.25         2\n",
      "       429.0       0.50      0.80      0.62         5\n",
      "       430.0       0.17      0.50      0.25         2\n",
      "       431.0       0.14      1.00      0.25         1\n",
      "       432.0       0.79      0.88      0.83        17\n",
      "       433.0       0.58      0.67      0.62        33\n",
      "       434.0       0.00      0.00      0.00         6\n",
      "       435.0       0.61      0.58      0.59        19\n",
      "       436.0       0.94      0.90      0.92       205\n",
      "       437.0       0.71      0.56      0.63         9\n",
      "       438.0       0.50      1.00      0.67         4\n",
      "       439.0       0.89      0.68      0.77        25\n",
      "       440.0       0.20      0.18      0.19        11\n",
      "       441.0       0.17      1.00      0.29         1\n",
      "       442.0       0.00      0.00      0.00         0\n",
      "       445.0       0.73      0.73      0.73        11\n",
      "       446.0       0.12      1.00      0.22         1\n",
      "       447.0       0.00      0.00      0.00         1\n",
      "       448.0       0.11      0.11      0.11         9\n",
      "\n",
      "    accuracy                           0.73     14387\n",
      "   macro avg       0.47      0.56      0.48     14387\n",
      "weighted avg       0.80      0.73      0.75     14387\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sklearn as sk\n",
    "print(sk.metrics.classification_report(y_pred, y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DGS-env",
   "language": "python",
   "name": "dgs-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
