{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Inférence de l'effet - Stratégie Multilabels - tableau des scores\n",
    "Dans ce Notebook, nous cosntruisons un modèle qui permet d'inférer l'EFFET à partir de la classification de l'incident et des données textuelles\n",
    "\n",
    "Nous considérons ce problème comme un problème de classification multiclasses et multilabels. En effet, il y a plusieurs effets possibles et un incidents peut entrainer plusieurs effets.\n",
    "\n",
    "Ainsi, notre métrique d'évaluation sera le f1_samples\n",
    "\n",
    "Dans le Notebook précedent, nous n'avions pas pris en compte l'aspect multilabel et notre score était de  f1_weighted = 0,28.\n",
    "\n",
    "Dans ce notebook, nous testons différents modèles :\n",
    "- SVM\n",
    "- XGboost\n",
    "- LSTM\n",
    "- NBSVM\n",
    "\n",
    "Et différents encodages : \n",
    "- TFIDF\n",
    "- countvectorizer\n",
    "\n",
    "Les scores sont résumé dans le tableau suivant : https://starclay-my.sharepoint.com/:x:/g/personal/rquillivic_starclay_fr/EZPS3DrBBQ9MrZskrcwKVAEBGsLY61W089kd8RFvIEirjg?e=ve9g9K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,HashingVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer,CountVectorizer\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, balanced_accuracy_score,f1_score,classification_report,recall_score,precision_score\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "\n",
    "import spacy\n",
    "nlp =spacy.load('fr')\n",
    "from spacy.lang.fr.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 7.63 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "train = pd.read_pickle('./data_split/train.pkl')\n",
    "# Pour faire un modèle sans le \n",
    "#train = train[~train['TEF_ID'].map(lambda x : 106 in x)]\n",
    "X_train = train[['FABRICANT','CLASSIFICATION','DESCRIPTION_INCIDENT','ETAT_PATIENT']]\n",
    "y_train = mlb.fit_transform(train['TEF_ID'])\n",
    "test =  pd.read_pickle('./data_split/test.pkl')\n",
    "#test = test[~test['TEF_ID'].map(lambda x : k in x)]\n",
    "X_test = test[['FABRICANT','CLASSIFICATION','DESCRIPTION_INCIDENT','ETAT_PATIENT']]\n",
    "y_test = mlb.transform(test['TEF_ID'])\n",
    "\n",
    "\n",
    "X_train_dgs = np.load('results/dgs_camenbert_train_vec.npy')\n",
    "X_test_dgs =np.load('results/dgs_camenbert_test_vec.npy')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_effets = pd.read_csv(\"data/ref_MRV/referentiel_dispositif_effets_connus.csv\",delimiter=';',encoding='ISO-8859-1')\n",
    "df_dys = pd.read_csv(\"data/ref_MRV/referentiel_dispositif_dysfonctionnement.csv\",delimiter=';',encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Construction du pipeline avec une stratégie ONE-VS-REST\n",
    "\n",
    "> \"This strategy, also known as one-vs-all, is implemented in OneVsRestClassifier. The strategy consists in fitting one classifier per class. For each classifier, the class is fitted against all the other classes. In addition to its computational efficiency (only n_classes classifiers are needed), one advantage of this approach is its interpretability. Since each class is represented by one and only one classifier, it is possible to gain knowledge about the class by inspecting its corresponding classifier. This is the most commonly used strategy and is a fair default choice.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 507 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "preprocess = ColumnTransformer(\n",
    "    [('description_tfidf',TfidfVectorizer(sublinear_tf=True, min_df=3,\n",
    "                            ngram_range=(1, 1),\n",
    "                            stop_words=STOP_WORDS,\n",
    "                            max_features = 10000,norm = 'l2'), 'DESCRIPTION_INCIDENT'),\n",
    "     \n",
    "     ('etat_pat_tfidf', TfidfVectorizer(sublinear_tf=True, min_df=3,ngram_range=(1, 1),\n",
    "                                       stop_words=STOP_WORDS,\n",
    "                                       max_features = 10000,norm = 'l2'), 'ETAT_PATIENT'),\n",
    "     \n",
    "     ('fabricant_tfidf',TfidfVectorizer(sublinear_tf=True, min_df=3,\n",
    "                            ngram_range=(1, 1),\n",
    "                            stop_words=STOP_WORDS,\n",
    "                            max_features = 5000,norm = 'l2'), 'FABRICANT')\n",
    "     ],\n",
    "    \n",
    "    remainder='passthrough')\n",
    "\n",
    "preprocess_2 = ColumnTransformer(\n",
    "    [('description_tfidf',CountVectorizer( min_df=3,\n",
    "                            ngram_range=(1, 1),\n",
    "                            stop_words=STOP_WORDS,\n",
    "                            max_features = 10000), 'DESCRIPTION_INCIDENT'),\n",
    "     \n",
    "     ('etat_pat_tfidf', CountVectorizer( min_df=3,ngram_range=(1, 1),\n",
    "                                       stop_words=STOP_WORDS,\n",
    "                                       max_features = 10000), 'ETAT_PATIENT'),\n",
    "     \n",
    "     ('fabricant_tfidf',CountVectorizer(min_df=3,\n",
    "                            ngram_range=(1, 1),\n",
    "                            stop_words=STOP_WORDS,\n",
    "                            max_features = 5000), 'FABRICANT')\n",
    "     ],\n",
    "    \n",
    "    remainder='passthrough')\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', preprocess),\n",
    "    ('clf', OneVsRestClassifier(LinearSVC(class_weight='balanced'))),\n",
    "])\n",
    "\n",
    "pipeline_2 = Pipeline([\n",
    "    ('vect', preprocess_2),\n",
    "    ('clf', OneVsRestClassifier(LinearSVC(class_weight='balanced'))),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score samples :  0.6378449926352557\n",
      "CPU times: user 1min 5s, sys: 700 ms, total: 1min 6s\n",
      "Wall time: 1min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "pipeline.fit(X_train,y_train)\n",
    "\n",
    "y_pred = pipeline.predict(X_test)\n",
    "f1 = f1_score(y_test , y_pred,average='samples')\n",
    "print('f1_score samples : ',f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.29      0.40         7\n",
      "           1       1.00      0.20      0.33         5\n",
      "           2       0.00      0.00      0.00         1\n",
      "           3       0.00      0.00      0.00         3\n",
      "           4       0.89      0.57      0.70        14\n",
      "           5       0.00      0.00      0.00         1\n",
      "           6       0.00      0.00      0.00        10\n",
      "           7       0.36      0.39      0.37        44\n",
      "           8       0.32      0.23      0.27        48\n",
      "           9       0.00      0.00      0.00         2\n",
      "          10       1.00      0.50      0.67         2\n",
      "          11       0.40      0.22      0.29         9\n",
      "          12       0.00      0.00      0.00        20\n",
      "          13       0.00      0.00      0.00         5\n",
      "          14       1.00      0.20      0.33        10\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.00      0.00      0.00         7\n",
      "          17       0.00      0.00      0.00         8\n",
      "          18       0.00      0.00      0.00         5\n",
      "          19       0.00      0.00      0.00         3\n",
      "          20       0.00      0.00      0.00         5\n",
      "          21       0.72      0.63      0.67        97\n",
      "          22       1.00      0.23      0.38        13\n",
      "          23       0.00      0.00      0.00         2\n",
      "          24       0.71      0.42      0.53        12\n",
      "          25       0.71      0.56      0.63        39\n",
      "          26       0.00      0.00      0.00         4\n",
      "          27       1.00      1.00      1.00         1\n",
      "          28       0.00      0.00      0.00         0\n",
      "          29       0.00      0.00      0.00         1\n",
      "          30       0.67      0.25      0.36         8\n",
      "          31       0.00      0.00      0.00         4\n",
      "          32       0.33      0.37      0.35        30\n",
      "          33       0.51      0.63      0.56        35\n",
      "          34       0.69      0.79      0.74        57\n",
      "          35       0.74      0.69      0.71        36\n",
      "          36       0.82      0.71      0.76        72\n",
      "          37       0.00      0.00      0.00         0\n",
      "          38       0.00      0.00      0.00         0\n",
      "          39       0.62      0.50      0.56        10\n",
      "          40       0.00      0.00      0.00         1\n",
      "          41       0.00      0.00      0.00         1\n",
      "          42       0.50      0.17      0.25        12\n",
      "          43       0.00      0.00      0.00         4\n",
      "          44       0.20      0.09      0.13        11\n",
      "          45       0.56      0.63      0.59       179\n",
      "          46       0.27      0.17      0.21        18\n",
      "          47       0.00      0.00      0.00         9\n",
      "          48       0.55      0.22      0.32        27\n",
      "          49       0.67      0.50      0.57         4\n",
      "          50       0.00      0.00      0.00         2\n",
      "          51       0.83      0.45      0.59        11\n",
      "          52       0.00      0.00      0.00         2\n",
      "          53       0.48      0.44      0.46        34\n",
      "          54       0.50      0.36      0.42        11\n",
      "          55       1.00      1.00      1.00         1\n",
      "          56       0.00      0.00      0.00         2\n",
      "          57       1.00      1.00      1.00         4\n",
      "          58       0.00      0.00      0.00         2\n",
      "          59       0.00      0.00      0.00         1\n",
      "          60       0.26      0.62      0.37        65\n",
      "          61       0.00      0.00      0.00         0\n",
      "          62       0.00      0.00      0.00         1\n",
      "          63       0.00      0.00      0.00         1\n",
      "          64       0.00      0.00      0.00         3\n",
      "          65       0.00      0.00      0.00         2\n",
      "          66       0.00      0.00      0.00         5\n",
      "          67       1.00      1.00      1.00         1\n",
      "          68       0.00      0.00      0.00         0\n",
      "          69       0.56      0.56      0.56         9\n",
      "          70       0.87      0.87      0.87        23\n",
      "          71       0.00      0.00      0.00         3\n",
      "          72       0.00      0.00      0.00         1\n",
      "          73       0.00      0.00      0.00         1\n",
      "          74       0.81      0.78      0.79        69\n",
      "          75       0.67      0.40      0.50        15\n",
      "          76       0.00      0.00      0.00         1\n",
      "          77       0.00      0.00      0.00         1\n",
      "          78       0.83      0.50      0.62        10\n",
      "          79       0.50      0.33      0.40         6\n",
      "          80       1.00      0.25      0.40         4\n",
      "          81       0.00      0.00      0.00         3\n",
      "          82       0.54      0.34      0.42        61\n",
      "          83       0.00      0.00      0.00         0\n",
      "          84       0.61      0.55      0.58        89\n",
      "          85       0.29      0.12      0.17        17\n",
      "          86       0.00      0.00      0.00         1\n",
      "          87       0.00      0.00      0.00         0\n",
      "          88       0.00      0.00      0.00         1\n",
      "          89       0.00      0.00      0.00         0\n",
      "          90       0.50      0.06      0.11        17\n",
      "          91       0.00      0.00      0.00         1\n",
      "          92       0.60      0.30      0.40        10\n",
      "          93       0.00      0.00      0.00         2\n",
      "          94       1.00      0.64      0.78        11\n",
      "          95       0.00      0.00      0.00         5\n",
      "          96       0.00      0.00      0.00         5\n",
      "          97       0.00      0.00      0.00         6\n",
      "          98       0.00      0.00      0.00         0\n",
      "          99       0.67      0.25      0.36         8\n",
      "         100       0.00      0.00      0.00         0\n",
      "         101       1.00      0.33      0.50         3\n",
      "         102       0.00      0.00      0.00         3\n",
      "         103       0.18      0.17      0.17        60\n",
      "         104       0.80      0.19      0.31        21\n",
      "         105       0.00      0.00      0.00         1\n",
      "         106       0.82      0.82      0.82      3633\n",
      "         107       0.00      0.00      0.00         1\n",
      "         108       0.43      0.19      0.26        16\n",
      "         109       0.00      0.00      0.00         2\n",
      "         110       1.00      1.00      1.00         3\n",
      "         111       1.00      0.33      0.50         3\n",
      "         112       0.00      0.00      0.00         1\n",
      "         113       0.61      0.62      0.62        90\n",
      "         114       0.00      0.00      0.00         0\n",
      "         115       0.00      0.00      0.00         1\n",
      "         116       1.00      0.33      0.50         3\n",
      "         117       1.00      0.38      0.55         8\n",
      "         118       0.00      0.00      0.00         1\n",
      "         119       0.00      0.00      0.00         1\n",
      "         120       0.00      0.00      0.00         0\n",
      "         121       0.00      0.00      0.00         1\n",
      "         122       0.00      0.00      0.00         1\n",
      "         123       0.84      0.79      0.81       141\n",
      "         124       0.40      0.29      0.33         7\n",
      "         125       0.00      0.00      0.00         2\n",
      "         126       1.00      0.33      0.50         3\n",
      "         127       0.00      0.00      0.00         0\n",
      "         128       0.00      0.00      0.00         3\n",
      "         129       0.37      0.27      0.31        41\n",
      "         130       0.31      0.39      0.34       269\n",
      "         131       0.00      0.00      0.00         1\n",
      "         132       0.27      0.15      0.19        27\n",
      "         133       0.58      0.71      0.64       204\n",
      "         134       0.00      0.00      0.00         2\n",
      "         135       0.54      0.47      0.50        15\n",
      "         136       0.00      0.00      0.00         2\n",
      "         137       1.00      0.50      0.67        12\n",
      "         138       0.50      0.14      0.22         7\n",
      "         139       1.00      0.67      0.80         6\n",
      "         140       1.00      0.33      0.50         3\n",
      "         141       0.00      0.00      0.00         1\n",
      "         142       0.89      0.89      0.89         9\n",
      "         143       0.24      0.12      0.16        57\n",
      "         144       0.00      0.00      0.00         2\n",
      "         145       0.00      0.00      0.00         3\n",
      "         146       0.67      0.67      0.67         3\n",
      "         147       0.00      0.00      0.00         3\n",
      "         148       0.00      0.00      0.00         2\n",
      "         149       0.00      0.00      0.00         0\n",
      "         150       0.00      0.00      0.00         2\n",
      "         151       0.00      0.00      0.00         1\n",
      "         152       0.00      0.00      0.00        12\n",
      "         153       0.88      0.98      0.93       435\n",
      "         154       0.50      0.11      0.18         9\n",
      "         155       0.42      0.74      0.53        38\n",
      "         156       0.00      0.00      0.00         1\n",
      "         157       0.00      0.00      0.00         0\n",
      "         158       1.00      1.00      1.00         1\n",
      "         159       0.79      0.59      0.68        32\n",
      "         160       1.00      0.25      0.40         4\n",
      "         161       0.00      0.00      0.00         2\n",
      "         162       0.31      0.57      0.40       230\n",
      "         163       0.74      0.50      0.60        40\n",
      "         164       0.00      0.00      0.00         0\n",
      "         165       1.00      0.33      0.50         3\n",
      "         166       0.00      0.00      0.00         0\n",
      "         167       0.00      0.00      0.00         0\n",
      "         168       0.64      0.42      0.51        50\n",
      "         169       0.60      0.78      0.68       223\n",
      "         170       0.65      0.89      0.75       281\n",
      "         171       0.61      0.60      0.61        96\n",
      "         172       0.58      0.68      0.62       115\n",
      "         173       0.57      0.69      0.62       128\n",
      "         174       0.62      0.38      0.47        55\n",
      "         175       0.53      0.68      0.60        99\n",
      "         176       0.67      0.88      0.76       248\n",
      "         177       0.60      0.35      0.44        26\n",
      "         178       0.00      0.00      0.00        10\n",
      "         179       0.55      0.69      0.61       207\n",
      "         180       0.80      0.31      0.44        13\n",
      "         181       0.86      0.32      0.46        19\n",
      "         182       0.00      0.00      0.00         0\n",
      "         183       1.00      0.20      0.33         5\n",
      "         184       0.47      0.26      0.33        27\n",
      "         185       0.00      0.00      0.00         8\n",
      "         186       0.55      0.51      0.53        77\n",
      "         187       0.43      0.47      0.45        74\n",
      "         188       0.72      0.48      0.58        44\n",
      "         189       0.70      0.32      0.44        22\n",
      "         190       0.00      0.00      0.00         5\n",
      "         191       0.44      0.22      0.30        18\n",
      "         192       0.00      0.00      0.00         7\n",
      "         193       0.64      0.54      0.58        13\n",
      "         194       0.00      0.00      0.00         2\n",
      "         195       0.46      0.27      0.34        22\n",
      "         196       0.42      0.42      0.42        33\n",
      "         197       0.46      0.63      0.53       315\n",
      "         198       0.39      0.29      0.33        56\n",
      "         199       0.67      0.67      0.67         3\n",
      "         200       0.33      0.29      0.31        89\n",
      "         201       0.67      0.22      0.33         9\n",
      "         202       1.00      0.25      0.40         4\n",
      "         203       0.00      0.00      0.00         1\n",
      "         204       0.00      0.00      0.00         1\n",
      "         205       0.00      0.00      0.00         1\n",
      "         206       0.00      0.00      0.00         0\n",
      "         207       0.55      0.26      0.35        23\n",
      "         208       0.71      0.26      0.38        19\n",
      "         209       1.00      0.21      0.35        14\n",
      "         210       0.67      0.29      0.40        14\n",
      "         211       0.00      0.00      0.00         6\n",
      "         212       0.00      0.00      0.00         1\n",
      "         213       0.00      0.00      0.00         1\n",
      "         214       0.00      0.00      0.00         2\n",
      "         215       0.00      0.00      0.00         2\n",
      "         216       0.00      0.00      0.00         1\n",
      "         217       0.00      0.00      0.00         1\n",
      "         218       0.00      0.00      0.00         0\n",
      "         219       0.00      0.00      0.00         3\n",
      "         220       1.00      0.67      0.80         3\n",
      "         221       0.00      0.00      0.00         1\n",
      "         222       0.00      0.00      0.00         2\n",
      "         223       0.00      0.00      0.00         2\n",
      "         224       0.00      0.00      0.00         0\n",
      "         225       0.00      0.00      0.00         3\n",
      "         226       0.00      0.00      0.00         1\n",
      "         227       0.00      0.00      0.00         0\n",
      "         228       0.00      0.00      0.00         0\n",
      "         229       0.00      0.00      0.00         0\n",
      "         230       0.00      0.00      0.00         0\n",
      "         231       0.00      0.00      0.00         0\n",
      "         232       0.00      0.00      0.00         0\n",
      "         233       0.00      0.00      0.00         1\n",
      "         234       0.00      0.00      0.00         0\n",
      "         235       0.00      0.00      0.00         3\n",
      "         236       1.00      0.20      0.33         5\n",
      "         237       0.33      0.20      0.25         5\n",
      "         238       0.00      0.00      0.00         1\n",
      "         239       1.00      0.50      0.67         2\n",
      "         240       0.00      0.00      0.00         1\n",
      "         241       0.00      0.00      0.00         1\n",
      "         242       0.00      0.00      0.00         3\n",
      "         243       0.00      0.00      0.00         0\n",
      "         244       0.00      0.00      0.00         0\n",
      "         245       0.00      0.00      0.00         1\n",
      "         246       0.00      0.00      0.00         1\n",
      "         247       0.00      0.00      0.00         0\n",
      "         248       0.00      0.00      0.00         0\n",
      "         249       0.00      0.00      0.00         1\n",
      "         250       0.00      0.00      0.00         0\n",
      "         251       1.00      1.00      1.00         1\n",
      "         252       0.00      0.00      0.00         2\n",
      "         253       0.00      0.00      0.00         5\n",
      "         254       0.00      0.00      0.00         0\n",
      "         255       0.00      0.00      0.00         1\n",
      "         256       0.00      0.00      0.00         1\n",
      "         257       0.00      0.00      0.00         0\n",
      "         258       1.00      0.20      0.33         5\n",
      "         259       0.00      0.00      0.00         0\n",
      "         260       0.00      0.00      0.00         3\n",
      "         261       0.10      0.33      0.15         3\n",
      "         262       0.00      0.00      0.00         0\n",
      "         263       0.00      0.00      0.00         1\n",
      "         264       0.40      0.17      0.24        12\n",
      "         265       0.00      0.00      0.00         0\n",
      "         266       1.00      0.50      0.67         2\n",
      "         267       0.00      0.00      0.00         0\n",
      "         268       0.00      0.00      0.00         2\n",
      "         269       0.00      0.00      0.00         4\n",
      "         270       0.00      0.00      0.00         0\n",
      "         271       0.33      0.25      0.29         4\n",
      "         272       1.00      0.67      0.80         3\n",
      "\n",
      "   micro avg       0.66      0.67      0.66      9500\n",
      "   macro avg       0.31      0.21      0.23      9500\n",
      "weighted avg       0.65      0.67      0.65      9500\n",
      " samples avg       0.63      0.69      0.64      9500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test , y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score samples :  0.5980593937810289\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.43      0.50         7\n",
      "           1       0.50      0.20      0.29         5\n",
      "           2       0.00      0.00      0.00         1\n",
      "           3       0.00      0.00      0.00         3\n",
      "           4       0.86      0.43      0.57        14\n",
      "           5       0.00      0.00      0.00         1\n",
      "           6       0.00      0.00      0.00        10\n",
      "           7       0.35      0.27      0.31        44\n",
      "           8       0.28      0.25      0.26        48\n",
      "           9       0.00      0.00      0.00         2\n",
      "          10       1.00      0.50      0.67         2\n",
      "          11       0.67      0.22      0.33         9\n",
      "          12       0.00      0.00      0.00        20\n",
      "          13       0.00      0.00      0.00         5\n",
      "          14       0.33      0.20      0.25        10\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.00      0.00      0.00         7\n",
      "          17       0.00      0.00      0.00         8\n",
      "          18       0.50      0.40      0.44         5\n",
      "          19       0.00      0.00      0.00         3\n",
      "          20       0.00      0.00      0.00         5\n",
      "          21       0.70      0.68      0.69        97\n",
      "          22       0.43      0.46      0.44        13\n",
      "          23       0.00      0.00      0.00         2\n",
      "          24       0.62      0.42      0.50        12\n",
      "          25       0.61      0.49      0.54        39\n",
      "          26       0.00      0.00      0.00         4\n",
      "          27       1.00      1.00      1.00         1\n",
      "          28       0.00      0.00      0.00         0\n",
      "          29       0.00      0.00      0.00         1\n",
      "          30       1.00      0.50      0.67         8\n",
      "          31       0.00      0.00      0.00         4\n",
      "          32       0.42      0.37      0.39        30\n",
      "          33       0.49      0.49      0.49        35\n",
      "          34       0.74      0.74      0.74        57\n",
      "          35       0.81      0.69      0.75        36\n",
      "          36       0.76      0.57      0.65        72\n",
      "          37       0.00      0.00      0.00         0\n",
      "          38       0.00      0.00      0.00         0\n",
      "          39       0.56      0.50      0.53        10\n",
      "          40       0.00      0.00      0.00         1\n",
      "          41       0.00      0.00      0.00         1\n",
      "          42       0.33      0.17      0.22        12\n",
      "          43       0.00      0.00      0.00         4\n",
      "          44       0.07      0.09      0.08        11\n",
      "          45       0.51      0.58      0.54       179\n",
      "          46       0.00      0.00      0.00        18\n",
      "          47       0.00      0.00      0.00         9\n",
      "          48       0.40      0.22      0.29        27\n",
      "          49       0.67      0.50      0.57         4\n",
      "          50       0.00      0.00      0.00         2\n",
      "          51       0.80      0.36      0.50        11\n",
      "          52       0.00      0.00      0.00         2\n",
      "          53       0.52      0.41      0.46        34\n",
      "          54       0.33      0.27      0.30        11\n",
      "          55       1.00      1.00      1.00         1\n",
      "          56       0.00      0.00      0.00         2\n",
      "          57       1.00      0.50      0.67         4\n",
      "          58       0.00      0.00      0.00         2\n",
      "          59       0.00      0.00      0.00         1\n",
      "          60       0.25      0.46      0.32        65\n",
      "          61       0.00      0.00      0.00         0\n",
      "          62       0.00      0.00      0.00         1\n",
      "          63       0.00      0.00      0.00         1\n",
      "          64       0.00      0.00      0.00         3\n",
      "          65       0.00      0.00      0.00         2\n",
      "          66       1.00      0.20      0.33         5\n",
      "          67       1.00      1.00      1.00         1\n",
      "          68       0.00      0.00      0.00         0\n",
      "          69       0.43      0.33      0.38         9\n",
      "          70       0.73      0.83      0.78        23\n",
      "          71       0.60      1.00      0.75         3\n",
      "          72       0.00      0.00      0.00         1\n",
      "          73       0.00      0.00      0.00         1\n",
      "          74       0.77      0.70      0.73        69\n",
      "          75       0.71      0.33      0.45        15\n",
      "          76       0.00      0.00      0.00         1\n",
      "          77       0.00      0.00      0.00         1\n",
      "          78       0.67      0.20      0.31        10\n",
      "          79       0.50      0.17      0.25         6\n",
      "          80       1.00      0.25      0.40         4\n",
      "          81       0.00      0.00      0.00         3\n",
      "          82       0.55      0.39      0.46        61\n",
      "          83       0.00      0.00      0.00         0\n",
      "          84       0.57      0.42      0.48        89\n",
      "          85       0.11      0.06      0.08        17\n",
      "          86       0.00      0.00      0.00         1\n",
      "          87       0.00      0.00      0.00         0\n",
      "          88       0.00      0.00      0.00         1\n",
      "          89       0.00      0.00      0.00         0\n",
      "          90       0.40      0.12      0.18        17\n",
      "          91       0.00      0.00      0.00         1\n",
      "          92       0.25      0.10      0.14        10\n",
      "          93       0.00      0.00      0.00         2\n",
      "          94       0.71      0.45      0.56        11\n",
      "          95       0.00      0.00      0.00         5\n",
      "          96       0.00      0.00      0.00         5\n",
      "          97       0.20      0.17      0.18         6\n",
      "          98       0.00      0.00      0.00         0\n",
      "          99       0.50      0.25      0.33         8\n",
      "         100       0.00      0.00      0.00         0\n",
      "         101       1.00      0.33      0.50         3\n",
      "         102       0.00      0.00      0.00         3\n",
      "         103       0.17      0.18      0.18        60\n",
      "         104       0.33      0.10      0.15        21\n",
      "         105       0.00      0.00      0.00         1\n",
      "         106       0.79      0.80      0.79      3633\n",
      "         107       0.00      0.00      0.00         1\n",
      "         108       0.17      0.06      0.09        16\n",
      "         109       0.00      0.00      0.00         2\n",
      "         110       1.00      0.67      0.80         3\n",
      "         111       1.00      0.33      0.50         3\n",
      "         112       0.00      0.00      0.00         1\n",
      "         113       0.56      0.53      0.55        90\n",
      "         114       0.00      0.00      0.00         0\n",
      "         115       0.00      0.00      0.00         1\n",
      "         116       1.00      0.33      0.50         3\n",
      "         117       1.00      0.38      0.55         8\n",
      "         118       0.00      0.00      0.00         1\n",
      "         119       0.00      0.00      0.00         1\n",
      "         120       0.00      0.00      0.00         0\n",
      "         121       0.00      0.00      0.00         1\n",
      "         122       0.00      0.00      0.00         1\n",
      "         123       0.77      0.75      0.76       141\n",
      "         124       0.33      0.14      0.20         7\n",
      "         125       0.00      0.00      0.00         2\n",
      "         126       1.00      0.33      0.50         3\n",
      "         127       0.00      0.00      0.00         0\n",
      "         128       0.00      0.00      0.00         3\n",
      "         129       0.33      0.20      0.25        41\n",
      "         130       0.29      0.36      0.32       269\n",
      "         131       0.00      0.00      0.00         1\n",
      "         132       0.27      0.11      0.16        27\n",
      "         133       0.55      0.58      0.57       204\n",
      "         134       0.00      0.00      0.00         2\n",
      "         135       0.46      0.40      0.43        15\n",
      "         136       0.00      0.00      0.00         2\n",
      "         137       1.00      0.42      0.59        12\n",
      "         138       0.67      0.29      0.40         7\n",
      "         139       0.67      0.33      0.44         6\n",
      "         140       1.00      0.33      0.50         3\n",
      "         141       0.00      0.00      0.00         1\n",
      "         142       0.80      0.89      0.84         9\n",
      "         143       0.12      0.07      0.09        57\n",
      "         144       0.00      0.00      0.00         2\n",
      "         145       0.00      0.00      0.00         3\n",
      "         146       0.67      0.67      0.67         3\n",
      "         147       0.00      0.00      0.00         3\n",
      "         148       0.00      0.00      0.00         2\n",
      "         149       0.00      0.00      0.00         0\n",
      "         150       0.00      0.00      0.00         2\n",
      "         151       0.00      0.00      0.00         1\n",
      "         152       0.00      0.00      0.00        12\n",
      "         153       0.88      0.93      0.90       435\n",
      "         154       0.50      0.11      0.18         9\n",
      "         155       0.38      0.53      0.44        38\n",
      "         156       0.00      0.00      0.00         1\n",
      "         157       0.00      0.00      0.00         0\n",
      "         158       1.00      1.00      1.00         1\n",
      "         159       0.79      0.47      0.59        32\n",
      "         160       1.00      0.25      0.40         4\n",
      "         161       0.00      0.00      0.00         2\n",
      "         162       0.25      0.40      0.31       230\n",
      "         163       0.53      0.47      0.50        40\n",
      "         164       0.00      0.00      0.00         0\n",
      "         165       1.00      0.33      0.50         3\n",
      "         166       0.00      0.00      0.00         0\n",
      "         167       0.00      0.00      0.00         0\n",
      "         168       0.53      0.42      0.47        50\n",
      "         169       0.61      0.65      0.62       223\n",
      "         170       0.64      0.70      0.67       281\n",
      "         171       0.60      0.55      0.57        96\n",
      "         172       0.60      0.62      0.61       115\n",
      "         173       0.57      0.62      0.59       128\n",
      "         174       0.46      0.29      0.36        55\n",
      "         175       0.55      0.53      0.54        99\n",
      "         176       0.70      0.75      0.72       248\n",
      "         177       0.35      0.27      0.30        26\n",
      "         178       0.50      0.10      0.17        10\n",
      "         179       0.54      0.56      0.55       207\n",
      "         180       0.50      0.08      0.13        13\n",
      "         181       0.71      0.26      0.38        19\n",
      "         182       0.00      0.00      0.00         0\n",
      "         183       1.00      0.20      0.33         5\n",
      "         184       0.50      0.33      0.40        27\n",
      "         185       1.00      0.12      0.22         8\n",
      "         186       0.48      0.39      0.43        77\n",
      "         187       0.47      0.49      0.48        74\n",
      "         188       0.53      0.41      0.46        44\n",
      "         189       0.31      0.18      0.23        22\n",
      "         190       0.00      0.00      0.00         5\n",
      "         191       0.55      0.33      0.41        18\n",
      "         192       0.00      0.00      0.00         7\n",
      "         193       0.75      0.46      0.57        13\n",
      "         194       0.00      0.00      0.00         2\n",
      "         195       0.50      0.23      0.31        22\n",
      "         196       0.41      0.42      0.42        33\n",
      "         197       0.42      0.54      0.47       315\n",
      "         198       0.27      0.23      0.25        56\n",
      "         199       0.50      0.33      0.40         3\n",
      "         200       0.21      0.17      0.19        89\n",
      "         201       0.25      0.11      0.15         9\n",
      "         202       0.50      0.25      0.33         4\n",
      "         203       0.00      0.00      0.00         1\n",
      "         204       0.00      0.00      0.00         1\n",
      "         205       0.00      0.00      0.00         1\n",
      "         206       0.00      0.00      0.00         0\n",
      "         207       0.54      0.30      0.39        23\n",
      "         208       0.62      0.26      0.37        19\n",
      "         209       0.33      0.21      0.26        14\n",
      "         210       0.50      0.21      0.30        14\n",
      "         211       0.00      0.00      0.00         6\n",
      "         212       0.00      0.00      0.00         1\n",
      "         213       0.00      0.00      0.00         1\n",
      "         214       0.00      0.00      0.00         2\n",
      "         215       0.00      0.00      0.00         2\n",
      "         216       0.00      0.00      0.00         1\n",
      "         217       0.00      0.00      0.00         1\n",
      "         218       0.00      0.00      0.00         0\n",
      "         219       0.50      0.33      0.40         3\n",
      "         220       0.50      0.33      0.40         3\n",
      "         221       0.00      0.00      0.00         1\n",
      "         222       0.00      0.00      0.00         2\n",
      "         223       0.00      0.00      0.00         2\n",
      "         224       0.00      0.00      0.00         0\n",
      "         225       1.00      0.33      0.50         3\n",
      "         226       0.00      0.00      0.00         1\n",
      "         227       0.00      0.00      0.00         0\n",
      "         228       0.00      0.00      0.00         0\n",
      "         229       0.00      0.00      0.00         0\n",
      "         230       0.00      0.00      0.00         0\n",
      "         231       0.00      0.00      0.00         0\n",
      "         232       0.00      0.00      0.00         0\n",
      "         233       0.00      0.00      0.00         1\n",
      "         234       0.00      0.00      0.00         0\n",
      "         235       0.00      0.00      0.00         3\n",
      "         236       0.50      0.20      0.29         5\n",
      "         237       0.25      0.20      0.22         5\n",
      "         238       0.00      0.00      0.00         1\n",
      "         239       1.00      0.50      0.67         2\n",
      "         240       0.00      0.00      0.00         1\n",
      "         241       0.00      0.00      0.00         1\n",
      "         242       0.00      0.00      0.00         3\n",
      "         243       0.00      0.00      0.00         0\n",
      "         244       0.00      0.00      0.00         0\n",
      "         245       0.00      0.00      0.00         1\n",
      "         246       0.00      0.00      0.00         1\n",
      "         247       0.00      0.00      0.00         0\n",
      "         248       0.00      0.00      0.00         0\n",
      "         249       0.00      0.00      0.00         1\n",
      "         250       0.00      0.00      0.00         0\n",
      "         251       1.00      1.00      1.00         1\n",
      "         252       0.00      0.00      0.00         2\n",
      "         253       0.00      0.00      0.00         5\n",
      "         254       0.00      0.00      0.00         0\n",
      "         255       0.00      0.00      0.00         1\n",
      "         256       0.00      0.00      0.00         1\n",
      "         257       0.00      0.00      0.00         0\n",
      "         258       0.00      0.00      0.00         5\n",
      "         259       0.00      0.00      0.00         0\n",
      "         260       0.00      0.00      0.00         3\n",
      "         261       0.00      0.00      0.00         3\n",
      "         262       0.00      0.00      0.00         0\n",
      "         263       0.00      0.00      0.00         1\n",
      "         264       0.40      0.17      0.24        12\n",
      "         265       0.00      0.00      0.00         0\n",
      "         266       0.50      0.50      0.50         2\n",
      "         267       0.00      0.00      0.00         0\n",
      "         268       0.00      0.00      0.00         2\n",
      "         269       0.00      0.00      0.00         4\n",
      "         270       0.00      0.00      0.00         0\n",
      "         271       0.00      0.00      0.00         4\n",
      "         272       1.00      0.67      0.80         3\n",
      "\n",
      "   micro avg       0.63      0.61      0.62      9500\n",
      "   macro avg       0.28      0.19      0.22      9500\n",
      "weighted avg       0.62      0.61      0.61      9500\n",
      " samples avg       0.59      0.64      0.60      9500\n",
      "\n",
      "CPU times: user 29.3 s, sys: 0 ns, total: 29.3 s\n",
      "Wall time: 29.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "pipeline_2.fit(X_train,y_train)\n",
    "\n",
    "y_pred = pipeline_2.predict(X_test)\n",
    "f1 = f1_score(y_test , y_pred,average='samples')\n",
    "print('f1_score samples : ',f1)\n",
    "print(classification_report(y_test , y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quelles sont les colonnes les plus importanes ?\n",
    "Nous cosntruisons un SVM pour chaqune des colonnes et observons les différents scores obtenues : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################\n",
      "FABRICANT\n",
      "f1_score samples :  0.18312606719785002\n",
      "##############################\n",
      "DESCRIPTION_INCIDENT\n",
      "f1_score samples :  0.5921288892501373\n",
      "##############################\n",
      "ETAT_PATIENT\n",
      "f1_score samples :  0.36551621481944147\n",
      "##############################\n",
      "ACTION_PATIENT\n",
      "f1_score samples :  0.26827221425301057\n"
     ]
    }
   ],
   "source": [
    "pipeline_col =Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(sublinear_tf=True, min_df=2,\n",
    "                            ngram_range=(1, 1),\n",
    "                            stop_words=STOP_WORDS,\n",
    "                            max_features = 10000,norm = 'l2')),\n",
    "    ('clf', OneVsRestClassifier(LinearSVC(class_weight='balanced'))),\n",
    "])\n",
    "PRED = []\n",
    "for col in ['FABRICANT','DESCRIPTION_INCIDENT','ETAT_PATIENT','ACTION_PATIENT'] :\n",
    "    x_train,x_test = X_train[col],X_test[col]\n",
    "    pipeline_col.fit(x_train,y_train)\n",
    "    pred= pipeline_col.predict(x_test)\n",
    "    PRED.append(pred)\n",
    "    f1 = f1_score(y_test , pred,average='samples')\n",
    "    print('##############################')\n",
    "    print(col)\n",
    "    print('f1_score samples : ',f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For threshold:  0.4\n",
      "Samples-average quality numbers\n",
      "Precision: 0.3606, Recall: 0.8025, F1-measure: 0.4411\n",
      "For threshold:  0.5\n",
      "Samples-average quality numbers\n",
      "Precision: 0.3606, Recall: 0.8025, F1-measure: 0.4411\n",
      "For threshold:  0.6\n",
      "Samples-average quality numbers\n",
      "Precision: 0.5238, Recall: 0.6161, F1-measure: 0.5393\n",
      "For threshold:  0.65\n",
      "Samples-average quality numbers\n",
      "Precision: 0.5238, Recall: 0.6161, F1-measure: 0.5393\n",
      "For threshold:  0.7\n",
      "Samples-average quality numbers\n",
      "Precision: 0.5238, Recall: 0.6161, F1-measure: 0.5393\n",
      "For threshold:  0.72\n",
      "Samples-average quality numbers\n",
      "Precision: 0.5238, Recall: 0.6161, F1-measure: 0.5393\n",
      "For threshold:  0.75\n",
      "Samples-average quality numbers\n",
      "Precision: 0.5238, Recall: 0.6161, F1-measure: 0.5393\n",
      "For threshold:  0.8\n",
      "Samples-average quality numbers\n",
      "Precision: 0.3151, Recall: 0.3113, F1-measure: 0.3059\n"
     ]
    }
   ],
   "source": [
    "y_e = np.mean(PRED,axis=0)\n",
    "thresholds = [0.4,0.5,0.6,0.65,0.7,0.72,0.75,0.8]\n",
    "for val in thresholds:\n",
    "    print(\"For threshold: \", val)\n",
    "    pred=y_e.copy()\n",
    "  \n",
    "    pred[pred>=val]=1\n",
    "    pred[pred<val]=0\n",
    "  \n",
    "    precision = precision_score(y_test, pred, average='samples')\n",
    "    recall = recall_score(y_test, pred, average='samples')\n",
    "    f1 = f1_score(y_test, pred, average='samples')\n",
    "   \n",
    "    print(\"Samples-average quality numbers\")\n",
    "    print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commentaire : \n",
    "\n",
    "La colonne DESCRIPTION_INCIDENT sempble de loin la plus importante en ce qui concerne la prédiction de l'effet.\n",
    "\n",
    " \n",
    "## 2.0 L'approche Multioutput\n",
    "\n",
    "> Multioutput classification support can be added to any classifier with MultiOutputClassifier. This strategy consists of fitting one classifier per target. This allows multiple target variable classifications. The purpose of this class is to extend estimators to be able to estimate a series of target functions (f1,f2,f3…,fn) that are trained on a single X predictor matrix to predict a series of responses (y1,y2,y3…,yn).\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score samples :  0.6378449926352557\n"
     ]
    }
   ],
   "source": [
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', preprocess),\n",
    "    ('clf', MultiOutputClassifier(LinearSVC(class_weight='balanced'))),\n",
    "])\n",
    "#### prédiction \n",
    "pipeline.fit(X_train,y_train)\n",
    "\n",
    "y_pred = pipeline.predict(X_test)\n",
    "f1 = f1_score(y_test , y_pred,average='samples')\n",
    "print('f1_score samples : ',f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commentaire\n",
    "Comme attendu, nous n'observons pas de grande différence car les deux approches sont très similaires\n",
    "\n",
    "## 2.1 Approche One vs One\n",
    "\n",
    ">This strategy consists in fitting one classifier per class pair. At prediction time, the class which received the most votes is selected. Since it requires to fit n_classes * (n_classes - 1) / 2 classifiers, this method is usually slower than one-vs-the-rest, due to its O(n_classes^2) complexity. However, this method may be advantageous for algorithms such as kernel algorithms which don’t scale well with n_samples. This is because each individual learning problem only involves a small subset of the data whereas, with one-vs-the-rest, the complete dataset is used n_classes times.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score samples :  0.6378449926352557\n",
      "CPU times: user 1min 10s, sys: 440 ms, total: 1min 10s\n",
      "Wall time: 1min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "pipeline = Pipeline([\n",
    "    ('vect', preprocess),\n",
    "    ('clf', MultiOutputClassifier(OneVsOneClassifier(LinearSVC(class_weight='balanced')))),\n",
    "])\n",
    "#### prédiction \n",
    "pipeline.fit(X_train,y_train)\n",
    "\n",
    "y_pred = pipeline.predict(X_test)\n",
    "f1 = f1_score(y_test , y_pred,average='samples')\n",
    "print('f1_score samples : ',f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commentaire\n",
    "Nous n'oservons pas de changement de performances, seulement une hausse du temps de calcul\n",
    "## 2.2 l'approche ClassifierChain\n",
    ">A multi-label model that arranges binary classifiers into a chain.\n",
    "Each model makes a prediction in the order specified by the chain using all of the available features provided to the model plus the predictions of models that are earlier in the chain.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multioutput import ClassifierChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6826849841667144\n",
      "CPU times: user 11min 43s, sys: 3.68 s, total: 11min 47s\n",
      "Wall time: 11min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train_, X_test_ =preprocess.fit_transform(X_train),preprocess.transform(X_test)\n",
    "clf = LinearSVC(class_weight='balanced')\n",
    "\n",
    "\n",
    "chains = [ClassifierChain(clf, order='random', random_state=i) for i in range(10)]\n",
    "\n",
    "for chain in chains:\n",
    "    chain.fit(X_train_, y_train)\n",
    "    \n",
    "y_pred_chains = np.array([chain.predict(X_test_) for chain in chains])\n",
    "\n",
    "chain_f1_scores = [f1_score(y_test, y_pred_chain, average='samples') for y_pred_chain in y_pred_chains]\n",
    "\n",
    "y_pred_ensemble = y_pred_chains.mean(axis=0)\n",
    "\n",
    "y_e = y_pred_ensemble>=0.4\n",
    "\n",
    "ensemble_f1_score = f1_score(y_test,y_e, average='samples')\n",
    "\n",
    "print(ensemble_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.29      0.40         7\n",
      "           1       1.00      0.20      0.33         5\n",
      "           2       0.00      0.00      0.00         1\n",
      "           3       0.00      0.00      0.00         3\n",
      "           4       0.73      0.57      0.64        14\n",
      "           5       0.00      0.00      0.00         1\n",
      "           6       0.00      0.00      0.00        10\n",
      "           7       0.39      0.48      0.43        44\n",
      "           8       0.33      0.29      0.31        48\n",
      "           9       0.00      0.00      0.00         2\n",
      "          10       1.00      0.50      0.67         2\n",
      "          11       0.33      0.22      0.27         9\n",
      "          12       0.00      0.00      0.00        20\n",
      "          13       0.00      0.00      0.00         5\n",
      "          14       0.50      0.20      0.29        10\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.00      0.00      0.00         7\n",
      "          17       0.00      0.00      0.00         8\n",
      "          18       0.00      0.00      0.00         5\n",
      "          19       0.00      0.00      0.00         3\n",
      "          20       0.00      0.00      0.00         5\n",
      "          21       0.75      0.66      0.70        97\n",
      "          22       0.75      0.23      0.35        13\n",
      "          23       0.00      0.00      0.00         2\n",
      "          24       0.67      0.50      0.57        12\n",
      "          25       0.69      0.69      0.69        39\n",
      "          26       0.00      0.00      0.00         4\n",
      "          27       1.00      1.00      1.00         1\n",
      "          28       0.00      0.00      0.00         0\n",
      "          29       0.00      0.00      0.00         1\n",
      "          30       0.50      0.25      0.33         8\n",
      "          31       0.00      0.00      0.00         4\n",
      "          32       0.38      0.40      0.39        30\n",
      "          33       0.46      0.63      0.53        35\n",
      "          34       0.69      0.81      0.74        57\n",
      "          35       0.74      0.69      0.71        36\n",
      "          36       0.83      0.69      0.76        72\n",
      "          37       0.00      0.00      0.00         0\n",
      "          38       0.00      0.00      0.00         0\n",
      "          39       0.67      0.60      0.63        10\n",
      "          40       0.00      0.00      0.00         1\n",
      "          41       0.00      0.00      0.00         1\n",
      "          42       0.33      0.17      0.22        12\n",
      "          43       0.00      0.00      0.00         4\n",
      "          44       0.20      0.09      0.13        11\n",
      "          45       0.54      0.64      0.59       179\n",
      "          46       0.30      0.17      0.21        18\n",
      "          47       0.00      0.00      0.00         9\n",
      "          48       0.55      0.22      0.32        27\n",
      "          49       0.75      0.75      0.75         4\n",
      "          50       0.00      0.00      0.00         2\n",
      "          51       0.83      0.45      0.59        11\n",
      "          52       0.00      0.00      0.00         2\n",
      "          53       0.47      0.47      0.47        34\n",
      "          54       0.40      0.18      0.25        11\n",
      "          55       1.00      1.00      1.00         1\n",
      "          56       0.00      0.00      0.00         2\n",
      "          57       1.00      1.00      1.00         4\n",
      "          58       0.00      0.00      0.00         2\n",
      "          59       1.00      1.00      1.00         1\n",
      "          60       0.26      0.71      0.38        65\n",
      "          61       0.00      0.00      0.00         0\n",
      "          62       0.00      0.00      0.00         1\n",
      "          63       0.00      0.00      0.00         1\n",
      "          64       0.00      0.00      0.00         3\n",
      "          65       0.00      0.00      0.00         2\n",
      "          66       0.00      0.00      0.00         5\n",
      "          67       1.00      1.00      1.00         1\n",
      "          68       0.00      0.00      0.00         0\n",
      "          69       0.56      0.56      0.56         9\n",
      "          70       0.87      0.87      0.87        23\n",
      "          71       0.00      0.00      0.00         3\n",
      "          72       0.00      0.00      0.00         1\n",
      "          73       0.00      0.00      0.00         1\n",
      "          74       0.79      0.81      0.80        69\n",
      "          75       0.83      0.33      0.48        15\n",
      "          76       0.00      0.00      0.00         1\n",
      "          77       0.00      0.00      0.00         1\n",
      "          78       0.83      0.50      0.62        10\n",
      "          79       0.40      0.33      0.36         6\n",
      "          80       1.00      0.25      0.40         4\n",
      "          81       0.00      0.00      0.00         3\n",
      "          82       0.51      0.34      0.41        61\n",
      "          83       0.00      0.00      0.00         0\n",
      "          84       0.56      0.57      0.57        89\n",
      "          85       0.50      0.18      0.26        17\n",
      "          86       0.00      0.00      0.00         1\n",
      "          87       0.00      0.00      0.00         0\n",
      "          88       0.00      0.00      0.00         1\n",
      "          89       0.00      0.00      0.00         0\n",
      "          90       0.33      0.06      0.10        17\n",
      "          91       0.00      0.00      0.00         1\n",
      "          92       0.60      0.30      0.40        10\n",
      "          93       0.00      0.00      0.00         2\n",
      "          94       1.00      0.64      0.78        11\n",
      "          95       0.00      0.00      0.00         5\n",
      "          96       0.00      0.00      0.00         5\n",
      "          97       0.00      0.00      0.00         6\n",
      "          98       0.00      0.00      0.00         0\n",
      "          99       0.67      0.25      0.36         8\n",
      "         100       0.00      0.00      0.00         0\n",
      "         101       1.00      0.33      0.50         3\n",
      "         102       0.00      0.00      0.00         3\n",
      "         103       0.17      0.18      0.18        60\n",
      "         104       0.80      0.19      0.31        21\n",
      "         105       0.00      0.00      0.00         1\n",
      "         106       0.78      0.89      0.83      3633\n",
      "         107       0.00      0.00      0.00         1\n",
      "         108       0.50      0.19      0.27        16\n",
      "         109       0.00      0.00      0.00         2\n",
      "         110       1.00      0.67      0.80         3\n",
      "         111       1.00      0.33      0.50         3\n",
      "         112       0.00      0.00      0.00         1\n",
      "         113       0.57      0.67      0.62        90\n",
      "         114       0.00      0.00      0.00         0\n",
      "         115       0.00      0.00      0.00         1\n",
      "         116       1.00      0.33      0.50         3\n",
      "         117       1.00      0.38      0.55         8\n",
      "         118       0.00      0.00      0.00         1\n",
      "         119       0.00      0.00      0.00         1\n",
      "         120       0.00      0.00      0.00         0\n",
      "         121       0.00      0.00      0.00         1\n",
      "         122       0.00      0.00      0.00         1\n",
      "         123       0.80      0.83      0.82       141\n",
      "         124       0.40      0.29      0.33         7\n",
      "         125       0.00      0.00      0.00         2\n",
      "         126       1.00      0.33      0.50         3\n",
      "         127       0.00      0.00      0.00         0\n",
      "         128       0.00      0.00      0.00         3\n",
      "         129       0.30      0.27      0.28        41\n",
      "         130       0.29      0.42      0.34       269\n",
      "         131       0.00      0.00      0.00         1\n",
      "         132       0.29      0.19      0.23        27\n",
      "         133       0.53      0.78      0.63       204\n",
      "         134       0.00      0.00      0.00         2\n",
      "         135       0.50      0.47      0.48        15\n",
      "         136       0.00      0.00      0.00         2\n",
      "         137       1.00      0.50      0.67        12\n",
      "         138       1.00      0.14      0.25         7\n",
      "         139       1.00      0.67      0.80         6\n",
      "         140       1.00      0.33      0.50         3\n",
      "         141       0.00      0.00      0.00         1\n",
      "         142       0.89      0.89      0.89         9\n",
      "         143       0.20      0.14      0.16        57\n",
      "         144       0.00      0.00      0.00         2\n",
      "         145       0.00      0.00      0.00         3\n",
      "         146       0.67      0.67      0.67         3\n",
      "         147       0.00      0.00      0.00         3\n",
      "         148       0.00      0.00      0.00         2\n",
      "         149       0.00      0.00      0.00         0\n",
      "         150       0.00      0.00      0.00         2\n",
      "         151       0.00      0.00      0.00         1\n",
      "         152       0.00      0.00      0.00        12\n",
      "         153       0.88      0.98      0.93       435\n",
      "         154       0.50      0.11      0.18         9\n",
      "         155       0.45      0.37      0.41        38\n",
      "         156       0.00      0.00      0.00         1\n",
      "         157       0.00      0.00      0.00         0\n",
      "         158       1.00      1.00      1.00         1\n",
      "         159       0.80      0.62      0.70        32\n",
      "         160       1.00      0.25      0.40         4\n",
      "         161       0.00      0.00      0.00         2\n",
      "         162       0.31      0.59      0.40       230\n",
      "         163       0.74      0.50      0.60        40\n",
      "         164       0.00      0.00      0.00         0\n",
      "         165       1.00      0.33      0.50         3\n",
      "         166       0.00      0.00      0.00         0\n",
      "         167       0.00      0.00      0.00         0\n",
      "         168       0.59      0.44      0.51        50\n",
      "         169       0.56      0.78      0.65       223\n",
      "         170       0.64      0.89      0.74       281\n",
      "         171       0.51      0.66      0.58        96\n",
      "         172       0.52      0.74      0.61       115\n",
      "         173       0.50      0.75      0.60       128\n",
      "         174       0.47      0.51      0.49        55\n",
      "         175       0.49      0.78      0.60        99\n",
      "         176       0.64      0.89      0.75       248\n",
      "         177       0.42      0.38      0.40        26\n",
      "         178       0.00      0.00      0.00        10\n",
      "         179       0.52      0.75      0.62       207\n",
      "         180       0.75      0.23      0.35        13\n",
      "         181       0.88      0.37      0.52        19\n",
      "         182       0.00      0.00      0.00         0\n",
      "         183       1.00      0.20      0.33         5\n",
      "         184       0.36      0.30      0.33        27\n",
      "         185       0.00      0.00      0.00         8\n",
      "         186       0.41      0.62      0.49        77\n",
      "         187       0.40      0.66      0.50        74\n",
      "         188       0.48      0.52      0.50        44\n",
      "         189       0.47      0.32      0.38        22\n",
      "         190       0.00      0.00      0.00         5\n",
      "         191       0.45      0.28      0.34        18\n",
      "         192       0.00      0.00      0.00         7\n",
      "         193       0.73      0.62      0.67        13\n",
      "         194       0.00      0.00      0.00         2\n",
      "         195       0.46      0.27      0.34        22\n",
      "         196       0.43      0.45      0.44        33\n",
      "         197       0.47      0.62      0.54       315\n",
      "         198       0.39      0.30      0.34        56\n",
      "         199       0.67      0.67      0.67         3\n",
      "         200       0.34      0.30      0.32        89\n",
      "         201       0.75      0.33      0.46         9\n",
      "         202       1.00      0.25      0.40         4\n",
      "         203       0.00      0.00      0.00         1\n",
      "         204       0.00      0.00      0.00         1\n",
      "         205       0.00      0.00      0.00         1\n",
      "         206       0.00      0.00      0.00         0\n",
      "         207       0.46      0.26      0.33        23\n",
      "         208       0.67      0.21      0.32        19\n",
      "         209       0.75      0.21      0.33        14\n",
      "         210       0.50      0.29      0.36        14\n",
      "         211       0.00      0.00      0.00         6\n",
      "         212       0.00      0.00      0.00         1\n",
      "         213       0.00      0.00      0.00         1\n",
      "         214       0.00      0.00      0.00         2\n",
      "         215       0.00      0.00      0.00         2\n",
      "         216       0.00      0.00      0.00         1\n",
      "         217       0.00      0.00      0.00         1\n",
      "         218       0.00      0.00      0.00         0\n",
      "         219       0.00      0.00      0.00         3\n",
      "         220       1.00      0.67      0.80         3\n",
      "         221       0.00      0.00      0.00         1\n",
      "         222       0.00      0.00      0.00         2\n",
      "         223       0.00      0.00      0.00         2\n",
      "         224       0.00      0.00      0.00         0\n",
      "         225       0.00      0.00      0.00         3\n",
      "         226       0.00      0.00      0.00         1\n",
      "         227       0.00      0.00      0.00         0\n",
      "         228       0.00      0.00      0.00         0\n",
      "         229       0.00      0.00      0.00         0\n",
      "         230       0.00      0.00      0.00         0\n",
      "         231       0.00      0.00      0.00         0\n",
      "         232       0.00      0.00      0.00         0\n",
      "         233       0.00      0.00      0.00         1\n",
      "         234       0.00      0.00      0.00         0\n",
      "         235       0.00      0.00      0.00         3\n",
      "         236       0.33      0.20      0.25         5\n",
      "         237       0.33      0.20      0.25         5\n",
      "         238       0.00      0.00      0.00         1\n",
      "         239       1.00      0.50      0.67         2\n",
      "         240       0.00      0.00      0.00         1\n",
      "         241       0.00      0.00      0.00         1\n",
      "         242       0.00      0.00      0.00         3\n",
      "         243       0.00      0.00      0.00         0\n",
      "         244       0.00      0.00      0.00         0\n",
      "         245       0.00      0.00      0.00         1\n",
      "         246       0.00      0.00      0.00         1\n",
      "         247       0.00      0.00      0.00         0\n",
      "         248       0.00      0.00      0.00         0\n",
      "         249       0.00      0.00      0.00         1\n",
      "         250       0.00      0.00      0.00         0\n",
      "         251       1.00      1.00      1.00         1\n",
      "         252       0.00      0.00      0.00         2\n",
      "         253       0.00      0.00      0.00         5\n",
      "         254       0.00      0.00      0.00         0\n",
      "         255       0.00      0.00      0.00         1\n",
      "         256       0.00      0.00      0.00         1\n",
      "         257       0.00      0.00      0.00         0\n",
      "         258       1.00      0.20      0.33         5\n",
      "         259       0.00      0.00      0.00         0\n",
      "         260       0.00      0.00      0.00         3\n",
      "         261       0.10      0.33      0.15         3\n",
      "         262       0.00      0.00      0.00         0\n",
      "         263       0.00      0.00      0.00         1\n",
      "         264       0.50      0.17      0.25        12\n",
      "         265       0.00      0.00      0.00         0\n",
      "         266       1.00      0.50      0.67         2\n",
      "         267       0.00      0.00      0.00         0\n",
      "         268       0.00      0.00      0.00         2\n",
      "         269       0.00      0.00      0.00         4\n",
      "         270       0.00      0.00      0.00         0\n",
      "         271       0.33      0.25      0.29         4\n",
      "         272       1.00      0.67      0.80         3\n",
      "\n",
      "   micro avg       0.63      0.71      0.67      9500\n",
      "   macro avg       0.30      0.22      0.24      9500\n",
      "weighted avg       0.62      0.71      0.65      9500\n",
      " samples avg       0.67      0.73      0.68      9500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,y_e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avec le count vectorizer ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6547455834998173\n",
      "CPU times: user 6min 10s, sys: 0 ns, total: 6min 10s\n",
      "Wall time: 6min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train_, X_test_ =preprocess_2.fit_transform(X_train),preprocess_2.transform(X_test)\n",
    "clf = LinearSVC(class_weight='balanced')\n",
    "\n",
    "\n",
    "chains = [ClassifierChain(clf, order='random', random_state=i) for i in range(10)]\n",
    "\n",
    "for chain in chains:\n",
    "    chain.fit(X_train_, y_train)\n",
    "    \n",
    "y_pred_chains = np.array([chain.predict(X_test_) for chain in chains])\n",
    "\n",
    "chain_f1_scores = [f1_score(y_test, y_pred_chain, average='samples') for y_pred_chain in y_pred_chains]\n",
    "\n",
    "y_pred_ensemble = y_pred_chains.mean(axis=0)\n",
    "\n",
    "y_e = y_pred_ensemble>=0.4\n",
    "\n",
    "ensemble_f1_score = f1_score(y_test,y_e, average='samples')\n",
    "\n",
    "print(ensemble_f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commentaire\n",
    "Nous observons un changement de performance, significatif, l'approche ClassifierChain permet de prendre en compte les lien entre différents Labels\n",
    "## 3.  D'autres modèle de Machine Learning\n",
    "### 3.1 XGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.06 s, sys: 20 ms, total: 3.08 s\n",
      "Wall time: 3.08 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import xgboost as xgb\n",
    "\n",
    "X_train_, X_test_ =preprocess.fit_transform(X_train),preprocess.transform(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing...\n",
      "Done !\n",
      "Fitting the model...\n",
      "Done !\n",
      "Prediction..\n",
      "Done !\n",
      "f1_score samples : 0.6559961687794019\n",
      "CPU times: user 15min 43s, sys: 3.86 s, total: 15min 47s\n",
      "Wall time: 15min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from xgboost import XGBClassifier\n",
    "#binary:hinge\n",
    "#Objective candidate: multi:softmax\n",
    "#Objective candidate: multi:softprob\n",
    "\n",
    "\n",
    "print(\"Preprocessing...\")\n",
    "X_train_, X_test_ =preprocess.fit_transform(X_train),preprocess.transform(X_test)\n",
    "print(\"Done !\")\n",
    "\n",
    "\n",
    "\n",
    "clf = OneVsRestClassifier(XGBClassifier(n_jobs=-1,eta= 0.1, max_depth=10,\n",
    "                                        n_estimators=10 ,objective ='binary:hinge'))\n",
    "\n",
    "print(\"Fitting the model...\")\n",
    "clf.fit(X_train_,y_train)\n",
    "print(\"Done !\")\n",
    "print(\"Prediction..\")\n",
    "pred = clf.predict(X_test_)\n",
    "print(\"Done !\")\n",
    "f1 = f1_score(y_test,pred, average='samples')\n",
    "print(\"f1_score samples :\",f1 )\n",
    "                                                                         \n",
    "                                                                         \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1.2 LGBM (A faire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,LSTM,Embedding,SpatialDropout1D, Bidirectional, Flatten, LSTM, Conv1D, Conv2D, MaxPooling1D, Dropout, Activation,GlobalMaxPool1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_, X_test_ =preprocess.fit_transform(X_train),preprocess.transform(X_test)\n",
    "X_train_= np.array(X_train_.todense())\n",
    "##\n",
    "X_test_= np.array(X_test_.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 48min 9s, sys: 21min 35s, total: 1h 9min 44s\n",
      "Wall time: 5min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=1000)\n",
    "X_train_ = svd.fit_transform(X_train_)\n",
    "X_test_ = svd.transform(X_test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ = np.reshape(X_train_, (X_train_.shape[0], 1, X_train_.shape[1]))\n",
    "X_test_ = np.reshape(X_test_, (X_test_.shape[0], 1, X_test_.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 21059 samples, validate on 5265 samples\n",
      "Epoch 1/5\n",
      "21059/21059 [==============================] - 11s 502us/step - loss: 0.0206 - categorical_accuracy: 0.5656 - val_loss: 0.0138 - val_categorical_accuracy: 0.5985\n",
      "Epoch 2/5\n",
      "21059/21059 [==============================] - 8s 381us/step - loss: 0.0150 - categorical_accuracy: 0.6196 - val_loss: 0.0125 - val_categorical_accuracy: 0.6370\n",
      "Epoch 3/5\n",
      "21059/21059 [==============================] - 9s 405us/step - loss: 0.0136 - categorical_accuracy: 0.6534 - val_loss: 0.0120 - val_categorical_accuracy: 0.6433\n",
      "Epoch 4/5\n",
      "21059/21059 [==============================] - 9s 427us/step - loss: 0.0127 - categorical_accuracy: 0.6658 - val_loss: 0.0117 - val_categorical_accuracy: 0.6473\n",
      "Epoch 5/5\n",
      "21059/21059 [==============================] - 9s 405us/step - loss: 0.0121 - categorical_accuracy: 0.6733 - val_loss: 0.0116 - val_categorical_accuracy: 0.6422\n",
      "6580/6580 [==============================] - 1s 111us/step\n",
      "loss :  0.012502517492869887\n",
      "categorical accuracy:  0.6524316072463989\n",
      "####################################\n",
      "For threshold:  0.01\n",
      "Samples-average quality numbers\n",
      "Precision: 0.2997, Recall: 0.9150, F1-measure: 0.4039\n",
      "For threshold:  0.04\n",
      "Samples-average quality numbers\n",
      "Precision: 0.5410, Recall: 0.8305, F1-measure: 0.6148\n",
      "For threshold:  0.06\n",
      "Samples-average quality numbers\n",
      "Precision: 0.5972, Recall: 0.7972, F1-measure: 0.6486\n",
      "For threshold:  0.08\n",
      "Samples-average quality numbers\n",
      "Precision: 0.6322, Recall: 0.7724, F1-measure: 0.6637\n",
      "For threshold:  0.1\n",
      "Samples-average quality numbers\n",
      "Precision: 0.6554, Recall: 0.7514, F1-measure: 0.6703\n",
      "For threshold:  0.12\n",
      "Samples-average quality numbers\n",
      "Precision: 0.6666, Recall: 0.7312, F1-measure: 0.6687\n",
      "For threshold:  0.14\n",
      "Samples-average quality numbers\n",
      "Precision: 0.6729, Recall: 0.7152, F1-measure: 0.6674\n",
      "For threshold:  0.16\n",
      "Samples-average quality numbers\n",
      "Precision: 0.6697, Recall: 0.6977, F1-measure: 0.6603\n",
      "For threshold:  0.2\n",
      "Samples-average quality numbers\n",
      "Precision: 0.6558, Recall: 0.6665, F1-measure: 0.6450\n",
      "For threshold:  0.25\n",
      "Samples-average quality numbers\n",
      "Precision: 0.6360, Recall: 0.6377, F1-measure: 0.6267\n",
      "For threshold:  0.3\n",
      "Samples-average quality numbers\n",
      "Precision: 0.6206, Recall: 0.6139, F1-measure: 0.6102\n",
      "For threshold:  0.35\n",
      "Samples-average quality numbers\n",
      "Precision: 0.6040, Recall: 0.5941, F1-measure: 0.5936\n",
      "For threshold:  0.4\n",
      "Samples-average quality numbers\n",
      "Precision: 0.5868, Recall: 0.5746, F1-measure: 0.5768\n",
      "For threshold:  0.5\n",
      "Samples-average quality numbers\n",
      "Precision: 0.5494, Recall: 0.5371, F1-measure: 0.5408\n",
      "For threshold:  0.6\n",
      "Samples-average quality numbers\n",
      "Precision: 0.5024, Recall: 0.4945, F1-measure: 0.4969\n",
      "For threshold:  0.7\n",
      "Samples-average quality numbers\n",
      "Precision: 0.4304, Recall: 0.4254, F1-measure: 0.4269\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(200))\n",
    "model.add(Dense(y_train.shape[1], activation='softmax'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n",
    "\n",
    "epochs = 5\n",
    "batch_size = 32\n",
    "\n",
    "history = model.fit(X_train_, y_train, epochs=epochs, batch_size=batch_size,validation_split=0.2)\n",
    "\n",
    "score,cat_acc = model.evaluate(X_test_,y_test)\n",
    "\n",
    "y_pred = model.predict(X_test_)\n",
    "\n",
    "print('loss : ', score)\n",
    "print('categorical accuracy: ',cat_acc)\n",
    "\n",
    "print('####################################')\n",
    "\n",
    "thresholds = [0.01,0.04,0.06,0.08,0.1,0.12,0.14,0.16,0.2,0.25,0.3,0.35,0.4,0.5,0.6,0.7]\n",
    "for val in thresholds:\n",
    "    print(\"For threshold: \", val)\n",
    "    pred=y_pred.copy()\n",
    "  \n",
    "    pred[pred>=val]=1\n",
    "    pred[pred<val]=0\n",
    "  \n",
    "    precision = precision_score(y_test, pred, average='samples')\n",
    "    recall = recall_score(y_test, pred, average='samples')\n",
    "    f1 = f1_score(y_test, pred, average='samples')\n",
    "   \n",
    "    print(\"Samples-average quality numbers\")\n",
    "    print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_7 (LSTM)                (None, 200)               960800    \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 273)               54873     \n",
      "=================================================================\n",
      "Total params: 1,015,673\n",
      "Trainable params: 1,015,673\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commentaire : \n",
    "En ajustant le seuil à 0.1, on obtient le meilleur résultat: \n",
    "\n",
    "**Samples-average quality numbers**\n",
    "- Precision: 0.6554, \n",
    "-  Recall: 0.7514, \n",
    "- F1-measure: 0.6703"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 nbSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbsvm import NBSVMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "X_train_, X_test_ =preprocess.fit_transform(X_train),preprocess.transform(X_test)\n",
    "X_train_= np.array(X_train_.todense())\n",
    "##\n",
    "X_test_= np.array(X_test_.todense())\n",
    "\n",
    "svd = TruncatedSVD(n_components=300)\n",
    "X_train_ = svd.fit_transform(X_train_)\n",
    "X_test_ = svd.transform(X_test_)\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train_ = scaler.fit_transform(X_train_)\n",
    "X_test_ = scaler.fit_transform(X_test_)\n",
    "\n",
    "clf = OneVsRestClassifier(NBSVMClassifier(class_weight='balanced'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#### prédiction \n",
    "clf.fit(X_train_,y_train)\n",
    "\n",
    "y_pred = pipeline.predict(X_test_)\n",
    "f1 = f1_score(y_test , y_pred,average='samples')\n",
    "print('f1_score samples : ',f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 K_train : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task: text classification\n",
      "language: fr\n",
      "Word Counts: 21168\n",
      "Nrows: 26324\n",
      "26324 train sequences\n",
      "train sequence lengths:\n",
      "\tmean : 12\n",
      "\t95percentile : 39\n",
      "\t99percentile : 83\n",
      "x_train shape: (26324,350)\n",
      "y_train shape: (26324, 273)\n",
      "Is Multi-Label? True\n",
      "6580 test sequences\n",
      "test sequence lengths:\n",
      "\tmean : 11\n",
      "\t95percentile : 37\n",
      "\t99percentile : 70\n",
      "x_test shape: (6580,350)\n",
      "y_test shape: (6580, 273)\n"
     ]
    }
   ],
   "source": [
    "import ktrain\n",
    "from ktrain import text\n",
    "encoder_TEF_ID = joblib.load('data_split/TEF_ID_encodeur.sav')\n",
    "\n",
    "features = ['ETAT_PATIENT','DESCRIPTION_INCIDENT']\n",
    "train_list = [elt[0] for elt in X_train[features].values.tolist()]\n",
    "test_list =  [elt[0] for elt in X_test[features].values.tolist()]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "trn, val, preproc = text.texts_from_array(x_train=train_list, y_train=y_train,\n",
    "                                          x_test=test_list, y_test=y_test,\n",
    "                                          class_names=encoder_TEF_ID.classes_.tolist(),\n",
    "                                          preprocess_mode='standard',maxlen=350)\n",
    "\n",
    "#t = text.Transformer(MODEL_NAME, maxlen=256)\n",
    "#trn = t.preprocess_train(train_list, y_train)\n",
    "#val = t.preprocess_test(test_list, y_test)\n",
    "#model = t.get_classifier('nbsvm', multilabel=True, class_names = encoder_TEF_ID.transform(encoder_TEF_ID.classes_))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Multi-Label? True\n",
      "compiling word ID features...\n",
      "maxlen is 350\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "model = text.text_classifier('nbsvm', train_data=trn, preproc=preproc,multilabel =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "class f1_Evaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            val=0.5\n",
    "            y_pred[y_pred>=val]=1\n",
    "            y_pred[y_pred<val]=0\n",
    "            score = f1_score(self.y_val, y_pred,average='samples')\n",
    "            print(\"\\n f1 samples - epoch: %d - score: %.6f \\n\" % (epoch+1, score))\n",
    "            \n",
    "f1 = f1_Evaluation(validation_data=val, interval=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "begin training using onecycle policy with max lr of 3e-05...\n",
      "Train on 26324 samples, validate on 6580 samples\n",
      "Epoch 1/4\n",
      "26270/26324 [============================>.] - ETA: 0s - loss: 0.6263 - accuracy: 0.6471\n",
      " f1 samples - epoch: 1 - score: 0.014942 \n",
      "\n",
      "26324/26324 [==============================] - 39s 1ms/sample - loss: 0.6260 - accuracy: 0.6475 - val_loss: 0.4845 - val_accuracy: 0.9272\n",
      "Epoch 2/4\n",
      "26290/26324 [============================>.] - ETA: 0s - loss: 0.2538 - accuracy: 0.9487\n",
      " f1 samples - epoch: 2 - score: 0.348176 \n",
      "\n",
      "26324/26324 [==============================] - 45s 2ms/sample - loss: 0.2537 - accuracy: 0.9487 - val_loss: 0.0694 - val_accuracy: 0.9951\n",
      "Epoch 3/4\n",
      "26290/26324 [============================>.] - ETA: 0s - loss: 0.0570 - accuracy: 0.9947\n",
      " f1 samples - epoch: 3 - score: 0.349797 \n",
      "\n",
      "26324/26324 [==============================] - 42s 2ms/sample - loss: 0.0570 - accuracy: 0.9947 - val_loss: 0.0276 - val_accuracy: 0.9951\n",
      "Epoch 4/4\n",
      "26290/26324 [============================>.] - ETA: 0s - loss: 0.0355 - accuracy: 0.9950\n",
      " f1 samples - epoch: 4 - score: 0.345137 \n",
      "\n",
      "26324/26324 [==============================] - 41s 2ms/sample - loss: 0.0355 - accuracy: 0.9950 - val_loss: 0.0242 - val_accuracy: 0.9951\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f70181a5210>"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner = ktrain.get_learner(model, train_data=trn, val_data=val, batch_size=10)\n",
    "learner.lr_plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion : \n",
    "\n",
    "Les différents tests que nous avons menés nous permettent de conclure :\n",
    "- Le SVM + TFIDF reste un modèle qui nous propose une baseline solide avec une facilité de mise en oeuvre et déploiement.\n",
    "- Le XGboost et le LSTM permettent d'améliorer légerement les performance (0.65 et respectivement 0.67). D'autant plus que ces modèle possède de nombreux hyperparamètres à finetuner. Gràce à la librairie Optuna, nous allons le faire dans la suite de notre travail.\n",
    "- Le meilleur résultat est obtenu en utilisant ClassifierChain, cela signifie qu'l existe des relations entre nos différents Label. En appliquant un mapping de regroupement, les autres modèle devrait pouvoir concurencer ce modèle. Nous testerons également cette hypothèse dans la suite.\n",
    "\n",
    "Notre travail d'exploration des modèles nous a permis d'augmenter significativement nos performances. Le premier modèle que nous avions fait (actuellement dans l'application) avait un score f1-sample de 0.59, notre meilleur modèle est possède aujourd'hui un f1 sample de 0.68. C'est encouragenat pour la suite car nous avons encore beaucoup de finetuning à réaliser.\n",
    "\n",
    "Nous avons également pu comprendre que l'encodage par colonne était un vecteur pour mieux capturer l'information et donc augmenter les performances. De même que la réalisation d'une SVD s'accompagne souvent d'une baisse de performances.\n",
    "\n",
    "Enfin, nous avons remarqué que les embedding préentrainé fonctionné mal sur notre problème en comparaison de la tfidf.\n",
    "\n",
    "En dehors de ce Notebook, nous avons essayé l'ensemble des modèles accecible en CPu de la librairie ktrain, malheuresement ils ne nous permette pas d'augmenter nos performances de manière significatives : https://github.com/amaiya/ktrain/tree/master/examples#textclass\n",
    "\n",
    "Les travaux à réaliser pour la suite sont : \n",
    "- Finetuner XgBoost (https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst)\n",
    "- Finetuner le LSTM\n",
    "- Créer des LSTM avec un encodage séparé pour chaqune des colonnes :\n",
    "    - https://keras.io/examples/nlp/text_classification_from_scratch/\n",
    "- Tester de nouvelles architectures (Gru, biGru, BiLSTM etc.)\n",
    "- Essayer de rajouter une couche d'attention sur nos modèles de deep Learning car dans la litterature, elle est souvent synonyme d'une augmentation des performances\n",
    "- Un travail sur les Loss est également nécessaire car nous travaillons avec un corpus très désequilibré : https://www.dlology.com/blog/multi-class-classification-with-focal-loss-for-imbalanced-datasets/\n",
    "- Enfin, nous allons essayer les méthodes développées dans ces deux papiers sur la classification de texte (Extreme classification basée sur l'attention) : \n",
    "    - https://github.com/iliaschalkidis/lmtc-eurlex57k\n",
    "    - https://github.com/yourh/AttentionXML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DGS-env",
   "language": "python",
   "name": "dgs-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
