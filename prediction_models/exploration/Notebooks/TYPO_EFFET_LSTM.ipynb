{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Inférence de l'effet - Stratégie Multilabels - Approche deep Learning\n",
    "Dans ce Notebook, nous cosntruisons un modèle qui permet d'inférer l'EFFET à partir de la classification de l'incident et des données textuelles en ce basant sur des reseau récurents commes GRU/LSTM etc.\n",
    "\n",
    "En effet, ces approches ont montré des réultats très encourageant et nous voulons explorer cette direction pour peut être faire des réceau recurent notre modèle par défault.\n",
    "\n",
    "Nous considérons ce problème comme un problème de classification multiclasses et multilabels. En effet, il y a plusieurs effets possibles et un incidents peut entrainer plusieurs effets.\n",
    "\n",
    "Dans ce note book nous nous posons les questions suivantes : \n",
    "- Quel est l'impact du drop out ?\n",
    "- Rajouter des couches augmentent-ils les performaces ?\n",
    "- L'utilisation de réseaux bidirectionnel est-elle pertinente ?\n",
    "- Une couche d'attention est-elle utile ?\n",
    "- Attention is all we need, really ?\n",
    "- Utilisation des embeddings \n",
    "- Concaténation des modèles sur différentes entrées ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,LSTM,Embedding,SpatialDropout1D, Bidirectional,SimpleRNN,Input, concatenate, Reshape\n",
    "import tensorflow \n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer\n",
    "from keras.layers import Concatenate, GlobalMaxPool1D, Dropout\n",
    "from tensorflow.keras.layers import Attention\n",
    "\n",
    "tensorflow.random.set_seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "train = pd.read_pickle('./data_split/train.pkl')\n",
    "# Pour faire un modèle sans le \n",
    "#train = train[~train['TEF_ID'].map(lambda x : 106 in x)]\n",
    "X_train = train[['FABRICANT','CLASSIFICATION','DESCRIPTION_INCIDENT','ETAT_PATIENT']]\n",
    "y_train = mlb.fit_transform(train['TEF_ID'])\n",
    "test =  pd.read_pickle('./data_split/test.pkl')\n",
    "#test = test[~test['TEF_ID'].map(lambda x : k in x)]\n",
    "X_test = test[['FABRICANT','CLASSIFICATION','DESCRIPTION_INCIDENT','ETAT_PATIENT']]\n",
    "y_test = mlb.transform(test['TEF_ID'])\n",
    "\n",
    "\n",
    "X_train_dgs = np.load('results/dgs_camenbert_train_vec.npy')\n",
    "X_test_dgs =np.load('results/dgs_camenbert_test_vec.npy')\n",
    "\n",
    "\n",
    "\n",
    "df_effets = pd.read_csv(\"data/ref_MRV/referentiel_dispositif_effets_connus.csv\",delimiter=';',encoding='ISO-8859-1')\n",
    "df_dys = pd.read_csv(\"data/ref_MRV/referentiel_dispositif_dysfonctionnement.csv\",delimiter=';',encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 LSTM et TFIDF, une première baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user1/DGS-env/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:386: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['qu', 'quelqu'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/home/user1/DGS-env/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:386: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['qu', 'quelqu'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/home/user1/DGS-env/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:386: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['qu', 'quelqu'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 55s, sys: 2min 52s, total: 5min 47s\n",
      "Wall time: 1min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,HashingVectorizer\n",
    "import spacy\n",
    "nlp =spacy.load('fr')\n",
    "from spacy.lang.fr.stop_words import STOP_WORDS\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    [('description_tfidf',TfidfVectorizer(sublinear_tf=True, min_df=3,\n",
    "                            ngram_range=(1, 1),\n",
    "                            stop_words=STOP_WORDS,\n",
    "                            max_features = 10000,norm = 'l2'), 'DESCRIPTION_INCIDENT'),\n",
    "     \n",
    "     ('etat_pat_tfidf', TfidfVectorizer(sublinear_tf=True, min_df=3,ngram_range=(1, 1),\n",
    "                                       stop_words=STOP_WORDS,\n",
    "                                       max_features = 10000,norm = 'l2'), 'ETAT_PATIENT'),\n",
    "     \n",
    "     ('fabricant_tfidf',TfidfVectorizer(sublinear_tf=True, min_df=3,\n",
    "                            ngram_range=(1, 1),\n",
    "                            stop_words=STOP_WORDS,\n",
    "                            max_features = 5000,norm = 'l2'), 'FABRICANT')\n",
    "     ],\n",
    "    \n",
    "    remainder='passthrough')\n",
    "\n",
    "X_train_, X_test_ =preprocess.fit_transform(X_train),preprocess.transform(X_test)\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=1000)\n",
    "X_train_ = svd.fit_transform(X_train_)\n",
    "X_test_ = svd.transform(X_test_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ = np.reshape(X_train_, (X_train_.shape[0], 1, X_train_.shape[1]))\n",
    "X_test_ = np.reshape(X_test_, (X_test_.shape[0], 1, X_test_.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 21059 samples, validate on 5265 samples\n",
      "Epoch 1/10\n",
      "21059/21059 [==============================] - 11s 531us/step - loss: 0.0205 - categorical_accuracy: 0.5677 - val_loss: 0.0137 - val_categorical_accuracy: 0.5991\n",
      "Epoch 2/10\n",
      "21059/21059 [==============================] - 10s 491us/step - loss: 0.0149 - categorical_accuracy: 0.6200 - val_loss: 0.0125 - val_categorical_accuracy: 0.6380\n",
      "Epoch 3/10\n",
      "21059/21059 [==============================] - 10s 480us/step - loss: 0.0136 - categorical_accuracy: 0.6497 - val_loss: 0.0120 - val_categorical_accuracy: 0.6422\n",
      "Epoch 4/10\n",
      "21059/21059 [==============================] - 10s 460us/step - loss: 0.0127 - categorical_accuracy: 0.6651 - val_loss: 0.0118 - val_categorical_accuracy: 0.6424\n",
      "Epoch 5/10\n",
      "21059/21059 [==============================] - 7s 356us/step - loss: 0.0121 - categorical_accuracy: 0.6727 - val_loss: 0.0116 - val_categorical_accuracy: 0.6477\n",
      "Epoch 6/10\n",
      "21059/21059 [==============================] - 9s 449us/step - loss: 0.0116 - categorical_accuracy: 0.6824 - val_loss: 0.0116 - val_categorical_accuracy: 0.6386\n",
      "Epoch 7/10\n",
      "21059/21059 [==============================] - 9s 420us/step - loss: 0.0112 - categorical_accuracy: 0.6892 - val_loss: 0.0115 - val_categorical_accuracy: 0.6425\n",
      "Epoch 8/10\n",
      "21059/21059 [==============================] - 8s 371us/step - loss: 0.0109 - categorical_accuracy: 0.6995 - val_loss: 0.0116 - val_categorical_accuracy: 0.6353\n",
      "Epoch 9/10\n",
      "21059/21059 [==============================] - 10s 477us/step - loss: 0.0106 - categorical_accuracy: 0.7060 - val_loss: 0.0116 - val_categorical_accuracy: 0.6374\n",
      "Epoch 10/10\n",
      "21059/21059 [==============================] - 11s 511us/step - loss: 0.0103 - categorical_accuracy: 0.7107 - val_loss: 0.0117 - val_categorical_accuracy: 0.6325\n",
      "6580/6580 [==============================] - 1s 137us/step\n",
      "loss :  0.012283515023763963\n",
      "categorical accuracy:  0.6472644209861755\n",
      "####################################\n",
      "For threshold:  0.01\n",
      "Samples-average quality numbers\n",
      "Precision: 0.3315, Recall: 0.9133, F1-measure: 0.4409\n",
      "For threshold:  0.04\n",
      "Samples-average quality numbers\n",
      "Precision: 0.5362, Recall: 0.8379, F1-measure: 0.6119\n",
      "For threshold:  0.06\n",
      "Samples-average quality numbers\n",
      "Precision: 0.5906, Recall: 0.8099, F1-measure: 0.6458\n",
      "For threshold:  0.08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user1/DGS-env/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples-average quality numbers\n",
      "Precision: 0.6239, Recall: 0.7872, F1-measure: 0.6618\n",
      "For threshold:  0.1\n",
      "Samples-average quality numbers\n",
      "Precision: 0.6483, Recall: 0.7660, F1-measure: 0.6700\n",
      "For threshold:  0.12\n",
      "Samples-average quality numbers\n",
      "Precision: 0.6640, Recall: 0.7496, F1-measure: 0.6730\n",
      "For threshold:  0.14\n",
      "Samples-average quality numbers\n",
      "Precision: 0.6724, Recall: 0.7332, F1-measure: 0.6718\n",
      "For threshold:  0.16\n",
      "Samples-average quality numbers\n",
      "Precision: 0.6736, Recall: 0.7159, F1-measure: 0.6673\n",
      "For threshold:  0.2\n",
      "Samples-average quality numbers\n",
      "Precision: 0.6647, Recall: 0.6844, F1-measure: 0.6540\n",
      "For threshold:  0.25\n",
      "Samples-average quality numbers\n",
      "Precision: 0.6487, Recall: 0.6560, F1-measure: 0.6386\n",
      "For threshold:  0.3\n",
      "Samples-average quality numbers\n",
      "Precision: 0.6376, Recall: 0.6332, F1-measure: 0.6258\n",
      "For threshold:  0.35\n",
      "Samples-average quality numbers\n",
      "Precision: 0.6224, Recall: 0.6096, F1-measure: 0.6090\n",
      "For threshold:  0.4\n",
      "Samples-average quality numbers\n",
      "Precision: 0.6089, Recall: 0.5928, F1-measure: 0.5955\n",
      "For threshold:  0.5\n",
      "Samples-average quality numbers\n",
      "Precision: 0.5714, Recall: 0.5545, F1-measure: 0.5596\n",
      "For threshold:  0.6\n",
      "Samples-average quality numbers\n",
      "Precision: 0.5271, Recall: 0.5143, F1-measure: 0.5182\n",
      "For threshold:  0.7\n",
      "Samples-average quality numbers\n",
      "Precision: 0.4693, Recall: 0.4601, F1-measure: 0.4629\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(200))\n",
    "model.add(Dense(y_train.shape[1], activation='softmax'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "history = model.fit(X_train_, y_train, epochs=epochs, batch_size=batch_size,validation_split=0.2)\n",
    "\n",
    "score,cat_acc = model.evaluate(X_test_,y_test)\n",
    "\n",
    "y_pred = model.predict(X_test_)\n",
    "\n",
    "print('loss : ', score)\n",
    "print('categorical accuracy: ',cat_acc)\n",
    "\n",
    "print('####################################')\n",
    "\n",
    "thresholds = [0.01,0.04,0.06,0.08,0.1,0.12,0.14,0.16,0.2,0.25,0.3,0.35,0.4,0.5,0.6,0.7]\n",
    "for val in thresholds:\n",
    "    print(\"For threshold: \", val)\n",
    "    pred=y_pred.copy()\n",
    "  \n",
    "    pred[pred>=val]=1\n",
    "    pred[pred<val]=0\n",
    "  \n",
    "    precision = precision_score(y_test, pred, average='samples')\n",
    "    recall = recall_score(y_test, pred, average='samples')\n",
    "    f1 = f1_score(y_test, pred, average='samples')\n",
    "   \n",
    "    print(\"Samples-average quality numbers\")\n",
    "    print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commentaires : \n",
    "\n",
    "Nous obtenons un score de base line de 0.6730. Nous allons essayer d'améliorer ce score avec difféntes approches : \n",
    "- Remplacer le tfidf par un embedding entrainé par le modèle \n",
    "    - Embedding simple\n",
    "    - Embedding par colonnes\n",
    "    \n",
    "- Essayer un modèle biLSTM ou GRU ou bi GRU\n",
    "- Essayer de rajouter une couche d'attention dans Keras\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Comment performe un reseau de BiLSTM  ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 21059 samples, validate on 5265 samples\n",
      "Epoch 1/5\n",
      "21059/21059 [==============================] - 17s 806us/step - loss: 0.0197 - categorical_accuracy: 0.5722 - val_loss: 0.0133 - val_categorical_accuracy: 0.6122\n",
      "Epoch 2/5\n",
      "21059/21059 [==============================] - 15s 711us/step - loss: 0.0146 - categorical_accuracy: 0.6302 - val_loss: 0.0123 - val_categorical_accuracy: 0.6414\n",
      "Epoch 3/5\n",
      "21059/21059 [==============================] - 15s 729us/step - loss: 0.0133 - categorical_accuracy: 0.6547 - val_loss: 0.0118 - val_categorical_accuracy: 0.6427\n",
      "Epoch 4/5\n",
      "21059/21059 [==============================] - 15s 691us/step - loss: 0.0125 - categorical_accuracy: 0.6640 - val_loss: 0.0115 - val_categorical_accuracy: 0.6475\n",
      "Epoch 5/5\n",
      "21059/21059 [==============================] - 15s 709us/step - loss: 0.0120 - categorical_accuracy: 0.6725 - val_loss: 0.0115 - val_categorical_accuracy: 0.6463\n",
      "6580/6580 [==============================] - 1s 196us/step\n",
      "loss :  0.012372263854986628\n",
      "categorical accuracy:  0.6585106253623962\n",
      "####################################\n",
      "For threshold:  0.01\n",
      "Samples-average quality numbers\n",
      "Precision: 0.3066, Recall: 0.9161, F1-measure: 0.4143\n",
      "For threshold:  0.04\n",
      "Samples-average quality numbers\n",
      "Precision: 0.5451, Recall: 0.8343, F1-measure: 0.6176\n",
      "For threshold:  0.06\n",
      "Samples-average quality numbers\n",
      "Precision: 0.6002, Recall: 0.8019, F1-measure: 0.6505\n",
      "For threshold:  0.08\n",
      "Samples-average quality numbers\n",
      "Precision: 0.6305, Recall: 0.7766, F1-measure: 0.6642\n",
      "For threshold:  0.1\n",
      "Samples-average quality numbers\n",
      "Precision: 0.6531, Recall: 0.7558, F1-measure: 0.6706\n",
      "For threshold:  0.12\n",
      "Samples-average quality numbers\n",
      "Precision: 0.6698, Recall: 0.7374, F1-measure: 0.6733\n",
      "For threshold:  0.14\n",
      "Samples-average quality numbers\n",
      "Precision: 0.6770, Recall: 0.7200, F1-measure: 0.6701\n",
      "For threshold:  0.16\n",
      "Samples-average quality numbers\n",
      "Precision: 0.6806, Recall: 0.7052, F1-measure: 0.6668\n",
      "For threshold:  0.2\n",
      "Samples-average quality numbers\n",
      "Precision: 0.6699, Recall: 0.6768, F1-measure: 0.6537\n",
      "For threshold:  0.25\n",
      "Samples-average quality numbers\n",
      "Precision: 0.6458, Recall: 0.6460, F1-measure: 0.6340\n",
      "For threshold:  0.3\n",
      "Samples-average quality numbers\n",
      "Precision: 0.6271, Recall: 0.6211, F1-measure: 0.6163\n",
      "For threshold:  0.35\n",
      "Samples-average quality numbers\n",
      "Precision: 0.6154, Recall: 0.6039, F1-measure: 0.6040\n",
      "For threshold:  0.4\n",
      "Samples-average quality numbers\n",
      "Precision: 0.5976, Recall: 0.5828, F1-measure: 0.5860\n",
      "For threshold:  0.5\n",
      "Samples-average quality numbers\n",
      "Precision: 0.5582, Recall: 0.5444, F1-measure: 0.5486\n",
      "For threshold:  0.6\n",
      "Samples-average quality numbers\n",
      "Precision: 0.5120, Recall: 0.5022, F1-measure: 0.5052\n",
      "For threshold:  0.7\n",
      "Samples-average quality numbers\n",
      "Precision: 0.4486, Recall: 0.4418, F1-measure: 0.4439\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(200, dropout=0.1)))\n",
    "model.add(Dense(y_train.shape[1], activation='softmax'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n",
    "\n",
    "epochs = 5\n",
    "batch_size = 32\n",
    "\n",
    "history = model.fit(X_train_, y_train, epochs=epochs, batch_size=batch_size,validation_split=0.2)\n",
    "\n",
    "score,cat_acc = model.evaluate(X_test_,y_test)\n",
    "\n",
    "y_pred = model.predict(X_test_)\n",
    "\n",
    "print('loss : ', score)\n",
    "print('categorical accuracy: ',cat_acc)\n",
    "\n",
    "print('####################################')\n",
    "\n",
    "thresholds = [0.01,0.04,0.06,0.08,0.1,0.12,0.14,0.16,0.2,0.25,0.3,0.35,0.4,0.5,0.6,0.7]\n",
    "for val in thresholds:\n",
    "    print(\"For threshold: \", val)\n",
    "    pred=y_pred.copy()\n",
    "  \n",
    "    pred[pred>=val]=1\n",
    "    pred[pred<val]=0\n",
    "  \n",
    "    precision = precision_score(y_test, pred, average='samples')\n",
    "    recall = recall_score(y_test, pred, average='samples')\n",
    "    f1 = f1_score(y_test, pred, average='samples')\n",
    "   \n",
    "    print(\"Samples-average quality numbers\")\n",
    "    print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commentaire ?\n",
    "\n",
    "L'augementation de la performance est très minime : 0.6733 ?\n",
    "Nous remarquons que notre modèle sur apprend assez rapidement, nous allons essayer d'ajouter une couche de dropout pour limiter ce sur apprentissage.\n",
    "## Si on ajoute du dropout ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 21059 samples, validate on 5265 samples\n",
      "Epoch 1/10\n",
      "21059/21059 [==============================] - 15s 732us/step - loss: 0.0186 - categorical_accuracy: 0.5732 - val_loss: 0.0136 - val_categorical_accuracy: 0.6118\n",
      "Epoch 2/10\n",
      "21059/21059 [==============================] - 14s 669us/step - loss: 0.0148 - categorical_accuracy: 0.6201 - val_loss: 0.0125 - val_categorical_accuracy: 0.6298\n",
      "Epoch 3/10\n",
      "21059/21059 [==============================] - 15s 696us/step - loss: 0.0137 - categorical_accuracy: 0.6484 - val_loss: 0.0122 - val_categorical_accuracy: 0.6349\n",
      "Epoch 4/10\n",
      "21059/21059 [==============================] - 14s 673us/step - loss: 0.0130 - categorical_accuracy: 0.6556 - val_loss: 0.0119 - val_categorical_accuracy: 0.6397\n",
      "Epoch 5/10\n",
      "21059/21059 [==============================] - 14s 656us/step - loss: 0.0125 - categorical_accuracy: 0.6635 - val_loss: 0.0119 - val_categorical_accuracy: 0.6410\n",
      "Epoch 6/10\n",
      "21059/21059 [==============================] - 13s 637us/step - loss: 0.0121 - categorical_accuracy: 0.6716 - val_loss: 0.0119 - val_categorical_accuracy: 0.6452\n",
      "Epoch 7/10\n",
      "21059/21059 [==============================] - 16s 771us/step - loss: 0.0118 - categorical_accuracy: 0.6755 - val_loss: 0.0118 - val_categorical_accuracy: 0.6378\n",
      "Epoch 8/10\n",
      "21059/21059 [==============================] - 15s 711us/step - loss: 0.0115 - categorical_accuracy: 0.6827 - val_loss: 0.0119 - val_categorical_accuracy: 0.6395\n",
      "Epoch 9/10\n",
      "21059/21059 [==============================] - 14s 676us/step - loss: 0.0112 - categorical_accuracy: 0.6880 - val_loss: 0.0119 - val_categorical_accuracy: 0.6414\n",
      "Epoch 10/10\n",
      "21059/21059 [==============================] - 14s 685us/step - loss: 0.0110 - categorical_accuracy: 0.6965 - val_loss: 0.0120 - val_categorical_accuracy: 0.6399\n",
      "6580/6580 [==============================] - 2s 319us/step\n",
      "loss :  0.012458362154051163\n",
      "categorical accuracy:  0.6588146090507507\n",
      "####################################\n",
      "For threshold:  0.01\n",
      "Samples-average quality numbers\n",
      "Precision: 0.3351, Recall: 0.9096, F1-measure: 0.4438\n",
      "For threshold:  0.04\n",
      "Samples-average quality numbers\n",
      "Precision: 0.5384, Recall: 0.8377, F1-measure: 0.6131\n",
      "For threshold:  0.06\n",
      "Samples-average quality numbers\n",
      "Precision: 0.5897, Recall: 0.8092, F1-measure: 0.6458\n",
      "For threshold:  0.08\n",
      "Samples-average quality numbers\n",
      "Precision: 0.6222, Recall: 0.7856, F1-measure: 0.6612\n",
      "For threshold:  0.1\n",
      "Samples-average quality numbers\n",
      "Precision: 0.6456, Recall: 0.7637, F1-measure: 0.6681\n",
      "For threshold:  0.12\n",
      "Samples-average quality numbers\n",
      "Precision: 0.6635, Recall: 0.7457, F1-measure: 0.6718\n",
      "For threshold:  0.14\n",
      "Samples-average quality numbers\n",
      "Precision: 0.6754, Recall: 0.7306, F1-measure: 0.6725\n",
      "For threshold:  0.16\n",
      "Samples-average quality numbers\n",
      "Precision: 0.6800, Recall: 0.7176, F1-measure: 0.6707\n",
      "For threshold:  0.2\n",
      "Samples-average quality numbers\n",
      "Precision: 0.6743, Recall: 0.6876, F1-measure: 0.6597\n",
      "For threshold:  0.25\n",
      "Samples-average quality numbers\n",
      "Precision: 0.6556, Recall: 0.6594, F1-measure: 0.6436\n",
      "For threshold:  0.3\n",
      "Samples-average quality numbers\n",
      "Precision: 0.6376, Recall: 0.6339, F1-measure: 0.6263\n",
      "For threshold:  0.35\n",
      "Samples-average quality numbers\n",
      "Precision: 0.6248, Recall: 0.6155, F1-measure: 0.6133\n",
      "For threshold:  0.4\n",
      "Samples-average quality numbers\n",
      "Precision: 0.6106, Recall: 0.5962, F1-measure: 0.5983\n",
      "For threshold:  0.5\n",
      "Samples-average quality numbers\n",
      "Precision: 0.5763, Recall: 0.5602, F1-measure: 0.5651\n",
      "For threshold:  0.6\n",
      "Samples-average quality numbers\n",
      "Precision: 0.5315, Recall: 0.5201, F1-measure: 0.5236\n",
      "For threshold:  0.7\n",
      "Samples-average quality numbers\n",
      "Precision: 0.4748, Recall: 0.4666, F1-measure: 0.4692\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(200, return_sequences=True, dropout=0.1)))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dense(100, activation=\"elu\"))\n",
    "model.add(Dropout(0.1))\n",
    "model.add( Dense(y_train.shape[1], activation=\"softmax\"))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n",
    "\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "history = model.fit(X_train_, y_train, epochs=epochs, batch_size=batch_size,validation_split=0.2)\n",
    "\n",
    "score,cat_acc = model.evaluate(X_test_,y_test)\n",
    "\n",
    "y_pred = model.predict(X_test_)\n",
    "\n",
    "print('loss : ', score)\n",
    "print('categorical accuracy: ',cat_acc)\n",
    "\n",
    "print('####################################')\n",
    "\n",
    "thresholds = [0.01,0.04,0.06,0.08,0.1,0.12,0.14,0.16,0.2,0.25,0.3,0.35,0.4,0.5,0.6,0.7]\n",
    "for val in thresholds:\n",
    "    print(\"For threshold: \", val)\n",
    "    pred=y_pred.copy()\n",
    "  \n",
    "    pred[pred>=val]=1\n",
    "    pred[pred<val]=0\n",
    "  \n",
    "    precision = precision_score(y_test, pred, average='samples')\n",
    "    recall = recall_score(y_test, pred, average='samples')\n",
    "    f1 = f1_score(y_test, pred, average='samples')\n",
    "   \n",
    "    print(\"Samples-average quality numbers\")\n",
    "    print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commentaire ?\n",
    "\n",
    "Les performances changent très peu,  nous allons essayer d'augementer la profondeur de notre reseau.\n",
    "\n",
    "## Si on ajoute plusieurs couches ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 21059 samples, validate on 5265 samples\n",
      "Epoch 1/10\n",
      "21059/21059 [==============================] - 23s 1ms/step - loss: 0.0181 - categorical_accuracy: 0.5780 - val_loss: 0.0132 - val_categorical_accuracy: 0.6114\n",
      "Epoch 2/10\n",
      "21059/21059 [==============================] - 23s 1ms/step - loss: 0.0144 - categorical_accuracy: 0.6289 - val_loss: 0.0122 - val_categorical_accuracy: 0.6452\n",
      "Epoch 3/10\n",
      "21059/21059 [==============================] - 22s 1ms/step - loss: 0.0134 - categorical_accuracy: 0.6505 - val_loss: 0.0120 - val_categorical_accuracy: 0.6380\n",
      "Epoch 4/10\n",
      "21059/21059 [==============================] - 23s 1ms/step - loss: 0.0128 - categorical_accuracy: 0.6605 - val_loss: 0.0117 - val_categorical_accuracy: 0.6391\n",
      "Epoch 5/10\n",
      "21059/21059 [==============================] - 21s 1ms/step - loss: 0.0123 - categorical_accuracy: 0.6672 - val_loss: 0.0117 - val_categorical_accuracy: 0.6416\n",
      "Epoch 6/10\n",
      "21059/21059 [==============================] - 23s 1ms/step - loss: 0.0119 - categorical_accuracy: 0.6723 - val_loss: 0.0117 - val_categorical_accuracy: 0.6431\n",
      "Epoch 7/10\n",
      "21059/21059 [==============================] - 22s 1ms/step - loss: 0.0116 - categorical_accuracy: 0.6775 - val_loss: 0.0118 - val_categorical_accuracy: 0.6370\n",
      "Epoch 8/10\n",
      "21059/21059 [==============================] - 22s 1ms/step - loss: 0.0113 - categorical_accuracy: 0.6852 - val_loss: 0.0117 - val_categorical_accuracy: 0.6334\n",
      "Epoch 9/10\n",
      "21059/21059 [==============================] - 20s 948us/step - loss: 0.0110 - categorical_accuracy: 0.6941 - val_loss: 0.0118 - val_categorical_accuracy: 0.6372\n",
      "Epoch 10/10\n",
      "21059/21059 [==============================] - 21s 992us/step - loss: 0.0107 - categorical_accuracy: 0.6983 - val_loss: 0.0119 - val_categorical_accuracy: 0.6391\n",
      "6580/6580 [==============================] - 2s 334us/step\n",
      "loss :  0.012482169690035216\n",
      "categorical accuracy:  0.6621580719947815\n",
      "####################################\n",
      "For threshold:  0.01\n",
      "Samples-average quality numbers\n",
      "Precision: 0.3609, Recall: 0.9061, F1-measure: 0.4667\n",
      "For threshold:  0.04\n",
      "Samples-average quality numbers\n",
      "Precision: 0.5465, Recall: 0.8341, F1-measure: 0.6191\n",
      "For threshold:  0.06\n",
      "Samples-average quality numbers\n",
      "Precision: 0.5947, Recall: 0.8050, F1-measure: 0.6480\n",
      "For threshold:  0.08\n",
      "Samples-average quality numbers\n",
      "Precision: 0.6285, Recall: 0.7825, F1-measure: 0.6637\n",
      "For threshold:  0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user1/DGS-env/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples-average quality numbers\n",
      "Precision: 0.6510, Recall: 0.7618, F1-measure: 0.6704\n",
      "For threshold:  0.12\n",
      "Samples-average quality numbers\n",
      "Precision: 0.6659, Recall: 0.7449, F1-measure: 0.6726\n",
      "For threshold:  0.14\n",
      "Samples-average quality numbers\n",
      "Precision: 0.6761, Recall: 0.7289, F1-measure: 0.6719\n",
      "For threshold:  0.16\n",
      "Samples-average quality numbers\n",
      "Precision: 0.6820, Recall: 0.7161, F1-measure: 0.6708\n",
      "For threshold:  0.2\n",
      "Samples-average quality numbers\n",
      "Precision: 0.6804, Recall: 0.6945, F1-measure: 0.6641\n",
      "For threshold:  0.25\n",
      "Samples-average quality numbers\n",
      "Precision: 0.6659, Recall: 0.6664, F1-measure: 0.6498\n",
      "For threshold:  0.3\n",
      "Samples-average quality numbers\n",
      "Precision: 0.6514, Recall: 0.6436, F1-measure: 0.6370\n",
      "For threshold:  0.35\n",
      "Samples-average quality numbers\n",
      "Precision: 0.6343, Recall: 0.6231, F1-measure: 0.6216\n",
      "For threshold:  0.4\n",
      "Samples-average quality numbers\n",
      "Precision: 0.6186, Recall: 0.6024, F1-measure: 0.6052\n",
      "For threshold:  0.5\n",
      "Samples-average quality numbers\n",
      "Precision: 0.5836, Recall: 0.5678, F1-measure: 0.5727\n",
      "For threshold:  0.6\n",
      "Samples-average quality numbers\n",
      "Precision: 0.5398, Recall: 0.5284, F1-measure: 0.5319\n",
      "For threshold:  0.7\n",
      "Samples-average quality numbers\n",
      "Precision: 0.4874, Recall: 0.4796, F1-measure: 0.4820\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "reg = regularizers.l2(1e-4)\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(256,  return_sequences=True, dropout=0.1)))\n",
    "model.add((SimpleRNN(128,return_sequences=True)))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(y_train.shape[1], activation=\"softmax\"))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n",
    "\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "history = model.fit(X_train_, y_train, epochs=epochs, batch_size=batch_size,validation_split=0.2)\n",
    "\n",
    "score,cat_acc = model.evaluate(X_test_,y_test)\n",
    "\n",
    "y_pred = model.predict(X_test_)\n",
    "\n",
    "print('loss : ', score)\n",
    "print('categorical accuracy: ',cat_acc)\n",
    "\n",
    "print('####################################')\n",
    "\n",
    "thresholds = [0.01,0.04,0.06,0.08,0.1,0.12,0.14,0.16,0.2,0.25,0.3,0.35,0.4,0.5,0.6,0.7]\n",
    "for val in thresholds:\n",
    "    print(\"For threshold: \", val)\n",
    "    pred=y_pred.copy()\n",
    "  \n",
    "    pred[pred>=val]=1\n",
    "    pred[pred<val]=0\n",
    "  \n",
    "    precision = precision_score(y_test, pred, average='samples')\n",
    "    recall = recall_score(y_test, pred, average='samples')\n",
    "    f1 = f1_score(y_test, pred, average='samples')\n",
    "   \n",
    "    print(\"Samples-average quality numbers\")\n",
    "    print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commentaires : \n",
    "\n",
    "Encore une fois, le modèle semble avoir atteint un plafond de verre qui ne soit pas dépassable ni par la prodondeur, ni par l'ajout de droptout. Nous allons essayer d'ajouter une couche d'attention.\n",
    "\n",
    "## Une couche d'attention ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 21059 samples, validate on 5265 samples\n",
      "Epoch 1/10\n",
      "21059/21059 [==============================] - 15s 693us/step - loss: 0.1118 - categorical_accuracy: 0.5384 - val_loss: 0.0176 - val_categorical_accuracy: 0.5597\n",
      "Epoch 2/10\n",
      "21059/21059 [==============================] - 13s 595us/step - loss: 0.0189 - categorical_accuracy: 0.5604 - val_loss: 0.0158 - val_categorical_accuracy: 0.5905\n",
      "Epoch 3/10\n",
      "21059/21059 [==============================] - 13s 612us/step - loss: 0.0155 - categorical_accuracy: 0.5836 - val_loss: 0.0150 - val_categorical_accuracy: 0.5928\n",
      "Epoch 4/10\n",
      "21059/21059 [==============================] - 10s 483us/step - loss: 0.0141 - categorical_accuracy: 0.5958 - val_loss: 0.0143 - val_categorical_accuracy: 0.6000\n",
      "Epoch 5/10\n",
      "21059/21059 [==============================] - 11s 543us/step - loss: 0.0131 - categorical_accuracy: 0.6110 - val_loss: 0.0136 - val_categorical_accuracy: 0.6160\n",
      "Epoch 6/10\n",
      "21059/21059 [==============================] - 14s 667us/step - loss: 0.0123 - categorical_accuracy: 0.6284 - val_loss: 0.0131 - val_categorical_accuracy: 0.6302\n",
      "Epoch 7/10\n",
      "21059/21059 [==============================] - 13s 613us/step - loss: 0.0117 - categorical_accuracy: 0.6459 - val_loss: 0.0128 - val_categorical_accuracy: 0.6427\n",
      "Epoch 8/10\n",
      "21059/21059 [==============================] - 12s 569us/step - loss: 0.0111 - categorical_accuracy: 0.6564 - val_loss: 0.0125 - val_categorical_accuracy: 0.6475\n",
      "Epoch 9/10\n",
      "21059/21059 [==============================] - 12s 559us/step - loss: 0.0106 - categorical_accuracy: 0.6648 - val_loss: 0.0123 - val_categorical_accuracy: 0.6509\n",
      "Epoch 10/10\n",
      "21059/21059 [==============================] - 14s 651us/step - loss: 0.0101 - categorical_accuracy: 0.6751 - val_loss: 0.0123 - val_categorical_accuracy: 0.6526\n",
      "6580/6580 [==============================] - 1s 100us/step\n",
      "loss :  0.011042395084643436\n",
      "categorical accuracy:  0.6550151705741882\n",
      "####################################\n",
      "For threshold:  0.01\n",
      "Samples-average quality numbers\n",
      "Precision: 0.2572, Recall: 0.9181, F1-measure: 0.3617\n",
      "For threshold:  0.04\n",
      "Samples-average quality numbers\n",
      "Precision: 0.5035, Recall: 0.8411, F1-measure: 0.5867\n",
      "For threshold:  0.06\n",
      "Samples-average quality numbers\n",
      "Precision: 0.5565, Recall: 0.8186, F1-measure: 0.6245\n",
      "For threshold:  0.08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user1/DGS-env/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples-average quality numbers\n",
      "Precision: 0.5878, Recall: 0.7989, F1-measure: 0.6441\n",
      "For threshold:  0.1\n",
      "Samples-average quality numbers\n",
      "Precision: 0.6065, Recall: 0.7834, F1-measure: 0.6544\n",
      "For threshold:  0.12\n",
      "Samples-average quality numbers\n",
      "Precision: 0.6206, Recall: 0.7694, F1-measure: 0.6608\n",
      "For threshold:  0.14\n",
      "Samples-average quality numbers\n",
      "Precision: 0.6306, Recall: 0.7564, F1-measure: 0.6640\n",
      "For threshold:  0.16\n",
      "Samples-average quality numbers\n",
      "Precision: 0.6375, Recall: 0.7449, F1-measure: 0.6654\n",
      "For threshold:  0.2\n",
      "Samples-average quality numbers\n",
      "Precision: 0.6485, Recall: 0.7260, F1-measure: 0.6664\n",
      "For threshold:  0.25\n",
      "Samples-average quality numbers\n",
      "Precision: 0.6559, Recall: 0.7057, F1-measure: 0.6643\n",
      "For threshold:  0.3\n",
      "Samples-average quality numbers\n",
      "Precision: 0.6561, Recall: 0.6853, F1-measure: 0.6570\n",
      "For threshold:  0.35\n",
      "Samples-average quality numbers\n",
      "Precision: 0.6523, Recall: 0.6675, F1-measure: 0.6481\n",
      "For threshold:  0.4\n",
      "Samples-average quality numbers\n",
      "Precision: 0.6455, Recall: 0.6490, F1-measure: 0.6365\n",
      "For threshold:  0.5\n",
      "Samples-average quality numbers\n",
      "Precision: 0.6172, Recall: 0.6053, F1-measure: 0.6018\n",
      "For threshold:  0.6\n",
      "Samples-average quality numbers\n",
      "Precision: 0.5759, Recall: 0.5536, F1-measure: 0.5554\n",
      "For threshold:  0.7\n",
      "Samples-average quality numbers\n",
      "Precision: 0.5150, Recall: 0.4853, F1-measure: 0.4902\n"
     ]
    }
   ],
   "source": [
    "from keras_self_attention import SeqSelfAttention\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(200, return_sequences=True))\n",
    "model.add(SeqSelfAttention(attention_activation='sigmoid'))\n",
    "model.add(keras.layers.Flatten(name='Flatten'))\n",
    "model.add(Dense(y_train.shape[1], activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n",
    "\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "history = model.fit(X_train_, y_train, epochs=epochs, batch_size=batch_size,validation_split=0.2)\n",
    "\n",
    "score,cat_acc = model.evaluate(X_test_,y_test)\n",
    "\n",
    "y_pred = model.predict(X_test_)\n",
    "\n",
    "print('loss : ', score)\n",
    "print('categorical accuracy: ',cat_acc)\n",
    "\n",
    "print('####################################')\n",
    "\n",
    "thresholds = [0.01,0.04,0.06,0.08,0.1,0.12,0.14,0.16,0.2,0.25,0.26,0.27,0.28,0.29,0.3,0.35,0.4,0.5,0.6,0.7]\n",
    "for val in thresholds:\n",
    "    print(\"For threshold: \", val)\n",
    "    pred=y_pred.copy()\n",
    "  \n",
    "    pred[pred>=val]=1\n",
    "    pred[pred<val]=0\n",
    "  \n",
    "    precision = precision_score(y_test, pred, average='samples')\n",
    "    recall = recall_score(y_test, pred, average='samples')\n",
    "    f1 = f1_score(y_test, pred, average='samples')\n",
    "   \n",
    "    print(\"Samples-average quality numbers\")\n",
    "    print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commentaire : \n",
    "Les résultats sembles similaire. Toutefois, la loss de validation est bien meilleur sans pour autant augmenter le f1 score. Nous allons essayer de faire seulement un réseau basé sur l'attention. Nous utilison ici, une verion multiplicative de l'attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 21059 samples, validate on 5265 samples\n",
      "Epoch 1/20\n",
      "21059/21059 [==============================] - 18s 838us/step - loss: 0.0393 - categorical_accuracy: 0.3177 - val_loss: 0.0231 - val_categorical_accuracy: 0.4809\n",
      "Epoch 2/20\n",
      "21059/21059 [==============================] - 14s 687us/step - loss: 0.0262 - categorical_accuracy: 0.5653 - val_loss: 0.0212 - val_categorical_accuracy: 0.5742\n",
      "Epoch 3/20\n",
      "21059/21059 [==============================] - 15s 725us/step - loss: 0.0240 - categorical_accuracy: 0.5950 - val_loss: 0.0205 - val_categorical_accuracy: 0.5867\n",
      "Epoch 4/20\n",
      "21059/21059 [==============================] - 13s 626us/step - loss: 0.0228 - categorical_accuracy: 0.6030 - val_loss: 0.0200 - val_categorical_accuracy: 0.5909\n",
      "Epoch 5/20\n",
      "21059/21059 [==============================] - 14s 667us/step - loss: 0.0217 - categorical_accuracy: 0.6062 - val_loss: 0.0198 - val_categorical_accuracy: 0.5992\n",
      "Epoch 6/20\n",
      "21059/21059 [==============================] - 13s 626us/step - loss: 0.0192 - categorical_accuracy: 0.6140 - val_loss: 0.0198 - val_categorical_accuracy: 0.6127\n",
      "Epoch 7/20\n",
      "21059/21059 [==============================] - 14s 683us/step - loss: 0.0185 - categorical_accuracy: 0.6244 - val_loss: 0.0197 - val_categorical_accuracy: 0.6201\n",
      "Epoch 8/20\n",
      "21059/21059 [==============================] - 14s 677us/step - loss: 0.0180 - categorical_accuracy: 0.6313 - val_loss: 0.0197 - val_categorical_accuracy: 0.6243\n",
      "Epoch 9/20\n",
      "21059/21059 [==============================] - 13s 620us/step - loss: 0.0178 - categorical_accuracy: 0.6363 - val_loss: 0.0198 - val_categorical_accuracy: 0.6285\n",
      "Epoch 10/20\n",
      "21059/21059 [==============================] - 14s 646us/step - loss: 0.0175 - categorical_accuracy: 0.6443 - val_loss: 0.0199 - val_categorical_accuracy: 0.6310\n",
      "Epoch 11/20\n",
      "21059/21059 [==============================] - 14s 664us/step - loss: 0.0172 - categorical_accuracy: 0.6488 - val_loss: 0.0200 - val_categorical_accuracy: 0.6332\n",
      "Epoch 12/20\n",
      "21059/21059 [==============================] - 14s 687us/step - loss: 0.0170 - categorical_accuracy: 0.6527 - val_loss: 0.0203 - val_categorical_accuracy: 0.6368\n",
      "Epoch 13/20\n",
      "21059/21059 [==============================] - 15s 700us/step - loss: 0.0168 - categorical_accuracy: 0.6590 - val_loss: 0.0204 - val_categorical_accuracy: 0.6410\n",
      "Epoch 14/20\n",
      "21059/21059 [==============================] - 15s 690us/step - loss: 0.0166 - categorical_accuracy: 0.6645 - val_loss: 0.0207 - val_categorical_accuracy: 0.6420\n",
      "Epoch 15/20\n",
      "21059/21059 [==============================] - 14s 680us/step - loss: 0.0165 - categorical_accuracy: 0.6675 - val_loss: 0.0210 - val_categorical_accuracy: 0.6439\n",
      "Epoch 16/20\n",
      "21059/21059 [==============================] - 15s 714us/step - loss: 0.0164 - categorical_accuracy: 0.6714 - val_loss: 0.0212 - val_categorical_accuracy: 0.6444\n",
      "Epoch 17/20\n",
      "21059/21059 [==============================] - 15s 693us/step - loss: 0.0163 - categorical_accuracy: 0.6708 - val_loss: 0.0213 - val_categorical_accuracy: 0.6452\n",
      "Epoch 18/20\n",
      "21059/21059 [==============================] - 14s 670us/step - loss: 0.0162 - categorical_accuracy: 0.6749 - val_loss: 0.0214 - val_categorical_accuracy: 0.6465\n",
      "Epoch 19/20\n",
      "21059/21059 [==============================] - 14s 642us/step - loss: 0.0161 - categorical_accuracy: 0.6752 - val_loss: 0.0216 - val_categorical_accuracy: 0.6463\n",
      "Epoch 20/20\n",
      "21059/21059 [==============================] - 16s 736us/step - loss: 0.0161 - categorical_accuracy: 0.6768 - val_loss: 0.0217 - val_categorical_accuracy: 0.6462\n",
      "6580/6580 [==============================] - 3s 482us/step\n",
      "loss :  0.02118711931047831\n",
      "categorical accuracy:  0.6486322283744812\n",
      "####################################\n",
      "For threshold:  0.01\n",
      "Samples-average quality numbers\n",
      "Precision: 0.4074, Recall: 0.8404, F1-measure: 0.5129\n",
      "For threshold:  0.04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user1/DGS-env/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples-average quality numbers\n",
      "Precision: 0.4750, Recall: 0.8283, F1-measure: 0.5668\n",
      "For threshold:  0.06\n",
      "Samples-average quality numbers\n",
      "Precision: 0.5086, Recall: 0.8210, F1-measure: 0.5922\n",
      "For threshold:  0.08\n",
      "Samples-average quality numbers\n",
      "Precision: 0.5357, Recall: 0.8101, F1-measure: 0.6108\n",
      "For threshold:  0.1\n",
      "Samples-average quality numbers\n",
      "Precision: 0.5608, Recall: 0.8012, F1-measure: 0.6276\n",
      "For threshold:  0.12\n",
      "Samples-average quality numbers\n",
      "Precision: 0.5825, Recall: 0.7924, F1-measure: 0.6414\n",
      "For threshold:  0.14\n",
      "Samples-average quality numbers\n",
      "Precision: 0.5994, Recall: 0.7840, F1-measure: 0.6515\n",
      "For threshold:  0.16\n",
      "Samples-average quality numbers\n",
      "Precision: 0.6130, Recall: 0.7728, F1-measure: 0.6577\n",
      "For threshold:  0.2\n",
      "Samples-average quality numbers\n",
      "Precision: 0.6318, Recall: 0.7507, F1-measure: 0.6638\n",
      "For threshold:  0.25\n",
      "Samples-average quality numbers\n",
      "Precision: 0.6495, Recall: 0.7273, F1-measure: 0.6678\n",
      "For threshold:  0.3\n",
      "Samples-average quality numbers\n",
      "Precision: 0.6542, Recall: 0.7011, F1-measure: 0.6620\n",
      "For threshold:  0.35\n",
      "Samples-average quality numbers\n",
      "Precision: 0.6546, Recall: 0.6784, F1-measure: 0.6535\n",
      "For threshold:  0.4\n",
      "Samples-average quality numbers\n",
      "Precision: 0.6481, Recall: 0.6553, F1-measure: 0.6403\n",
      "For threshold:  0.5\n",
      "Samples-average quality numbers\n",
      "Precision: 0.6079, Recall: 0.5925, F1-measure: 0.5905\n",
      "For threshold:  0.6\n",
      "Samples-average quality numbers\n",
      "Precision: 0.5237, Recall: 0.4982, F1-measure: 0.5010\n",
      "For threshold:  0.7\n",
      "Samples-average quality numbers\n",
      "Precision: 0.4129, Recall: 0.3802, F1-measure: 0.3858\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras_self_attention import SeqSelfAttention\n",
    "\n",
    "inputs = keras.layers.Input(shape=(1,1000))\n",
    "#lstm = keras.layers.Bidirectional(keras.layers.LSTM(units=256,\n",
    "                                                    #return_sequences=True))(inputs)\n",
    "att = SeqSelfAttention(attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL,\n",
    "                       kernel_regularizer=keras.regularizers.l2(1e-4),\n",
    "                       bias_regularizer=keras.regularizers.l1(1e-4),\n",
    "                       attention_regularizer_weight=1e-4,\n",
    "                       name='Attention')(inputs)\n",
    "\n",
    "flatten = keras.layers.Flatten(name='Flatten')(att)\n",
    "dense = keras.layers.Dense(units=y_train.shape[1], name='Dense')(flatten)\n",
    "\n",
    "model = keras.models.Model(inputs=inputs, outputs=[dense])\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n",
    "\n",
    "\n",
    "epochs = 20\n",
    "batch_size = 32\n",
    "\n",
    "history = model.fit(X_train_, y_train, epochs=epochs, batch_size=batch_size,validation_split=0.2)\n",
    "\n",
    "score,cat_acc = model.evaluate(X_test_,y_test)\n",
    "\n",
    "y_pred = model.predict(X_test_)\n",
    "\n",
    "print('loss : ', score)\n",
    "print('categorical accuracy: ',cat_acc)\n",
    "\n",
    "print('####################################')\n",
    "\n",
    "thresholds = [0.01,0.04,0.06,0.08,0.1,0.12,0.14,0.16,0.2,0.25,0.3,0.35,0.4,0.5,0.6,0.7]\n",
    "for val in thresholds:\n",
    "    print(\"For threshold: \", val)\n",
    "    pred=y_pred.copy()\n",
    "  \n",
    "    pred[pred>=val]=1\n",
    "    pred[pred<val]=0\n",
    "  \n",
    "    precision = precision_score(y_test, pred, average='samples')\n",
    "    recall = recall_score(y_test, pred, average='samples')\n",
    "    f1 = f1_score(y_test, pred, average='samples')\n",
    "   \n",
    "    print(\"Samples-average quality numbers\")\n",
    "    print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commentaire : \n",
    "\n",
    "Le modèle semble réellement bloqué a une valeure seuil de 0.67. Que ce soit des réseau recurent ou bien des mécanismes d'attention le score reste identique après 10 epochs. Nous allons donc changer de représentation car nous pensons avoir atteint les limite de la représentation en tfidf+SVD.\n",
    "\n",
    "## Et si l'on changait de représentation ? Utilisation d'Embedding globaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = train['DESCRIPTION_INCIDENT']+'. '+train['ETAT_PATIENT']#+ '. '+train['ACTION_PATIENT']\n",
    "df_test = test['DESCRIPTION_INCIDENT']+'. '+test['ETAT_PATIENT']#+'. '+test['ACTION_PATIENT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61213\n",
      "(26324, 300) (6580, 300)\n"
     ]
    }
   ],
   "source": [
    "# The maximum number of words to be used. \n",
    "MAX_NB_WORDS = 100000\n",
    "# Max number of words in each complaint.\n",
    "MAX_SEQUENCE_LENGTH = 300\n",
    "# This is fixed.\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(df_train.values)\n",
    "word_index = tokenizer.word_index\n",
    "print(len(word_index))\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(df_train.values)\n",
    "X_test = tokenizer.texts_to_sequences(df_test.values)\n",
    "word2index_inputs =  tokenizer.word_index\n",
    "\n",
    "X_train = pad_sequences(X_train,MAX_SEQUENCE_LENGTH)\n",
    "X_test = pad_sequences(X_test,MAX_SEQUENCE_LENGTH)\n",
    "print(X_train.shape,X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user1/DGS-env/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 21059 samples, validate on 5265 samples\n",
      "Epoch 1/10\n",
      "21059/21059 [==============================] - 641s 30ms/step - loss: 0.0197 - categorical_accuracy: 0.5669 - val_loss: 0.0154 - val_categorical_accuracy: 0.5932\n",
      "Epoch 2/10\n",
      "21059/21059 [==============================] - 620s 29ms/step - loss: 0.0170 - categorical_accuracy: 0.5896 - val_loss: 0.0149 - val_categorical_accuracy: 0.5953\n",
      "Epoch 3/10\n",
      "21059/21059 [==============================] - 722s 34ms/step - loss: 0.0164 - categorical_accuracy: 0.5963 - val_loss: 0.0148 - val_categorical_accuracy: 0.5947\n",
      "Epoch 4/10\n",
      "21059/21059 [==============================] - 696s 33ms/step - loss: 0.0160 - categorical_accuracy: 0.5967 - val_loss: 0.0146 - val_categorical_accuracy: 0.5888\n",
      "Epoch 5/10\n",
      "21059/21059 [==============================] - 751s 36ms/step - loss: 0.0157 - categorical_accuracy: 0.5984 - val_loss: 0.0146 - val_categorical_accuracy: 0.5793\n",
      "Epoch 6/10\n",
      "21059/21059 [==============================] - 766s 36ms/step - loss: 0.0154 - categorical_accuracy: 0.5977 - val_loss: 0.0146 - val_categorical_accuracy: 0.5835\n",
      "Epoch 7/10\n",
      "21059/21059 [==============================] - 740s 35ms/step - loss: 0.0153 - categorical_accuracy: 0.5998 - val_loss: 0.0146 - val_categorical_accuracy: 0.5734\n",
      "Epoch 8/10\n",
      "21059/21059 [==============================] - 830s 39ms/step - loss: 0.0151 - categorical_accuracy: 0.5971 - val_loss: 0.0145 - val_categorical_accuracy: 0.5816\n",
      "Epoch 9/10\n",
      "21059/21059 [==============================] - 766s 36ms/step - loss: 0.0150 - categorical_accuracy: 0.6023 - val_loss: 0.0147 - val_categorical_accuracy: 0.5816\n",
      "Epoch 10/10\n",
      "21059/21059 [==============================] - 775s 37ms/step - loss: 0.0148 - categorical_accuracy: 0.6023 - val_loss: 0.0147 - val_categorical_accuracy: 0.5816\n",
      "loss :  0.02118711931047831\n",
      "categorical accuracy:  0.6486322283744812\n",
      "####################################\n",
      "For threshold:  0.01\n",
      "Samples-average quality numbers\n",
      "Precision: 0.1751, Recall: 0.8631, F1-measure: 0.2634\n",
      "For threshold:  0.04\n",
      "Samples-average quality numbers\n",
      "Precision: 0.3960, Recall: 0.7556, F1-measure: 0.4813\n",
      "For threshold:  0.06\n",
      "Samples-average quality numbers\n",
      "Precision: 0.4845, Recall: 0.7199, F1-measure: 0.5450\n",
      "For threshold:  0.08\n",
      "Samples-average quality numbers\n",
      "Precision: 0.5434, Recall: 0.6839, F1-measure: 0.5747\n",
      "For threshold:  0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user1/DGS-env/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples-average quality numbers\n",
      "Precision: 0.5695, Recall: 0.6641, F1-measure: 0.5887\n",
      "For threshold:  0.12\n",
      "Samples-average quality numbers\n",
      "Precision: 0.5949, Recall: 0.6427, F1-measure: 0.5941\n",
      "For threshold:  0.14\n",
      "Samples-average quality numbers\n",
      "Precision: 0.5973, Recall: 0.6260, F1-measure: 0.5883\n",
      "For threshold:  0.16\n",
      "Samples-average quality numbers\n",
      "Precision: 0.5529, Recall: 0.5958, F1-measure: 0.5659\n",
      "For threshold:  0.2\n",
      "Samples-average quality numbers\n",
      "Precision: 0.5599, Recall: 0.5831, F1-measure: 0.5654\n",
      "For threshold:  0.25\n",
      "Samples-average quality numbers\n",
      "Precision: 0.5540, Recall: 0.5694, F1-measure: 0.5568\n",
      "For threshold:  0.3\n",
      "Samples-average quality numbers\n",
      "Precision: 0.5498, Recall: 0.5525, F1-measure: 0.5480\n",
      "For threshold:  0.35\n",
      "Samples-average quality numbers\n",
      "Precision: 0.5301, Recall: 0.5268, F1-measure: 0.5266\n",
      "For threshold:  0.4\n",
      "Samples-average quality numbers\n",
      "Precision: 0.5084, Recall: 0.5036, F1-measure: 0.5046\n",
      "For threshold:  0.5\n",
      "Samples-average quality numbers\n",
      "Precision: 0.4217, Recall: 0.4200, F1-measure: 0.4206\n",
      "For threshold:  0.6\n",
      "Samples-average quality numbers\n",
      "Precision: 0.3567, Recall: 0.3561, F1-measure: 0.3563\n",
      "For threshold:  0.7\n",
      "Samples-average quality numbers\n",
      "Precision: 0.2669, Recall: 0.2664, F1-measure: 0.2666\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X_train.shape[1]))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(200))\n",
    "model.add(Dense(y_train.shape[1],activation='softmax'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,validation_split=0.2)\n",
    "\n",
    "\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print('loss : ', score)\n",
    "print('categorical accuracy: ',cat_acc)\n",
    "\n",
    "print('####################################')\n",
    "\n",
    "thresholds = [0.01,0.04,0.06,0.08,0.1,0.12,0.14,0.16,0.2,0.25,0.3,0.35,0.4,0.5,0.6,0.7]\n",
    "for val in thresholds:\n",
    "    print(\"For threshold: \", val)\n",
    "    pred=y_pred.copy()\n",
    "  \n",
    "    pred[pred>=val]=1\n",
    "    pred[pred<val]=0\n",
    "  \n",
    "    precision = precision_score(y_test, pred, average='samples')\n",
    "    recall = recall_score(y_test, pred, average='samples')\n",
    "    f1 = f1_score(y_test, pred, average='samples')\n",
    "   \n",
    "    print(\"Samples-average quality numbers\")\n",
    "    print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commentaire : \n",
    "Les embedding appris n'offre pas de meilleurs résultats pour l'instant. Nous allons essayer de créer des embeddings par colonnes de texte, cette solution c'etait avérée fructueuse dans le cas du tfidf.\n",
    "\n",
    "## Et si l'on séparait les entrées ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57288\n",
      "21168\n",
      "2262\n"
     ]
    }
   ],
   "source": [
    "# Model constants.\n",
    "EMBEDDING_DIM =300\n",
    "MAX_SEQUENCE_LENGTH =300\n",
    "MAX_NB_WORDS = 50000\n",
    "\n",
    "def vectorize(df_train,df_test,MAX_NB_WORDS,MAX_SEQUENCE_LENGTH ):\n",
    "    tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "    tokenizer.fit_on_texts(df_train.values)\n",
    "    word_index = tokenizer.word_index\n",
    "    print(len(word_index))\n",
    "\n",
    "    X_train = tokenizer.texts_to_sequences(df_train.values)\n",
    "    X_test = tokenizer.texts_to_sequences(df_test.values)\n",
    "    word2index_inputs =  tokenizer.word_index\n",
    "\n",
    "    X_train = pad_sequences(X_train,MAX_SEQUENCE_LENGTH)\n",
    "    X_test = pad_sequences(X_test,MAX_SEQUENCE_LENGTH)\n",
    "    return (X_train, X_test)\n",
    "TRAIN = []\n",
    "for col in ['DESCRIPTION_INCIDENT', 'ETAT_PATIENT', 'FABRICANT'] : \n",
    "    X_train,X_test = vectorize(train[col],test[col],MAX_NB_WORDS,MAX_SEQUENCE_LENGTH )\n",
    "    TRAIN.append((X_train,X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_14\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_67 (InputLayer)           (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_68 (InputLayer)           (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_69 (InputLayer)           (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_15 (Embedding)        (None, 300, 300)     15000000    input_67[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_16 (Embedding)        (None, 300, 300)     15000000    input_68[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_17 (Embedding)        (None, 300, 300)     15000000    input_69[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_14 (SpatialDr (None, 300, 300)     0           embedding_15[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_15 (SpatialDr (None, 300, 300)     0           embedding_16[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_16 (SpatialDr (None, 300, 300)     0           embedding_17[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lstm_189 (LSTM)                 (None, 200)          400800      spatial_dropout1d_14[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "lstm_190 (LSTM)                 (None, 200)          400800      spatial_dropout1d_15[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "lstm_191 (LSTM)                 (None, 200)          400800      spatial_dropout1d_16[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 600)          0           lstm_189[0][0]                   \n",
      "                                                                 lstm_190[0][0]                   \n",
      "                                                                 lstm_191[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_151 (Dense)               (None, 273)          164073      concatenate_5[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 46,366,473\n",
      "Trainable params: 46,366,473\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs_1 = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "inputs_2 = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "inputs_3 = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "#x = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH)(inputs)\n",
    "#x = Reshape((MAX_SEQUENCE_LENGTH,1,))(inputs_1)\n",
    "x = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH)(inputs_1)\n",
    "x = SpatialDropout1D(0.2)(x)\n",
    "x = LSTM(200)(x)\n",
    "\n",
    "\n",
    "#y = Reshape((MAX_SEQUENCE_LENGTH,1,))(inputs_2)\n",
    "y = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH)(inputs_2)\n",
    "y = SpatialDropout1D(0.2)(y)\n",
    "y = LSTM(200)(y)\n",
    "\n",
    "\n",
    "#z = Reshape((MAX_SEQUENCE_LENGTH,1,))(inputs_3)\n",
    "z = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH)(inputs_3)\n",
    "z = SpatialDropout1D(0.2)(z)\n",
    "z = LSTM(200)(z)\n",
    "\n",
    "\n",
    "w = concatenate([x, y, z])\n",
    "\n",
    "out =  Dense(y_train.shape[1],activation='softmax')(w)\n",
    "\n",
    "model = keras.models.Model(inputs=[inputs_1,inputs_2,inputs_3], outputs=out)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user1/DGS-env/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 21059 samples, validate on 5265 samples\n",
      "Epoch 1/10\n",
      "21059/21059 [==============================] - 1123s 53ms/step - loss: 0.0183 - categorical_accuracy: 0.5735 - val_loss: 0.0139 - val_categorical_accuracy: 0.5920\n",
      "Epoch 2/10\n",
      "21059/21059 [==============================] - 1107s 53ms/step - loss: 0.0149 - categorical_accuracy: 0.6065 - val_loss: 0.0133 - val_categorical_accuracy: 0.6160\n",
      "Epoch 3/10\n",
      "21059/21059 [==============================] - 1043s 50ms/step - loss: 0.0135 - categorical_accuracy: 0.6428 - val_loss: 0.0131 - val_categorical_accuracy: 0.6161\n",
      "Epoch 4/10\n",
      "21059/21059 [==============================] - 1282s 61ms/step - loss: 0.0124 - categorical_accuracy: 0.6751 - val_loss: 0.0135 - val_categorical_accuracy: 0.5930\n",
      "Epoch 5/10\n",
      "21059/21059 [==============================] - 1119s 53ms/step - loss: 0.0110 - categorical_accuracy: 0.7212 - val_loss: 0.0139 - val_categorical_accuracy: 0.5899\n",
      "Epoch 6/10\n",
      "21059/21059 [==============================] - 1141s 54ms/step - loss: 0.0097 - categorical_accuracy: 0.7638 - val_loss: 0.0152 - val_categorical_accuracy: 0.5538\n",
      "Epoch 7/10\n",
      "21059/21059 [==============================] - 1065s 51ms/step - loss: 0.0087 - categorical_accuracy: 0.7956 - val_loss: 0.0157 - val_categorical_accuracy: 0.5613\n",
      "Epoch 8/10\n",
      "21059/21059 [==============================] - 1061s 50ms/step - loss: 0.0078 - categorical_accuracy: 0.8308 - val_loss: 0.0167 - val_categorical_accuracy: 0.5763\n",
      "Epoch 9/10\n",
      "21059/21059 [==============================] - 1045s 50ms/step - loss: 0.0069 - categorical_accuracy: 0.8567 - val_loss: 0.0176 - val_categorical_accuracy: 0.5660\n",
      "Epoch 10/10\n",
      "21059/21059 [==============================] - 1065s 51ms/step - loss: 0.0064 - categorical_accuracy: 0.8699 - val_loss: 0.0180 - val_categorical_accuracy: 0.5675\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([TRAIN[0][0],TRAIN[1][0],TRAIN[2][0]], y_train, epochs=10, validation_split=0.2, verbose=1, batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss :  0.02118711931047831\n",
      "categorical accuracy:  0.6486322283744812\n",
      "####################################\n",
      "For threshold:  0.01\n",
      "Samples-average quality numbers\n",
      "Precision: 0.4884, Recall: 0.7969, F1-measure: 0.5440\n",
      "For threshold:  0.04\n",
      "Samples-average quality numbers\n",
      "Precision: 0.5703, Recall: 0.7271, F1-measure: 0.6039\n",
      "For threshold:  0.06\n",
      "Samples-average quality numbers\n",
      "Precision: 0.5937, Recall: 0.7053, F1-measure: 0.6152\n",
      "For threshold:  0.08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user1/DGS-env/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples-average quality numbers\n",
      "Precision: 0.6082, Recall: 0.6877, F1-measure: 0.6195\n",
      "For threshold:  0.1\n",
      "Samples-average quality numbers\n",
      "Precision: 0.6178, Recall: 0.6719, F1-measure: 0.6199\n",
      "For threshold:  0.12\n",
      "Samples-average quality numbers\n",
      "Precision: 0.6259, Recall: 0.6592, F1-measure: 0.6192\n",
      "For threshold:  0.14\n",
      "Samples-average quality numbers\n",
      "Precision: 0.6304, Recall: 0.6468, F1-measure: 0.6165\n",
      "For threshold:  0.16\n",
      "Samples-average quality numbers\n",
      "Precision: 0.6310, Recall: 0.6369, F1-measure: 0.6134\n",
      "For threshold:  0.2\n",
      "Samples-average quality numbers\n",
      "Precision: 0.6274, Recall: 0.6188, F1-measure: 0.6051\n",
      "For threshold:  0.25\n",
      "Samples-average quality numbers\n",
      "Precision: 0.6157, Recall: 0.5973, F1-measure: 0.5920\n",
      "For threshold:  0.3\n",
      "Samples-average quality numbers\n",
      "Precision: 0.5998, Recall: 0.5795, F1-measure: 0.5788\n",
      "For threshold:  0.35\n",
      "Samples-average quality numbers\n",
      "Precision: 0.5893, Recall: 0.5662, F1-measure: 0.5696\n",
      "For threshold:  0.4\n",
      "Samples-average quality numbers\n",
      "Precision: 0.5771, Recall: 0.5547, F1-measure: 0.5596\n",
      "For threshold:  0.5\n",
      "Samples-average quality numbers\n",
      "Precision: 0.5555, Recall: 0.5352, F1-measure: 0.5410\n",
      "For threshold:  0.6\n",
      "Samples-average quality numbers\n",
      "Precision: 0.5315, Recall: 0.5156, F1-measure: 0.5202\n",
      "For threshold:  0.7\n",
      "Samples-average quality numbers\n",
      "Precision: 0.5036, Recall: 0.4917, F1-measure: 0.4953\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict([TRAIN[0][1],TRAIN[1][1],TRAIN[2][1]])\n",
    "\n",
    "print('loss : ', score)\n",
    "print('categorical accuracy: ',cat_acc)\n",
    "\n",
    "print('####################################')\n",
    "\n",
    "thresholds = [0.01,0.04,0.06,0.08,0.1,0.12,0.14,0.16,0.2,0.25,0.3,0.35,0.4,0.5,0.6,0.7]\n",
    "for val in thresholds:\n",
    "    print(\"For threshold: \", val)\n",
    "    pred=y_pred.copy()\n",
    "  \n",
    "    pred[pred>=val]=1\n",
    "    pred[pred<val]=0\n",
    "  \n",
    "    precision = precision_score(y_test, pred, average='samples')\n",
    "    recall = recall_score(y_test, pred, average='samples')\n",
    "    f1 = f1_score(y_test, pred, average='samples')\n",
    "   \n",
    "    print(\"Samples-average quality numbers\")\n",
    "    print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Commentaire :\n",
    "Nous observons que notre modèle sur apprend très rapidement et de manière importante. nous avons deux solutions classiques pour contrer cet effet : \n",
    "- Regularisation\n",
    "- Drop Out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion : \n",
    " Suite aux diverses expériences que nousa vosn mener dans ce notebook, nos conclusions sont : \n",
    "- La tf-idf est une représentation riche qui est difficile à battre tant en termes de performances qu'en temps de calcul.\n",
    "- Les modèles d'attention régularisés évitent mieux le sur-apprentissage que les modèles recurent avec du drop out\n",
    "- séparer les collones pour encoder\n",
    "\n",
    "\n",
    "Ce que nous devons essayer : \n",
    "- Multi head attention https://www.kaggle.com/fareise/multi-head-self-attention-for-text-classification, https://github.com/CyberZHG/keras-multi-head\n",
    "- Hierarchical attention : https://paperswithcode.com/paper/hierarchical-attentional-hybrid-neural\n",
    "- Concatenation des embedings et du tfidf\n",
    "- chercher de nouvelles méthodes de régularisation pour les réseaux récurrents\n",
    "- tester les CNN : https://www.kaggle.com/sanikamal/text-classification-with-python-and-keras\n",
    "- ajouter une couche de positinal encoding : https://github.com/kaushalshetty/Positional-Encoding\n",
    "- Librairie à essyaer rapidement :\n",
    "    - text-classification-keras : https://pypi.org/project/text-classification-keras/\n",
    "    - pytext :  https://github.com/facebookresearch/pytext\n",
    "\n",
    "- approche avec des emmbedings déjà entrainés : \n",
    "    - https://adventuresinmachinelearning.com/word2vec-keras-tutorial/\n",
    "    - https://medium.com/@ppasumarthi_69210/word-embeddings-in-keras-be6bb3092831\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DGS-env",
   "language": "python",
   "name": "dgs-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
