{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Inférence de l'effet - Stratégie Multilabels - Approche deep Learning\n",
    "Dans ce Notebook, nous cosntruisons un modèle qui permet d'inférer l'EFFET à partir de la classification de l'incident et des données textuelles en ce basant sur des reseau récurents commes GRU/LSTM etc.\n",
    "\n",
    "En effet, ces approches ont montré des réultats très encourageant et nous voulons explorer cette direction pour peut être faire des réceau recurent notre modèle par défault.\n",
    "\n",
    "Nous considérons ce problème comme un problème de classification multiclasses et multilabels. En effet, il y a plusieurs effets possibles et un incidents peut entrainer plusieurs effets.\n",
    "\n",
    "Dans ce note book nous nous posons les questions suivantes : \n",
    "- Quel est l'impact du drop out ?\n",
    "- Rajouter des couches augmentent-ils les performaces ?\n",
    "- L'utilisation de réseaux bidirectionnel est-elle pertinente ?\n",
    "- Une couche d'attention est-elle utile ?\n",
    "- Attention is all we need, really ?\n",
    "- Utilisation des embeddings \n",
    "- Concaténation des modèles sur différentes entrées ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,LSTM,Embedding,SpatialDropout1D, Bidirectional,SimpleRNN,Input, concatenate, Reshape\n",
    "import tensorflow \n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer\n",
    "from keras.layers import Concatenate, GlobalMaxPool1D, Dropout\n",
    "from tensorflow.keras.layers import Attention\n",
    "\n",
    "from sklearn.metrics import  precision_score, recall_score, f1_score\n",
    "\n",
    "tensorflow.random.set_seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user1/DGS-env/lib/python3.7/site-packages/sklearn/preprocessing/_label.py:995: UserWarning: unknown class(es) [55] will be ignored\n",
      "  .format(sorted(unknown, key=str)))\n"
     ]
    }
   ],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "train = pd.read_pickle('./data_split/train.pkl')\n",
    "# Pour faire un modèle sans le \n",
    "#train = train[~train['TEF_ID'].map(lambda x : 106 in x)]\n",
    "X_train = train[['FABRICANT','CLASSIFICATION','DESCRIPTION_INCIDENT','ETAT_PATIENT','ACTION_PATIENT']]\n",
    "y_train = mlb.fit_transform(train['CDY_ID'])\n",
    "test =  pd.read_pickle('./data_split/test.pkl')\n",
    "#test = test[~test['TEF_ID'].map(lambda x : k in x)]\n",
    "X_test = test[['FABRICANT','CLASSIFICATION','DESCRIPTION_INCIDENT','ETAT_PATIENT','ACTION_PATIENT']]\n",
    "y_test = mlb.transform(test['CDY_ID'])\n",
    "\n",
    "\n",
    "X_train_dgs = np.load('results/dgs_camenbert_train_vec.npy')\n",
    "X_test_dgs =np.load('results/dgs_camenbert_test_vec.npy')\n",
    "\n",
    "\n",
    "\n",
    "df_effets = pd.read_csv(\"data/ref_MRV/referentiel_dispositif_effets_connus.csv\",delimiter=';',encoding='ISO-8859-1')\n",
    "df_dys = pd.read_csv(\"data/ref_MRV/referentiel_dispositif_dysfonctionnement.csv\",delimiter=';',encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 LSTM et TFIDF, une première baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user1/DGS-env/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:386: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['qu', 'quelqu'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/home/user1/DGS-env/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:386: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['qu', 'quelqu'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/home/user1/DGS-env/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:386: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['qu', 'quelqu'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/home/user1/DGS-env/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:386: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['qu', 'quelqu'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 34s, sys: 3min 1s, total: 6min 35s\n",
      "Wall time: 1min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,HashingVectorizer\n",
    "import spacy\n",
    "nlp =spacy.load('fr')\n",
    "from spacy.lang.fr.stop_words import STOP_WORDS\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    [('description_tfidf',TfidfVectorizer(sublinear_tf=True, min_df=3,\n",
    "                            ngram_range=(1, 1),\n",
    "                            stop_words=STOP_WORDS,\n",
    "                            max_features = 10000,norm = 'l2'), 'DESCRIPTION_INCIDENT'),\n",
    "     \n",
    "     ('etat_pat_tfidf', TfidfVectorizer(sublinear_tf=True, min_df=3,ngram_range=(1, 1),\n",
    "                                       stop_words=STOP_WORDS,\n",
    "                                       max_features = 10000,norm = 'l2'), 'ETAT_PATIENT'),\n",
    "     ('action_tfidf',TfidfVectorizer(sublinear_tf=True, min_df=3,\n",
    "                            ngram_range=(1, 1),\n",
    "                            stop_words=STOP_WORDS,\n",
    "                            max_features = 5000,norm = 'l2'), 'ACTION_PATIENT'),\n",
    "     \n",
    "     \n",
    "     ('fabricant_tfidf',TfidfVectorizer(sublinear_tf=True, min_df=3,\n",
    "                            ngram_range=(1, 1),\n",
    "                            stop_words=STOP_WORDS,\n",
    "                            max_features = 5000,norm = 'l2'), 'FABRICANT')\n",
    "     ],\n",
    "    \n",
    "    remainder='passthrough')\n",
    "\n",
    "X_train_, X_test_ =preprocess.fit_transform(X_train),preprocess.transform(X_test)\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=1000)\n",
    "X_train_ = svd.fit_transform(X_train_)\n",
    "X_test_ = svd.transform(X_test_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ = np.reshape(X_train_, (X_train_.shape[0], 1, X_train_.shape[1]))\n",
    "X_test_ = np.reshape(X_test_, (X_test_.shape[0], 1, X_test_.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 21059 samples, validate on 5265 samples\n",
      "Epoch 1/10\n",
      "21059/21059 [==============================] - 13s 628us/step - loss: 0.0324 - categorical_accuracy: 0.8051 - val_loss: 0.0313 - val_categorical_accuracy: 0.7630\n",
      "Epoch 2/10\n",
      "21059/21059 [==============================] - 12s 580us/step - loss: 0.0203 - categorical_accuracy: 0.8245 - val_loss: 0.0298 - val_categorical_accuracy: 0.7578\n",
      "Epoch 3/10\n",
      "21059/21059 [==============================] - 13s 606us/step - loss: 0.0182 - categorical_accuracy: 0.8280 - val_loss: 0.0299 - val_categorical_accuracy: 0.7485\n",
      "Epoch 4/10\n",
      "21059/21059 [==============================] - 12s 558us/step - loss: 0.0169 - categorical_accuracy: 0.8330 - val_loss: 0.0297 - val_categorical_accuracy: 0.7445\n",
      "Epoch 5/10\n",
      "21059/21059 [==============================] - 10s 479us/step - loss: 0.0159 - categorical_accuracy: 0.8382 - val_loss: 0.0303 - val_categorical_accuracy: 0.7379\n",
      "Epoch 6/10\n",
      "21059/21059 [==============================] - 9s 447us/step - loss: 0.0151 - categorical_accuracy: 0.8438 - val_loss: 0.0305 - val_categorical_accuracy: 0.7434\n",
      "Epoch 7/10\n",
      "21059/21059 [==============================] - 7s 356us/step - loss: 0.0145 - categorical_accuracy: 0.8492 - val_loss: 0.0308 - val_categorical_accuracy: 0.7438\n",
      "Epoch 8/10\n",
      "21059/21059 [==============================] - 7s 329us/step - loss: 0.0139 - categorical_accuracy: 0.8543 - val_loss: 0.0315 - val_categorical_accuracy: 0.7434\n",
      "Epoch 9/10\n",
      "21059/21059 [==============================] - 11s 505us/step - loss: 0.0133 - categorical_accuracy: 0.8608 - val_loss: 0.0321 - val_categorical_accuracy: 0.7349\n",
      "Epoch 10/10\n",
      "21059/21059 [==============================] - 12s 584us/step - loss: 0.0128 - categorical_accuracy: 0.8659 - val_loss: 0.0328 - val_categorical_accuracy: 0.7400\n",
      "6580/6580 [==============================] - 1s 159us/step\n",
      "loss :  0.020829748735964118\n",
      "categorical accuracy:  0.8113981485366821\n",
      "####################################\n",
      "For threshold:  0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user1/DGS-env/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples-average quality numbers\n",
      "Precision: 0.5070, Recall: 0.9602, F1-measure: 0.6260\n",
      "For threshold:  0.04\n",
      "Samples-average quality numbers\n",
      "Precision: 0.6795, Recall: 0.9289, F1-measure: 0.7527\n",
      "For threshold:  0.06\n",
      "Samples-average quality numbers\n",
      "Precision: 0.7203, Recall: 0.9172, F1-measure: 0.7790\n",
      "For threshold:  0.08\n",
      "Samples-average quality numbers\n",
      "Precision: 0.7457, Recall: 0.9094, F1-measure: 0.7947\n",
      "For threshold:  0.1\n",
      "Samples-average quality numbers\n",
      "Precision: 0.7628, Recall: 0.9009, F1-measure: 0.8042\n",
      "For threshold:  0.12\n",
      "Samples-average quality numbers\n",
      "Precision: 0.7748, Recall: 0.8920, F1-measure: 0.8098\n",
      "For threshold:  0.14\n",
      "Samples-average quality numbers\n",
      "Precision: 0.7847, Recall: 0.8852, F1-measure: 0.8143\n",
      "For threshold:  0.16\n",
      "Samples-average quality numbers\n",
      "Precision: 0.7939, Recall: 0.8794, F1-measure: 0.8185\n",
      "For threshold:  0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user1/DGS-env/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples-average quality numbers\n",
      "Precision: 0.8036, Recall: 0.8652, F1-measure: 0.8200\n",
      "For threshold:  0.25\n",
      "Samples-average quality numbers\n",
      "Precision: 0.8131, Recall: 0.8509, F1-measure: 0.8211\n",
      "For threshold:  0.3\n",
      "Samples-average quality numbers\n",
      "Precision: 0.8187, Recall: 0.8377, F1-measure: 0.8192\n",
      "For threshold:  0.35\n",
      "Samples-average quality numbers\n",
      "Precision: 0.8214, Recall: 0.8232, F1-measure: 0.8152\n",
      "For threshold:  0.4\n",
      "Samples-average quality numbers\n",
      "Precision: 0.8199, Recall: 0.8110, F1-measure: 0.8096\n",
      "For threshold:  0.5\n",
      "Samples-average quality numbers\n",
      "Precision: 0.8030, Recall: 0.7806, F1-measure: 0.7879\n",
      "For threshold:  0.6\n",
      "Samples-average quality numbers\n",
      "Precision: 0.7611, Recall: 0.7449, F1-measure: 0.7502\n",
      "For threshold:  0.7\n",
      "Samples-average quality numbers\n",
      "Precision: 0.7167, Recall: 0.7051, F1-measure: 0.7089\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(200))\n",
    "model.add(Dense(y_train.shape[1], activation='softmax'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "history = model.fit(X_train_, y_train, epochs=epochs, batch_size=batch_size,validation_split=0.2)\n",
    "\n",
    "score,cat_acc = model.evaluate(X_test_,y_test)\n",
    "\n",
    "y_pred = model.predict(X_test_)\n",
    "\n",
    "print('loss : ', score)\n",
    "print('categorical accuracy: ',cat_acc)\n",
    "\n",
    "print('####################################')\n",
    "\n",
    "thresholds = [0.01,0.04,0.06,0.08,0.1,0.12,0.14,0.16,0.2,0.25,0.3,0.35,0.4,0.5,0.6,0.7]\n",
    "for val in thresholds:\n",
    "    print(\"For threshold: \", val)\n",
    "    pred=y_pred.copy()\n",
    "  \n",
    "    pred[pred>=val]=1\n",
    "    pred[pred<val]=0\n",
    "  \n",
    "    precision = precision_score(y_test, pred, average='samples')\n",
    "    recall = recall_score(y_test, pred, average='samples')\n",
    "    f1 = f1_score(y_test, pred, average='samples')\n",
    "   \n",
    "    print(\"Samples-average quality numbers\")\n",
    "    print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commentaires : \n",
    "\n",
    "Nous obtenons un score de base line de 0.6730. Nous allons essayer d'améliorer ce score avec difféntes approches : \n",
    "- Remplacer le tfidf par un embedding entrainé par le modèle \n",
    "    - Embedding simple\n",
    "    - Embedding par colonnes\n",
    "    \n",
    "- Essayer un modèle biLSTM ou GRU ou bi GRU\n",
    "- Essayer de rajouter une couche d'attention dans Keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 21059 samples, validate on 5265 samples\n",
      "Epoch 1/4\n",
      "21059/21059 [==============================] - 14s 688us/step - loss: 0.0276 - categorical_accuracy: 0.8105 - val_loss: 0.0298 - val_categorical_accuracy: 0.7592\n",
      "Epoch 2/4\n",
      "21059/21059 [==============================] - 14s 688us/step - loss: 0.0194 - categorical_accuracy: 0.8204 - val_loss: 0.0299 - val_categorical_accuracy: 0.7605\n",
      "Epoch 3/4\n",
      "21059/21059 [==============================] - 14s 660us/step - loss: 0.0177 - categorical_accuracy: 0.8254 - val_loss: 0.0299 - val_categorical_accuracy: 0.7489\n",
      "Epoch 4/4\n",
      "21059/21059 [==============================] - 8s 376us/step - loss: 0.0167 - categorical_accuracy: 0.8310 - val_loss: 0.0299 - val_categorical_accuracy: 0.7508\n",
      "6580/6580 [==============================] - 0s 74us/step\n",
      "loss :  0.019804676357758625\n",
      "categorical accuracy:  0.81686931848526\n",
      "####################################\n",
      "For threshold:  0.01\n",
      "Samples-average quality numbers\n",
      "Precision: 0.3990, Recall: 0.9731, F1-measure: 0.5411\n",
      "For threshold:  0.04\n",
      "Samples-average quality numbers\n",
      "Precision: 0.6366, Recall: 0.9380, F1-measure: 0.7260\n",
      "For threshold:  0.06\n",
      "Samples-average quality numbers\n",
      "Precision: 0.6981, Recall: 0.9270, F1-measure: 0.7674\n",
      "For threshold:  0.08\n",
      "Samples-average quality numbers\n",
      "Precision: 0.7338, Recall: 0.9158, F1-measure: 0.7892\n",
      "For threshold:  0.1\n",
      "Samples-average quality numbers\n",
      "Precision: 0.7591, Recall: 0.9063, F1-measure: 0.8039\n",
      "For threshold:  0.12\n",
      "Samples-average quality numbers\n",
      "Precision: 0.7750, Recall: 0.8970, F1-measure: 0.8119\n",
      "For threshold:  0.14\n",
      "Samples-average quality numbers\n",
      "Precision: 0.7865, Recall: 0.8893, F1-measure: 0.8170\n",
      "For threshold:  0.16\n",
      "Samples-average quality numbers\n",
      "Precision: 0.7977, Recall: 0.8828, F1-measure: 0.8223\n",
      "For threshold:  0.2\n",
      "Samples-average quality numbers\n",
      "Precision: 0.8109, Recall: 0.8701, F1-measure: 0.8268\n",
      "For threshold:  0.25\n",
      "Samples-average quality numbers\n",
      "Precision: 0.8166, Recall: 0.8532, F1-measure: 0.8243\n",
      "For threshold:  0.3\n",
      "Samples-average quality numbers\n",
      "Precision: 0.8209, Recall: 0.8383, F1-measure: 0.8213\n",
      "For threshold:  0.35\n",
      "Samples-average quality numbers\n",
      "Precision: 0.8226, Recall: 0.8243, F1-measure: 0.8166\n",
      "For threshold:  0.4\n",
      "Samples-average quality numbers\n",
      "Precision: 0.8222, Recall: 0.8113, F1-measure: 0.8114\n",
      "For threshold:  0.5\n",
      "Samples-average quality numbers\n",
      "Precision: 0.7991, Recall: 0.7790, F1-measure: 0.7856\n",
      "For threshold:  0.6\n",
      "Samples-average quality numbers\n",
      "Precision: 0.7603, Recall: 0.7457, F1-measure: 0.7505\n",
      "For threshold:  0.7\n",
      "Samples-average quality numbers\n",
      "Precision: 0.7112, Recall: 0.7009, F1-measure: 0.7043\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(200, dropout=0.15))\n",
    "model.add(Dense(y_train.shape[1], activation='softmax'))\n",
    "\n",
    "\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['categorical_accuracy'])\n",
    "\n",
    "epochs = 4\n",
    "batch_size = 32\n",
    "\n",
    "history = model.fit(X_train_, y_train, epochs=epochs, batch_size=batch_size,validation_split=0.2)\n",
    "\n",
    "score,cat_acc = model.evaluate(X_test_,y_test)\n",
    "\n",
    "y_pred = model.predict(X_test_)\n",
    "\n",
    "print('loss : ', score)\n",
    "print('categorical accuracy: ',cat_acc)\n",
    "\n",
    "print('####################################')\n",
    "\n",
    "thresholds = [0.01,0.04,0.06,0.08,0.1,0.12,0.14,0.16,0.2,0.25,0.3,0.35,0.4,0.5,0.6,0.7]\n",
    "for val in thresholds:\n",
    "    print(\"For threshold: \", val)\n",
    "    pred=y_pred.copy()\n",
    "  \n",
    "    pred[pred>=val]=1\n",
    "    pred[pred<val]=0\n",
    "  \n",
    "    precision = precision_score(y_test, pred, average='samples')\n",
    "    recall = recall_score(y_test, pred, average='samples')\n",
    "    f1 = f1_score(y_test, pred, average='samples')\n",
    "   \n",
    "    print(\"Samples-average quality numbers\")\n",
    "    print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Comment performe un reseau de BiLSTM  ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 21059 samples, validate on 5265 samples\n",
      "Epoch 1/5\n",
      "21059/21059 [==============================] - 13s 598us/step - loss: 0.0303 - categorical_accuracy: 0.8072 - val_loss: 0.0305 - val_categorical_accuracy: 0.7654\n",
      "Epoch 2/5\n",
      "21059/21059 [==============================] - 17s 799us/step - loss: 0.0198 - categorical_accuracy: 0.8212 - val_loss: 0.0296 - val_categorical_accuracy: 0.7573\n",
      "Epoch 3/5\n",
      "21059/21059 [==============================] - 20s 933us/step - loss: 0.0179 - categorical_accuracy: 0.8257 - val_loss: 0.0296 - val_categorical_accuracy: 0.7404\n",
      "Epoch 4/5\n",
      "21059/21059 [==============================] - 19s 891us/step - loss: 0.0167 - categorical_accuracy: 0.8300 - val_loss: 0.0299 - val_categorical_accuracy: 0.7455\n",
      "Epoch 5/5\n",
      "21059/21059 [==============================] - 19s 912us/step - loss: 0.0159 - categorical_accuracy: 0.8394 - val_loss: 0.0304 - val_categorical_accuracy: 0.7453\n",
      "6580/6580 [==============================] - 2s 270us/step\n",
      "loss :  0.01992118165497088\n",
      "categorical accuracy:  0.8158054947853088\n",
      "####################################\n",
      "For threshold:  0.01\n",
      "Samples-average quality numbers\n",
      "Precision: 0.4467, Recall: 0.9695, F1-measure: 0.5770\n",
      "For threshold:  0.04\n",
      "Samples-average quality numbers\n",
      "Precision: 0.6699, Recall: 0.9368, F1-measure: 0.7481\n",
      "For threshold:  0.06\n",
      "Samples-average quality numbers\n",
      "Precision: 0.7196, Recall: 0.9228, F1-measure: 0.7806\n",
      "For threshold:  0.08\n",
      "Samples-average quality numbers\n",
      "Precision: 0.7458, Recall: 0.9116, F1-measure: 0.7960\n",
      "For threshold:  0.1\n",
      "Samples-average quality numbers\n",
      "Precision: 0.7645, Recall: 0.9039, F1-measure: 0.8066\n",
      "For threshold:  0.12\n",
      "Samples-average quality numbers\n",
      "Precision: 0.7778, Recall: 0.8946, F1-measure: 0.8128\n",
      "For threshold:  0.14\n",
      "Samples-average quality numbers\n",
      "Precision: 0.7889, Recall: 0.8882, F1-measure: 0.8182\n",
      "For threshold:  0.16\n",
      "Samples-average quality numbers\n",
      "Precision: 0.7954, Recall: 0.8805, F1-measure: 0.8202\n",
      "For threshold:  0.2\n",
      "Samples-average quality numbers\n",
      "Precision: 0.8058, Recall: 0.8664, F1-measure: 0.8221\n",
      "For threshold:  0.25\n",
      "Samples-average quality numbers\n",
      "Precision: 0.8164, Recall: 0.8518, F1-measure: 0.8236\n",
      "For threshold:  0.3\n",
      "Samples-average quality numbers\n",
      "Precision: 0.8216, Recall: 0.8377, F1-measure: 0.8215\n",
      "For threshold:  0.35\n",
      "Samples-average quality numbers\n",
      "Precision: 0.8239, Recall: 0.8242, F1-measure: 0.8174\n",
      "For threshold:  0.4\n",
      "Samples-average quality numbers\n",
      "Precision: 0.8211, Recall: 0.8091, F1-measure: 0.8094\n",
      "For threshold:  0.5\n",
      "Samples-average quality numbers\n",
      "Precision: 0.7980, Recall: 0.7764, F1-measure: 0.7834\n",
      "For threshold:  0.6\n",
      "Samples-average quality numbers\n",
      "Precision: 0.7606, Recall: 0.7455, F1-measure: 0.7504\n",
      "For threshold:  0.7\n",
      "Samples-average quality numbers\n",
      "Precision: 0.7103, Recall: 0.6998, F1-measure: 0.7033\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(200, dropout=0.1)))\n",
    "model.add(Dense(y_train.shape[1], activation='softmax'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n",
    "\n",
    "epochs = 5\n",
    "batch_size = 32\n",
    "\n",
    "history = model.fit(X_train_, y_train, epochs=epochs, batch_size=batch_size,validation_split=0.2)\n",
    "\n",
    "score,cat_acc = model.evaluate(X_test_,y_test)\n",
    "\n",
    "y_pred = model.predict(X_test_)\n",
    "\n",
    "print('loss : ', score)\n",
    "print('categorical accuracy: ',cat_acc)\n",
    "\n",
    "print('####################################')\n",
    "\n",
    "thresholds = [0.01,0.04,0.06,0.08,0.1,0.12,0.14,0.16,0.2,0.25,0.3,0.35,0.4,0.5,0.6,0.7]\n",
    "for val in thresholds:\n",
    "    print(\"For threshold: \", val)\n",
    "    pred=y_pred.copy()\n",
    "  \n",
    "    pred[pred>=val]=1\n",
    "    pred[pred<val]=0\n",
    "  \n",
    "    precision = precision_score(y_test, pred, average='samples')\n",
    "    recall = recall_score(y_test, pred, average='samples')\n",
    "    f1 = f1_score(y_test, pred, average='samples')\n",
    "   \n",
    "    print(\"Samples-average quality numbers\")\n",
    "    print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commentaire ?\n",
    "\n",
    "L'augementation de la performance est très minime : 0.6733 ?\n",
    "Nous remarquons que notre modèle sur apprend assez rapidement, nous allons essayer d'ajouter une couche de dropout pour limiter ce sur apprentissage.\n",
    "## Si on ajoute du dropout ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 21059 samples, validate on 5265 samples\n",
      "Epoch 1/10\n",
      "21059/21059 [==============================] - 22s 1ms/step - loss: 0.0270 - categorical_accuracy: 0.8091 - val_loss: 0.0324 - val_categorical_accuracy: 0.7632\n",
      "Epoch 2/10\n",
      "21059/21059 [==============================] - 17s 823us/step - loss: 0.0200 - categorical_accuracy: 0.8202 - val_loss: 0.0301 - val_categorical_accuracy: 0.7478\n",
      "Epoch 3/10\n",
      "21059/21059 [==============================] - 15s 715us/step - loss: 0.0183 - categorical_accuracy: 0.8253 - val_loss: 0.0298 - val_categorical_accuracy: 0.7474\n",
      "Epoch 4/10\n",
      "21059/21059 [==============================] - 16s 750us/step - loss: 0.0171 - categorical_accuracy: 0.8281 - val_loss: 0.0304 - val_categorical_accuracy: 0.7538\n",
      "Epoch 5/10\n",
      "21059/21059 [==============================] - 20s 966us/step - loss: 0.0162 - categorical_accuracy: 0.8364 - val_loss: 0.0303 - val_categorical_accuracy: 0.7314\n",
      "Epoch 6/10\n",
      "21059/21059 [==============================] - 20s 952us/step - loss: 0.0155 - categorical_accuracy: 0.8412 - val_loss: 0.0313 - val_categorical_accuracy: 0.7428\n",
      "Epoch 7/10\n",
      "21059/21059 [==============================] - 20s 935us/step - loss: 0.0149 - categorical_accuracy: 0.8470 - val_loss: 0.0323 - val_categorical_accuracy: 0.7432\n",
      "Epoch 8/10\n",
      "21059/21059 [==============================] - 19s 918us/step - loss: 0.0143 - categorical_accuracy: 0.8513 - val_loss: 0.0328 - val_categorical_accuracy: 0.7335\n",
      "Epoch 9/10\n",
      "21059/21059 [==============================] - 19s 889us/step - loss: 0.0137 - categorical_accuracy: 0.8592 - val_loss: 0.0335 - val_categorical_accuracy: 0.7320\n",
      "Epoch 10/10\n",
      "21059/21059 [==============================] - 19s 918us/step - loss: 0.0130 - categorical_accuracy: 0.8657 - val_loss: 0.0338 - val_categorical_accuracy: 0.7231\n",
      "6580/6580 [==============================] - 2s 269us/step\n",
      "loss :  0.021202840195193116\n",
      "categorical accuracy:  0.8071428537368774\n",
      "####################################\n",
      "For threshold:  0.01\n",
      "Samples-average quality numbers\n",
      "Precision: 0.4840, Recall: 0.9589, F1-measure: 0.6080\n",
      "For threshold:  0.04\n",
      "Samples-average quality numbers\n",
      "Precision: 0.6623, Recall: 0.9295, F1-measure: 0.7414\n",
      "For threshold:  0.06\n",
      "Samples-average quality numbers\n",
      "Precision: 0.7036, Recall: 0.9161, F1-measure: 0.7677\n",
      "For threshold:  0.08\n",
      "Samples-average quality numbers\n",
      "Precision: 0.7331, Recall: 0.9065, F1-measure: 0.7858\n",
      "For threshold:  0.1\n",
      "Samples-average quality numbers\n",
      "Precision: 0.7542, Recall: 0.9009, F1-measure: 0.7990\n",
      "For threshold:  0.12\n",
      "Samples-average quality numbers\n",
      "Precision: 0.7692, Recall: 0.8937, F1-measure: 0.8072\n",
      "For threshold:  0.14\n",
      "Samples-average quality numbers\n",
      "Precision: 0.7799, Recall: 0.8870, F1-measure: 0.8122\n",
      "For threshold:  0.16\n",
      "Samples-average quality numbers\n",
      "Precision: 0.7889, Recall: 0.8804, F1-measure: 0.8159\n",
      "For threshold:  0.2\n",
      "Samples-average quality numbers\n",
      "Precision: 0.8015, Recall: 0.8693, F1-measure: 0.8205\n",
      "For threshold:  0.25\n",
      "Samples-average quality numbers\n",
      "Precision: 0.8097, Recall: 0.8567, F1-measure: 0.8211\n",
      "For threshold:  0.3\n",
      "Samples-average quality numbers\n",
      "Precision: 0.8138, Recall: 0.8410, F1-measure: 0.8177\n",
      "For threshold:  0.35\n",
      "Samples-average quality numbers\n",
      "Precision: 0.8171, Recall: 0.8231, F1-measure: 0.8126\n",
      "For threshold:  0.4\n",
      "Samples-average quality numbers\n",
      "Precision: 0.8157, Recall: 0.8078, F1-measure: 0.8059\n",
      "For threshold:  0.5\n",
      "Samples-average quality numbers\n",
      "Precision: 0.8003, Recall: 0.7784, F1-measure: 0.7856\n",
      "For threshold:  0.6\n",
      "Samples-average quality numbers\n",
      "Precision: 0.7556, Recall: 0.7404, F1-measure: 0.7454\n",
      "For threshold:  0.7\n",
      "Samples-average quality numbers\n",
      "Precision: 0.7032, Recall: 0.6929, F1-measure: 0.6963\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(200, return_sequences=True, dropout=0.1)))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dense(100, activation=\"elu\"))\n",
    "model.add(Dropout(0.1))\n",
    "model.add( Dense(y_train.shape[1], activation=\"softmax\"))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n",
    "\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "history = model.fit(X_train_, y_train, epochs=epochs, batch_size=batch_size,validation_split=0.2)\n",
    "\n",
    "score,cat_acc = model.evaluate(X_test_,y_test)\n",
    "\n",
    "y_pred = model.predict(X_test_)\n",
    "\n",
    "print('loss : ', score)\n",
    "print('categorical accuracy: ',cat_acc)\n",
    "\n",
    "print('####################################')\n",
    "\n",
    "thresholds = [0.01,0.04,0.06,0.08,0.1,0.12,0.14,0.16,0.2,0.25,0.3,0.35,0.4,0.5,0.6,0.7]\n",
    "for val in thresholds:\n",
    "    print(\"For threshold: \", val)\n",
    "    pred=y_pred.copy()\n",
    "  \n",
    "    pred[pred>=val]=1\n",
    "    pred[pred<val]=0\n",
    "  \n",
    "    precision = precision_score(y_test, pred, average='samples')\n",
    "    recall = recall_score(y_test, pred, average='samples')\n",
    "    f1 = f1_score(y_test, pred, average='samples')\n",
    "   \n",
    "    print(\"Samples-average quality numbers\")\n",
    "    print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commentaire ?\n",
    "\n",
    "Les performances changent très peu,  nous allons essayer d'augementer la profondeur de notre reseau.\n",
    "\n",
    "## Si on ajoute plusieurs couches ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 21059 samples, validate on 5265 samples\n",
      "Epoch 1/10\n",
      "21059/21059 [==============================] - 28s 1ms/step - loss: 0.0261 - categorical_accuracy: 0.8098 - val_loss: 0.0302 - val_categorical_accuracy: 0.7597\n",
      "Epoch 2/10\n",
      "21059/21059 [==============================] - 25s 1ms/step - loss: 0.0195 - categorical_accuracy: 0.8202 - val_loss: 0.0293 - val_categorical_accuracy: 0.7487\n",
      "Epoch 3/10\n",
      "21059/21059 [==============================] - 26s 1ms/step - loss: 0.0178 - categorical_accuracy: 0.8254 - val_loss: 0.0298 - val_categorical_accuracy: 0.7379\n",
      "Epoch 4/10\n",
      "21059/21059 [==============================] - 23s 1ms/step - loss: 0.0168 - categorical_accuracy: 0.8317 - val_loss: 0.0310 - val_categorical_accuracy: 0.7521\n",
      "Epoch 5/10\n",
      "21059/21059 [==============================] - 22s 1ms/step - loss: 0.0159 - categorical_accuracy: 0.8370 - val_loss: 0.0308 - val_categorical_accuracy: 0.7238\n",
      "Epoch 6/10\n",
      "21059/21059 [==============================] - 19s 883us/step - loss: 0.0152 - categorical_accuracy: 0.8427 - val_loss: 0.0317 - val_categorical_accuracy: 0.7402\n",
      "Epoch 7/10\n",
      "21059/21059 [==============================] - 24s 1ms/step - loss: 0.0146 - categorical_accuracy: 0.8478 - val_loss: 0.0326 - val_categorical_accuracy: 0.7231\n",
      "Epoch 8/10\n",
      "21059/21059 [==============================] - 25s 1ms/step - loss: 0.0139 - categorical_accuracy: 0.8549 - val_loss: 0.0328 - val_categorical_accuracy: 0.7449\n",
      "Epoch 9/10\n",
      "21059/21059 [==============================] - 25s 1ms/step - loss: 0.0134 - categorical_accuracy: 0.8598 - val_loss: 0.0337 - val_categorical_accuracy: 0.7394\n",
      "Epoch 10/10\n",
      "21059/21059 [==============================] - 26s 1ms/step - loss: 0.0126 - categorical_accuracy: 0.8672 - val_loss: 0.0337 - val_categorical_accuracy: 0.7373\n",
      "6580/6580 [==============================] - 2s 343us/step\n",
      "loss :  0.02160979865464215\n",
      "categorical accuracy:  0.8112462162971497\n",
      "####################################\n",
      "For threshold:  0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user1/DGS-env/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples-average quality numbers\n",
      "Precision: 0.5059, Recall: 0.9548, F1-measure: 0.6248\n",
      "For threshold:  0.04\n",
      "Samples-average quality numbers\n",
      "Precision: 0.6771, Recall: 0.9259, F1-measure: 0.7505\n",
      "For threshold:  0.06\n",
      "Samples-average quality numbers\n",
      "Precision: 0.7175, Recall: 0.9139, F1-measure: 0.7764\n",
      "For threshold:  0.08\n",
      "Samples-average quality numbers\n",
      "Precision: 0.7404, Recall: 0.9048, F1-measure: 0.7900\n",
      "For threshold:  0.1\n",
      "Samples-average quality numbers\n",
      "Precision: 0.7579, Recall: 0.8972, F1-measure: 0.8000\n",
      "For threshold:  0.12\n",
      "Samples-average quality numbers\n",
      "Precision: 0.7701, Recall: 0.8879, F1-measure: 0.8055\n",
      "For threshold:  0.14\n",
      "Samples-average quality numbers\n",
      "Precision: 0.7792, Recall: 0.8795, F1-measure: 0.8089\n",
      "For threshold:  0.16\n",
      "Samples-average quality numbers\n",
      "Precision: 0.7881, Recall: 0.8737, F1-measure: 0.8126\n",
      "For threshold:  0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user1/DGS-env/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples-average quality numbers\n",
      "Precision: 0.7993, Recall: 0.8619, F1-measure: 0.8160\n",
      "For threshold:  0.25\n",
      "Samples-average quality numbers\n",
      "Precision: 0.8078, Recall: 0.8470, F1-measure: 0.8160\n",
      "For threshold:  0.3\n",
      "Samples-average quality numbers\n",
      "Precision: 0.8135, Recall: 0.8341, F1-measure: 0.8146\n",
      "For threshold:  0.35\n",
      "Samples-average quality numbers\n",
      "Precision: 0.8169, Recall: 0.8224, F1-measure: 0.8120\n",
      "For threshold:  0.4\n",
      "Samples-average quality numbers\n",
      "Precision: 0.8176, Recall: 0.8095, F1-measure: 0.8073\n",
      "For threshold:  0.5\n",
      "Samples-average quality numbers\n",
      "Precision: 0.8017, Recall: 0.7773, F1-measure: 0.7853\n",
      "For threshold:  0.6\n",
      "Samples-average quality numbers\n",
      "Precision: 0.7591, Recall: 0.7420, F1-measure: 0.7476\n",
      "For threshold:  0.7\n",
      "Samples-average quality numbers\n",
      "Precision: 0.7122, Recall: 0.7002, F1-measure: 0.7041\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "reg = regularizers.l2(1e-4)\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(256,  return_sequences=True, dropout=0.1)))\n",
    "model.add((SimpleRNN(128,return_sequences=True)))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(y_train.shape[1], activation=\"softmax\"))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n",
    "\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "history = model.fit(X_train_, y_train, epochs=epochs, batch_size=batch_size,validation_split=0.2)\n",
    "\n",
    "score,cat_acc = model.evaluate(X_test_,y_test)\n",
    "\n",
    "y_pred = model.predict(X_test_)\n",
    "\n",
    "print('loss : ', score)\n",
    "print('categorical accuracy: ',cat_acc)\n",
    "\n",
    "print('####################################')\n",
    "\n",
    "thresholds = [0.01,0.04,0.06,0.08,0.1,0.12,0.14,0.16,0.2,0.25,0.3,0.35,0.4,0.5,0.6,0.7]\n",
    "for val in thresholds:\n",
    "    print(\"For threshold: \", val)\n",
    "    pred=y_pred.copy()\n",
    "  \n",
    "    pred[pred>=val]=1\n",
    "    pred[pred<val]=0\n",
    "  \n",
    "    precision = precision_score(y_test, pred, average='samples')\n",
    "    recall = recall_score(y_test, pred, average='samples')\n",
    "    f1 = f1_score(y_test, pred, average='samples')\n",
    "   \n",
    "    print(\"Samples-average quality numbers\")\n",
    "    print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commentaires : \n",
    "\n",
    "Encore une fois, le modèle semble avoir atteint un plafond de verre qui ne soit pas dépassable ni par la prodondeur, ni par l'ajout de droptout. Nous allons essayer d'ajouter une couche d'attention.\n",
    "\n",
    "## Une couche d'attention ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 21059 samples, validate on 5265 samples\n",
      "Epoch 1/10\n",
      "21059/21059 [==============================] - 13s 613us/step - loss: 0.1145 - categorical_accuracy: 0.7885 - val_loss: 0.0346 - val_categorical_accuracy: 0.7605\n",
      "Epoch 2/10\n",
      "21059/21059 [==============================] - 14s 678us/step - loss: 0.0251 - categorical_accuracy: 0.8139 - val_loss: 0.0319 - val_categorical_accuracy: 0.7643\n",
      "Epoch 3/10\n",
      "21059/21059 [==============================] - 12s 588us/step - loss: 0.0218 - categorical_accuracy: 0.8249 - val_loss: 0.0303 - val_categorical_accuracy: 0.7633\n",
      "Epoch 4/10\n",
      "21059/21059 [==============================] - 12s 577us/step - loss: 0.0196 - categorical_accuracy: 0.8267 - val_loss: 0.0296 - val_categorical_accuracy: 0.7635\n",
      "Epoch 5/10\n",
      "21059/21059 [==============================] - 15s 695us/step - loss: 0.0182 - categorical_accuracy: 0.8303 - val_loss: 0.0294 - val_categorical_accuracy: 0.7578\n",
      "Epoch 6/10\n",
      "21059/21059 [==============================] - 12s 547us/step - loss: 0.0172 - categorical_accuracy: 0.8306 - val_loss: 0.0294 - val_categorical_accuracy: 0.7531\n",
      "Epoch 7/10\n",
      "21059/21059 [==============================] - 13s 636us/step - loss: 0.0164 - categorical_accuracy: 0.8327 - val_loss: 0.0297 - val_categorical_accuracy: 0.7508\n",
      "Epoch 8/10\n",
      "21059/21059 [==============================] - 14s 656us/step - loss: 0.0157 - categorical_accuracy: 0.8341 - val_loss: 0.0297 - val_categorical_accuracy: 0.7559\n",
      "Epoch 9/10\n",
      "21059/21059 [==============================] - 14s 677us/step - loss: 0.0152 - categorical_accuracy: 0.8368 - val_loss: 0.0301 - val_categorical_accuracy: 0.7495\n",
      "Epoch 10/10\n",
      "21059/21059 [==============================] - 13s 610us/step - loss: 0.0147 - categorical_accuracy: 0.8405 - val_loss: 0.0304 - val_categorical_accuracy: 0.7491\n",
      "6580/6580 [==============================] - 1s 184us/step\n",
      "loss :  0.019460711606684788\n",
      "categorical accuracy:  0.8135258555412292\n",
      "####################################\n",
      "For threshold:  0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user1/DGS-env/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples-average quality numbers\n",
      "Precision: 0.4609, Recall: 0.9706, F1-measure: 0.5900\n",
      "For threshold:  0.04\n",
      "Samples-average quality numbers\n",
      "Precision: 0.6513, Recall: 0.9399, F1-measure: 0.7353\n",
      "For threshold:  0.06\n",
      "Samples-average quality numbers\n",
      "Precision: 0.6984, Recall: 0.9288, F1-measure: 0.7671\n",
      "For threshold:  0.08\n",
      "Samples-average quality numbers\n",
      "Precision: 0.7259, Recall: 0.9197, F1-measure: 0.7843\n",
      "For threshold:  0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user1/DGS-env/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples-average quality numbers\n",
      "Precision: 0.7444, Recall: 0.9119, F1-measure: 0.7951\n",
      "For threshold:  0.12\n",
      "Samples-average quality numbers\n",
      "Precision: 0.7586, Recall: 0.9044, F1-measure: 0.8026\n",
      "For threshold:  0.14\n",
      "Samples-average quality numbers\n",
      "Precision: 0.7692, Recall: 0.8983, F1-measure: 0.8081\n",
      "For threshold:  0.16\n",
      "Samples-average quality numbers\n",
      "Precision: 0.7787, Recall: 0.8935, F1-measure: 0.8132\n",
      "For threshold:  0.2\n",
      "Samples-average quality numbers\n",
      "Precision: 0.7914, Recall: 0.8821, F1-measure: 0.8184\n",
      "For threshold:  0.25\n",
      "Samples-average quality numbers\n",
      "Precision: 0.8022, Recall: 0.8703, F1-measure: 0.8216\n",
      "For threshold:  0.26\n",
      "Samples-average quality numbers\n",
      "Precision: 0.8043, Recall: 0.8683, F1-measure: 0.8222\n",
      "For threshold:  0.27\n",
      "Samples-average quality numbers\n",
      "Precision: 0.8057, Recall: 0.8666, F1-measure: 0.8226\n",
      "For threshold:  0.28\n",
      "Samples-average quality numbers\n",
      "Precision: 0.8070, Recall: 0.8641, F1-measure: 0.8227\n",
      "For threshold:  0.29\n",
      "Samples-average quality numbers\n",
      "Precision: 0.8080, Recall: 0.8613, F1-measure: 0.8223\n",
      "For threshold:  0.3\n",
      "Samples-average quality numbers\n",
      "Precision: 0.8091, Recall: 0.8595, F1-measure: 0.8225\n",
      "For threshold:  0.35\n",
      "Samples-average quality numbers\n",
      "Precision: 0.8117, Recall: 0.8470, F1-measure: 0.8196\n",
      "For threshold:  0.4\n",
      "Samples-average quality numbers\n",
      "Precision: 0.8130, Recall: 0.8368, F1-measure: 0.8169\n",
      "For threshold:  0.5\n",
      "Samples-average quality numbers\n",
      "Precision: 0.8068, Recall: 0.8096, F1-measure: 0.8031\n",
      "For threshold:  0.6\n",
      "Samples-average quality numbers\n",
      "Precision: 0.7874, Recall: 0.7804, F1-measure: 0.7803\n",
      "For threshold:  0.7\n",
      "Samples-average quality numbers\n",
      "Precision: 0.7443, Recall: 0.7328, F1-measure: 0.7358\n"
     ]
    }
   ],
   "source": [
    "from keras_self_attention import SeqSelfAttention\n",
    "import keras\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(200, return_sequences=True))\n",
    "model.add(SeqSelfAttention(attention_activation='sigmoid'))\n",
    "model.add(keras.layers.Flatten(name='Flatten'))\n",
    "model.add(Dense(y_train.shape[1], activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n",
    "\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "history = model.fit(X_train_, y_train, epochs=epochs, batch_size=batch_size,validation_split=0.2)\n",
    "\n",
    "score,cat_acc = model.evaluate(X_test_,y_test)\n",
    "\n",
    "y_pred = model.predict(X_test_)\n",
    "\n",
    "print('loss : ', score)\n",
    "print('categorical accuracy: ',cat_acc)\n",
    "\n",
    "print('####################################')\n",
    "\n",
    "thresholds = [0.01,0.04,0.06,0.08,0.1,0.12,0.14,0.16,0.2,0.25,0.26,0.27,0.28,0.29,0.3,0.35,0.4,0.5,0.6,0.7]\n",
    "for val in thresholds:\n",
    "    print(\"For threshold: \", val)\n",
    "    pred=y_pred.copy()\n",
    "  \n",
    "    pred[pred>=val]=1\n",
    "    pred[pred<val]=0\n",
    "  \n",
    "    precision = precision_score(y_test, pred, average='samples')\n",
    "    recall = recall_score(y_test, pred, average='samples')\n",
    "    f1 = f1_score(y_test, pred, average='samples')\n",
    "   \n",
    "    print(\"Samples-average quality numbers\")\n",
    "    print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commentaire : \n",
    "Les résultats sembles similaire. Toutefois, la loss de validation est bien meilleur sans pour autant augmenter le f1 score. Nous allons essayer de faire seulement un réseau basé sur l'attention. Nous utilison ici, une verion multiplicative de l'attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 21059 samples, validate on 5265 samples\n",
      "Epoch 1/20\n",
      "21059/21059 [==============================] - 12s 593us/step - loss: 0.0712 - categorical_accuracy: 0.5526 - val_loss: 0.0561 - val_categorical_accuracy: 0.6699\n",
      "Epoch 2/20\n",
      "21059/21059 [==============================] - 12s 575us/step - loss: 0.0403 - categorical_accuracy: 0.7873 - val_loss: 0.0496 - val_categorical_accuracy: 0.7398\n",
      "Epoch 3/20\n",
      "21059/21059 [==============================] - 12s 574us/step - loss: 0.0340 - categorical_accuracy: 0.8035 - val_loss: 0.0462 - val_categorical_accuracy: 0.7432\n",
      "Epoch 4/20\n",
      "21059/21059 [==============================] - 13s 631us/step - loss: 0.0304 - categorical_accuracy: 0.8037 - val_loss: 0.0443 - val_categorical_accuracy: 0.7516\n",
      "Epoch 5/20\n",
      "21059/21059 [==============================] - 12s 552us/step - loss: 0.0280 - categorical_accuracy: 0.8043 - val_loss: 0.0436 - val_categorical_accuracy: 0.7548\n",
      "Epoch 6/20\n",
      "21059/21059 [==============================] - 12s 568us/step - loss: 0.0264 - categorical_accuracy: 0.8043 - val_loss: 0.0426 - val_categorical_accuracy: 0.7557\n",
      "Epoch 7/20\n",
      "21059/21059 [==============================] - 12s 568us/step - loss: 0.0255 - categorical_accuracy: 0.8048 - val_loss: 0.0429 - val_categorical_accuracy: 0.7559\n",
      "Epoch 8/20\n",
      "21059/21059 [==============================] - 13s 616us/step - loss: 0.0247 - categorical_accuracy: 0.8054 - val_loss: 0.0437 - val_categorical_accuracy: 0.7559\n",
      "Epoch 9/20\n",
      "21059/21059 [==============================] - 13s 614us/step - loss: 0.0240 - categorical_accuracy: 0.8069 - val_loss: 0.0445 - val_categorical_accuracy: 0.7540\n",
      "Epoch 10/20\n",
      "21059/21059 [==============================] - 14s 670us/step - loss: 0.0235 - categorical_accuracy: 0.8088 - val_loss: 0.0448 - val_categorical_accuracy: 0.7561\n",
      "Epoch 11/20\n",
      "21059/21059 [==============================] - 11s 507us/step - loss: 0.0231 - categorical_accuracy: 0.8104 - val_loss: 0.0447 - val_categorical_accuracy: 0.7584\n",
      "Epoch 12/20\n",
      "21059/21059 [==============================] - 10s 484us/step - loss: 0.0226 - categorical_accuracy: 0.8129 - val_loss: 0.0460 - val_categorical_accuracy: 0.7586\n",
      "Epoch 13/20\n",
      "21059/21059 [==============================] - 11s 521us/step - loss: 0.0223 - categorical_accuracy: 0.8162 - val_loss: 0.0453 - val_categorical_accuracy: 0.7590\n",
      "Epoch 14/20\n",
      "21059/21059 [==============================] - 12s 588us/step - loss: 0.0220 - categorical_accuracy: 0.8179 - val_loss: 0.0472 - val_categorical_accuracy: 0.7586\n",
      "Epoch 15/20\n",
      "21059/21059 [==============================] - 11s 499us/step - loss: 0.0217 - categorical_accuracy: 0.8199 - val_loss: 0.0465 - val_categorical_accuracy: 0.7571\n",
      "Epoch 16/20\n",
      "21059/21059 [==============================] - 12s 567us/step - loss: 0.0216 - categorical_accuracy: 0.8222 - val_loss: 0.0478 - val_categorical_accuracy: 0.7567\n",
      "Epoch 17/20\n",
      "21059/21059 [==============================] - 13s 604us/step - loss: 0.0214 - categorical_accuracy: 0.8229 - val_loss: 0.0489 - val_categorical_accuracy: 0.7556\n",
      "Epoch 18/20\n",
      "21059/21059 [==============================] - 14s 680us/step - loss: 0.0212 - categorical_accuracy: 0.8244 - val_loss: 0.0498 - val_categorical_accuracy: 0.7540\n",
      "Epoch 19/20\n",
      "21059/21059 [==============================] - 14s 645us/step - loss: 0.0211 - categorical_accuracy: 0.8243 - val_loss: 0.0500 - val_categorical_accuracy: 0.7525\n",
      "Epoch 20/20\n",
      "21059/21059 [==============================] - 14s 648us/step - loss: 0.0210 - categorical_accuracy: 0.8256 - val_loss: 0.0500 - val_categorical_accuracy: 0.7523\n",
      "6580/6580 [==============================] - 1s 182us/step\n",
      "loss :  0.031716898623797306\n",
      "categorical accuracy:  0.8088145852088928\n",
      "####################################\n",
      "For threshold:  0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user1/DGS-env/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples-average quality numbers\n",
      "Precision: 0.5845, Recall: 0.9364, F1-measure: 0.6883\n",
      "For threshold:  0.04\n",
      "Samples-average quality numbers\n",
      "Precision: 0.6370, Recall: 0.9291, F1-measure: 0.7251\n",
      "For threshold:  0.06\n",
      "Samples-average quality numbers\n",
      "Precision: 0.6680, Recall: 0.9242, F1-measure: 0.7460\n",
      "For threshold:  0.08\n",
      "Samples-average quality numbers\n",
      "Precision: 0.6937, Recall: 0.9199, F1-measure: 0.7629\n",
      "For threshold:  0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user1/DGS-env/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples-average quality numbers\n",
      "Precision: 0.7152, Recall: 0.9148, F1-measure: 0.7765\n",
      "For threshold:  0.12\n",
      "Samples-average quality numbers\n",
      "Precision: 0.7332, Recall: 0.9098, F1-measure: 0.7874\n",
      "For threshold:  0.14\n",
      "Samples-average quality numbers\n",
      "Precision: 0.7473, Recall: 0.9034, F1-measure: 0.7952\n",
      "For threshold:  0.16\n",
      "Samples-average quality numbers\n",
      "Precision: 0.7595, Recall: 0.8979, F1-measure: 0.8018\n",
      "For threshold:  0.2\n",
      "Samples-average quality numbers\n",
      "Precision: 0.7778, Recall: 0.8873, F1-measure: 0.8109\n",
      "For threshold:  0.25\n",
      "Samples-average quality numbers\n",
      "Precision: 0.7932, Recall: 0.8714, F1-measure: 0.8161\n",
      "For threshold:  0.3\n",
      "Samples-average quality numbers\n",
      "Precision: 0.8026, Recall: 0.8562, F1-measure: 0.8170\n",
      "For threshold:  0.35\n",
      "Samples-average quality numbers\n",
      "Precision: 0.8122, Recall: 0.8459, F1-measure: 0.8197\n",
      "For threshold:  0.4\n",
      "Samples-average quality numbers\n",
      "Precision: 0.8152, Recall: 0.8332, F1-measure: 0.8171\n",
      "For threshold:  0.5\n",
      "Samples-average quality numbers\n",
      "Precision: 0.8057, Recall: 0.8004, F1-measure: 0.7992\n",
      "For threshold:  0.6\n",
      "Samples-average quality numbers\n",
      "Precision: 0.7672, Recall: 0.7560, F1-measure: 0.7590\n",
      "For threshold:  0.7\n",
      "Samples-average quality numbers\n",
      "Precision: 0.6809, Recall: 0.6702, F1-measure: 0.6733\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras_self_attention import SeqSelfAttention\n",
    "\n",
    "inputs = keras.layers.Input(shape=(1,1000))\n",
    "#lstm = keras.layers.Bidirectional(keras.layers.LSTM(units=256,\n",
    "                                                    #return_sequences=True))(inputs)\n",
    "att = SeqSelfAttention(attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL,\n",
    "                       kernel_regularizer=keras.regularizers.l2(1e-4),\n",
    "                       bias_regularizer=keras.regularizers.l1(1e-4),\n",
    "                       attention_regularizer_weight=1e-4,\n",
    "                       name='Attention')(inputs)\n",
    "\n",
    "flatten = keras.layers.Flatten(name='Flatten')(att)\n",
    "dense = keras.layers.Dense(units=y_train.shape[1], name='Dense')(flatten)\n",
    "\n",
    "model = keras.models.Model(inputs=inputs, outputs=[dense])\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n",
    "\n",
    "\n",
    "epochs = 20\n",
    "batch_size = 32\n",
    "\n",
    "history = model.fit(X_train_, y_train, epochs=epochs, batch_size=batch_size,validation_split=0.2)\n",
    "\n",
    "score,cat_acc = model.evaluate(X_test_,y_test)\n",
    "\n",
    "y_pred = model.predict(X_test_)\n",
    "\n",
    "print('loss : ', score)\n",
    "print('categorical accuracy: ',cat_acc)\n",
    "\n",
    "print('####################################')\n",
    "\n",
    "thresholds = [0.01,0.04,0.06,0.08,0.1,0.12,0.14,0.16,0.2,0.25,0.3,0.35,0.4,0.5,0.6,0.7]\n",
    "for val in thresholds:\n",
    "    print(\"For threshold: \", val)\n",
    "    pred=y_pred.copy()\n",
    "  \n",
    "    pred[pred>=val]=1\n",
    "    pred[pred<val]=0\n",
    "  \n",
    "    precision = precision_score(y_test, pred, average='samples')\n",
    "    recall = recall_score(y_test, pred, average='samples')\n",
    "    f1 = f1_score(y_test, pred, average='samples')\n",
    "   \n",
    "    print(\"Samples-average quality numbers\")\n",
    "    print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commentaire : \n",
    "\n",
    "Le modèle semble réellement bloqué a une valeure seuil de 0.67. Que ce soit des réseau recurent ou bien des mécanismes d'attention le score reste identique après 10 epochs. Nous allons donc changer de représentation car nous pensons avoir atteint les limite de la représentation en tfidf+SVD.\n",
    "\n",
    "## Et si l'on changait de représentation ? Utilisation d'Embedding globaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = train['DESCRIPTION_INCIDENT']+'. '+train['ETAT_PATIENT']#+ '. '+train['ACTION_PATIENT']\n",
    "df_test = test['DESCRIPTION_INCIDENT']+'. '+test['ETAT_PATIENT']#+'. '+test['ACTION_PATIENT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61213\n",
      "(26324, 300) (6580, 300)\n"
     ]
    }
   ],
   "source": [
    "# The maximum number of words to be used. \n",
    "MAX_NB_WORDS = 100000\n",
    "# Max number of words in each complaint.\n",
    "MAX_SEQUENCE_LENGTH = 300\n",
    "# This is fixed.\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(df_train.values)\n",
    "word_index = tokenizer.word_index\n",
    "print(len(word_index))\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(df_train.values)\n",
    "X_test = tokenizer.texts_to_sequences(df_test.values)\n",
    "word2index_inputs =  tokenizer.word_index\n",
    "\n",
    "X_train = pad_sequences(X_train,MAX_SEQUENCE_LENGTH)\n",
    "X_test = pad_sequences(X_test,MAX_SEQUENCE_LENGTH)\n",
    "print(X_train.shape,X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user1/DGS-env/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 21059 samples, validate on 5265 samples\n",
      "Epoch 1/10\n",
      "21059/21059 [==============================] - 754s 36ms/step - loss: 0.0293 - categorical_accuracy: 0.8019 - val_loss: 0.0347 - val_categorical_accuracy: 0.7586\n",
      "Epoch 2/10\n",
      "21059/21059 [==============================] - 824s 39ms/step - loss: 0.0244 - categorical_accuracy: 0.8111 - val_loss: 0.0344 - val_categorical_accuracy: 0.7605\n",
      "Epoch 3/10\n",
      "21059/21059 [==============================] - 829s 39ms/step - loss: 0.0208 - categorical_accuracy: 0.8249 - val_loss: 0.0343 - val_categorical_accuracy: 0.7567\n",
      "Epoch 4/10\n",
      "21059/21059 [==============================] - 815s 39ms/step - loss: 0.0169 - categorical_accuracy: 0.8441 - val_loss: 0.0341 - val_categorical_accuracy: 0.7404\n",
      "Epoch 5/10\n",
      "21059/21059 [==============================] - 800s 38ms/step - loss: 0.0133 - categorical_accuracy: 0.8703 - val_loss: 0.0384 - val_categorical_accuracy: 0.7185\n",
      "Epoch 6/10\n",
      "14208/21059 [===================>..........] - ETA: 4:06 - loss: 0.0101 - categorical_accuracy: 0.8951"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X_train.shape[1]))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(200))\n",
    "model.add(Dense(y_train.shape[1],activation='softmax'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,validation_split=0.2)\n",
    "\n",
    "\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print('loss : ', score)\n",
    "print('categorical accuracy: ',cat_acc)\n",
    "\n",
    "print('####################################')\n",
    "\n",
    "thresholds = [0.01,0.04,0.06,0.08,0.1,0.12,0.14,0.16,0.2,0.25,0.3,0.35,0.4,0.5,0.6,0.7]\n",
    "for val in thresholds:\n",
    "    print(\"For threshold: \", val)\n",
    "    pred=y_pred.copy()\n",
    "  \n",
    "    pred[pred>=val]=1\n",
    "    pred[pred<val]=0\n",
    "  \n",
    "    precision = precision_score(y_test, pred, average='samples')\n",
    "    recall = recall_score(y_test, pred, average='samples')\n",
    "    f1 = f1_score(y_test, pred, average='samples')\n",
    "   \n",
    "    print(\"Samples-average quality numbers\")\n",
    "    print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commentaire : \n",
    "Les embedding appris n'offre pas de meilleurs résultats pour l'instant. Nous allons essayer de créer des embeddings par colonnes de texte, cette solution c'etait avérée fructueuse dans le cas du tfidf.\n",
    "\n",
    "## Et si l'on séparait les entrées ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model constants.\n",
    "EMBEDDING_DIM =300\n",
    "MAX_SEQUENCE_LENGTH =300\n",
    "MAX_NB_WORDS = 50000\n",
    "\n",
    "def vectorize(df_train,df_test,MAX_NB_WORDS,MAX_SEQUENCE_LENGTH ):\n",
    "    tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "    tokenizer.fit_on_texts(df_train.values)\n",
    "    word_index = tokenizer.word_index\n",
    "    print(len(word_index))\n",
    "\n",
    "    X_train = tokenizer.texts_to_sequences(df_train.values)\n",
    "    X_test = tokenizer.texts_to_sequences(df_test.values)\n",
    "    word2index_inputs =  tokenizer.word_index\n",
    "\n",
    "    X_train = pad_sequences(X_train,MAX_SEQUENCE_LENGTH)\n",
    "    X_test = pad_sequences(X_test,MAX_SEQUENCE_LENGTH)\n",
    "    return (X_train, X_test)\n",
    "TRAIN = []\n",
    "for col in ['DESCRIPTION_INCIDENT', 'ETAT_PATIENT', 'FABRICANT'] : \n",
    "    X_train,X_test = vectorize(train[col],test[col],MAX_NB_WORDS,MAX_SEQUENCE_LENGTH )\n",
    "    TRAIN.append((X_train,X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_1 = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "inputs_2 = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "inputs_3 = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "#x = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH)(inputs)\n",
    "#x = Reshape((MAX_SEQUENCE_LENGTH,1,))(inputs_1)\n",
    "x = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH)(inputs_1)\n",
    "x = SpatialDropout1D(0.2)(x)\n",
    "x = LSTM(200)(x)\n",
    "\n",
    "\n",
    "#y = Reshape((MAX_SEQUENCE_LENGTH,1,))(inputs_2)\n",
    "y = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH)(inputs_2)\n",
    "y = SpatialDropout1D(0.2)(y)\n",
    "y = LSTM(200)(y)\n",
    "\n",
    "\n",
    "#z = Reshape((MAX_SEQUENCE_LENGTH,1,))(inputs_3)\n",
    "z = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH)(inputs_3)\n",
    "z = SpatialDropout1D(0.2)(z)\n",
    "z = LSTM(200)(z)\n",
    "\n",
    "\n",
    "w = concatenate([x, y, z])\n",
    "\n",
    "out =  Dense(y_train.shape[1],activation='softmax')(w)\n",
    "\n",
    "model = keras.models.Model(inputs=[inputs_1,inputs_2,inputs_3], outputs=out)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit([TRAIN[0][0],TRAIN[1][0],TRAIN[2][0]], y_train, epochs=10, validation_split=0.2, verbose=1, batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict([TRAIN[0][1],TRAIN[1][1],TRAIN[2][1]])\n",
    "\n",
    "print('loss : ', score)\n",
    "print('categorical accuracy: ',cat_acc)\n",
    "\n",
    "print('####################################')\n",
    "\n",
    "thresholds = [0.01,0.04,0.06,0.08,0.1,0.12,0.14,0.16,0.2,0.25,0.3,0.35,0.4,0.5,0.6,0.7]\n",
    "for val in thresholds:\n",
    "    print(\"For threshold: \", val)\n",
    "    pred=y_pred.copy()\n",
    "  \n",
    "    pred[pred>=val]=1\n",
    "    pred[pred<val]=0\n",
    "  \n",
    "    precision = precision_score(y_test, pred, average='samples')\n",
    "    recall = recall_score(y_test, pred, average='samples')\n",
    "    f1 = f1_score(y_test, pred, average='samples')\n",
    "   \n",
    "    print(\"Samples-average quality numbers\")\n",
    "    print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Commentaire :\n",
    "Nous observons que notre modèle sur apprend très rapidement et de manière importante. nous avons deux solutions classiques pour contrer cet effet : \n",
    "- Regularisation\n",
    "- Drop Out\n",
    "- Netoyer les données avec clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp =spacy.load('fr')\n",
    "from spacy.lang.fr.stop_words import STOP_WORDS\n",
    "\n",
    "import clean_text\n",
    "stop_words = STOP_WORDS\n",
    "\n",
    "\n",
    "def vectorize(df_train,df_test,MAX_NB_WORDS,MAX_SEQUENCE_LENGTH,stopword = False ):\n",
    "    tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "    if stopword==True : \n",
    "        df_train = df_train.map(lambda x: clean_text.preprocess_text(x))\n",
    "        df_train = df_train.map(lambda x: [item for item in x.split(\" \") if item not in stop_words])\n",
    "    \n",
    "    tokenizer.fit_on_texts(df_train.values)\n",
    "    word_index = tokenizer.word_index\n",
    "    print(len(word_index))\n",
    "\n",
    "    X_train = tokenizer.texts_to_sequences(df_train.values)\n",
    "    X_test = tokenizer.texts_to_sequences(df_test.values)\n",
    "    word2index_inputs =  tokenizer.word_index\n",
    "\n",
    "    X_train = pad_sequences(X_train,MAX_SEQUENCE_LENGTH)\n",
    "    X_test = pad_sequences(X_test,MAX_SEQUENCE_LENGTH)\n",
    "    return (X_train, X_test)\n",
    "TRAIN = []\n",
    "for col in ['DESCRIPTION_INCIDENT', 'ETAT_PATIENT', 'FABRICANT'] : \n",
    "    X_train,X_test = vectorize(train[col],test[col],MAX_NB_WORDS,MAX_SEQUENCE_LENGTH,stopword=True )\n",
    "    TRAIN.append((X_train,X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inputs_1 = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "inputs_2 = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "inputs_3 = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "#x = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH)(inputs)\n",
    "#x = Reshape((MAX_SEQUENCE_LENGTH,1,))(inputs_1)\n",
    "x = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH)(inputs_1)\n",
    "x = SpatialDropout1D(0.2)(x)\n",
    "x = LSTM(200)(x)\n",
    "\n",
    "\n",
    "#y = Reshape((MAX_SEQUENCE_LENGTH,1,))(inputs_2)\n",
    "y = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH)(inputs_2)\n",
    "y = SpatialDropout1D(0.2)(y)\n",
    "y = LSTM(200)(y)\n",
    "\n",
    "\n",
    "#z = Reshape((MAX_SEQUENCE_LENGTH,1,))(inputs_3)\n",
    "z = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH)(inputs_3)\n",
    "z = SpatialDropout1D(0.2)(z)\n",
    "z = LSTM(200)(z)\n",
    "\n",
    "\n",
    "w = concatenate([x, y, z])\n",
    "\n",
    "out =  Dense(y_train.shape[1],activation='softmax')(w)\n",
    "\n",
    "model = keras.models.Model(inputs=[inputs_1,inputs_2,inputs_3], outputs=out)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit([TRAIN[0][0],TRAIN[1][0],TRAIN[2][0]], y_train, epochs=5, validation_split=0.2, verbose=1, batch_size = 64)\n",
    "\n",
    "y_pred = model.predict([TRAIN[0][1],TRAIN[1][1],TRAIN[2][1]])\n",
    "\n",
    "print('loss : ', score)\n",
    "print('categorical accuracy: ',cat_acc)\n",
    "\n",
    "print('####################################')\n",
    "\n",
    "thresholds = [0.01,0.04,0.06,0.08,0.1,0.12,0.14,0.16,0.2,0.25,0.3,0.35,0.4,0.5,0.6,0.7]\n",
    "for val in thresholds:\n",
    "    print(\"For threshold: \", val)\n",
    "    pred=y_pred.copy()\n",
    "  \n",
    "    pred[pred>=val]=1\n",
    "    pred[pred<val]=0\n",
    "  \n",
    "    precision = precision_score(y_test, pred, average='samples')\n",
    "    recall = recall_score(y_test, pred, average='samples')\n",
    "    f1 = f1_score(y_test, pred, average='samples')\n",
    "   \n",
    "    print(\"Samples-average quality numbers\")\n",
    "    print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion : \n",
    " Suite aux diverses expériences que nousa vosn mener dans ce notebook, nos conclusions sont : \n",
    "- La tf-idf est une représentation riche qui est difficile à battre tant en termes de performances qu'en temps de calcul.\n",
    "- Les modèles d'attention régularisés évitent mieux le sur-apprentissage que les modèles recurent avec du drop out\n",
    "- séparer les collones pour encoder offre de meillers résultats mais toujours moins bon que ce proposé par le TF-iDF\n",
    "- Lef ait d'enlever les stop_words n'est pas \n",
    "\n",
    "\n",
    "Ce que nous devons essayer : \n",
    "- Multi head attention https://www.kaggle.com/fareise/multi-head-self-attention-for-text-classification, https://github.com/CyberZHG/keras-multi-head\n",
    "- Hierarchical attention : https://paperswithcode.com/paper/hierarchical-attentional-hybrid-neural\n",
    "- Concatenation des embedings et du tfidf\n",
    "- chercher de nouvelles méthodes de régularisation pour les réseaux récurrents\n",
    "- tester les CNN : https://www.kaggle.com/sanikamal/text-classification-with-python-and-keras\n",
    "- ajouter une couche de positinal encoding : https://github.com/kaushalshetty/Positional-Encoding\n",
    "- Librairie à essyaer rapidement :\n",
    "    - text-classification-keras : https://pypi.org/project/text-classification-keras/\n",
    "    - pytext :  https://github.com/facebookresearch/pytext\n",
    "\n",
    "- approche avec des emmbedings déjà entrainés : \n",
    "    - https://adventuresinmachinelearning.com/word2vec-keras-tutorial/\n",
    "    - https://medium.com/@ppasumarthi_69210/word-embeddings-in-keras-be6bb3092831\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DGS-env",
   "language": "python",
   "name": "dgs-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
