{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Inférence de l'effet - Stratégie Multilabels - Approche deep Learning\n",
    "Dans ce Notebook, nous cosntruisons un modèle qui permet d'inférer l'EFFET à partir de la classification de l'incident et des données textuelles en ce basant sur des reseau récurents commes GRU/LSTM etc.\n",
    "\n",
    "En effet, ces approches ont montré des réultats très encourageant et nous voulons explorer cette direction pour peut être faire des réceau recurent notre modèle par défault.\n",
    "\n",
    "Nous considérons ce problème comme un problème de classification multiclasses et multilabels. En effet, il y a plusieurs effets possibles et un incidents peut entrainer plusieurs effets.\n",
    "\n",
    "Dans le premier notebook nous nous posons les questions suivantes : \n",
    "- Quel est l'impact du drop out ?\n",
    "- Rajouter des couches augmentent-ils les performaces ?\n",
    "- L'utilisation de réseaux bidirectionnel est-elle pertinente ?\n",
    "- Une couche d'attention est-elle utile ?\n",
    "- Attention is all we need, really ?\n",
    "- Utilisation des embeddings \n",
    "- Concaténation des modèles sur différentes entrées ?\n",
    "\n",
    "Dans celui-ci, nous allons essayer les choses suivantes : \n",
    "\n",
    "Ce que nous devons essayer : \n",
    "- Multi head attention https://www.kaggle.com/fareise/multi-head-self-attention-for-text-classification, https://github.com/CyberZHG/keras-multi-head\n",
    "- Hierarchical attention : https://paperswithcode.com/paper/hierarchical-attentional-hybrid-neural\n",
    "- Concatenation des embedings et du tfidf\n",
    "- chercher de nouvelles méthodes de régularisation pour les réseaux récurrents\n",
    "- tester les CNN : https://www.kaggle.com/sanikamal/text-classification-with-python-and-keras\n",
    "- Librairie à essyaer rapidement :\n",
    "    - text-classification-keras : https://pypi.org/project/text-classification-keras/\n",
    "    - pytext :  https://github.com/facebookresearch/pytext\n",
    "\n",
    "- approche avec des emmbedings déjà entrainés : \n",
    "    - https://adventuresinmachinelearning.com/word2vec-keras-tutorial/\n",
    "    - https://medium.com/@ppasumarthi_69210/word-embeddings-in-keras-be6bb3092831"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,LSTM,Embedding,SpatialDropout1D, Bidirectional,SimpleRNN,Input, concatenate, Reshape,Input,Lambda,Conv1D\n",
    "import tensorflow \n",
    "import keras\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, balanced_accuracy_score,f1_score,classification_report,recall_score,precision_score\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer\n",
    "from keras.layers import Concatenate, GlobalMaxPool1D, Dropout, GlobalMaxPooling1D\n",
    "from tensorflow.keras.layers import Attention\n",
    "\n",
    "tensorflow.random.set_seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "train = pd.read_pickle('./data_split/train.pkl')\n",
    "# Pour faire un modèle sans le \n",
    "#train = train[~train['TEF_ID'].map(lambda x : 106 in x)]\n",
    "X_train = train[['FABRICANT','CLASSIFICATION','DESCRIPTION_INCIDENT','ETAT_PATIENT']]\n",
    "y_train = mlb.fit_transform(train['TEF_ID'])\n",
    "test =  pd.read_pickle('./data_split/test.pkl')\n",
    "#test = test[~test['TEF_ID'].map(lambda x : k in x)]\n",
    "X_test = test[['FABRICANT','CLASSIFICATION','DESCRIPTION_INCIDENT','ETAT_PATIENT']]\n",
    "y_test = mlb.transform(test['TEF_ID'])\n",
    "\n",
    "\n",
    "X_train_dgs = np.load('results/dgs_camenbert_train_vec.npy')\n",
    "X_test_dgs =np.load('results/dgs_camenbert_test_vec.npy')\n",
    "\n",
    "\n",
    "\n",
    "df_effets = pd.read_csv(\"data/ref_MRV/referentiel_dispositif_effets_connus.csv\",delimiter=';',encoding='ISO-8859-1')\n",
    "df_dys = pd.read_csv(\"data/ref_MRV/referentiel_dispositif_dysfonctionnement.csv\",delimiter=';',encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 text-classification-keras https://pypi.org/project/text-classification-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "attempted relative import beyond top-level package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: attempted relative import beyond top-level package"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "from __future__ import absolute_import\n",
    "\n",
    "from keras.layers import LSTM, Bidirectional, Conv1D, Dropout, GlobalAveragePooling1D, GlobalMaxPooling1D, MaxPooling1D, Dense, Flatten, GRU\n",
    "from keras.layers.merge import Concatenate, concatenate\n",
    "\n",
    "from layers import AttentionLayer\n",
    "from ..utils.format import to_fixed_digits\n",
    "\n",
    "\n",
    "class SequenceEncoderBase(object):\n",
    "\n",
    "    def __init__(self, dropout_rate=0.5):\n",
    "        \"\"\"Creates a new instance of sequence encoder.\n",
    "        Args:\n",
    "            dropout_rate: The final encoded output dropout.\n",
    "        \"\"\"\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\"Build the actual model here.\n",
    "        Args:\n",
    "            x: The encoded or embedded input sequence.\n",
    "        Returns:\n",
    "            The model output tensor.\n",
    "        \"\"\"\n",
    "\n",
    "        x = self.build_model(x)\n",
    "        if self.dropout_rate > 0:\n",
    "            x = Dropout(self.dropout_rate)(x)\n",
    "        return x\n",
    "\n",
    "    def build_model(self, x):\n",
    "        \"\"\"Build your model graph here.\n",
    "        Args:\n",
    "            x: The encoded or embedded input sequence.\n",
    "        Returns:\n",
    "            The model output tensor without the classification block.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def allows_dynamic_length(self):\n",
    "        \"\"\"Return a boolean indicating whether this model is capable of handling variable time steps per mini-batch.\n",
    "        For example, this should be True for RNN models since you can use them with variable time steps per mini-batch.\n",
    "        CNNs on the other hand expect fixed time steps across all mini-batches.\n",
    "        \"\"\"\n",
    "        # Assume default as False. Should be overridden as necessary.\n",
    "        return False\n",
    "\n",
    "class AttentionRNN(SequenceEncoderBase):\n",
    "\n",
    "    def __init__(self, rnn_class=LSTM, encoder_dims=50, bidirectional=True, dropout_rate=0.5, **rnn_kwargs):\n",
    "        \"\"\"Creates an RNN model with attention. The attention mechanism is implemented as described\n",
    "        in https://www.cs.cmu.edu/~hovy/papers/16HLT-hierarchical-attention-networks.pdf, but without\n",
    "        sentence level attention.\n",
    "        Args:\n",
    "            rnn_class: The type of RNN to use. (Default Value = LSTM)\n",
    "            encoder_dims: The number of hidden units of RNN. (Default Value: 50)\n",
    "            bidirectional: Whether to use bidirectional encoding. (Default Value = True)\n",
    "            **rnn_kwargs: Additional args for building the RNN.\n",
    "        \"\"\"\n",
    "        super(AttentionRNN, self).__init__(dropout_rate)\n",
    "        self.rnn_class = rnn_class\n",
    "        self.encoder_dims = encoder_dims\n",
    "        self.bidirectional = bidirectional\n",
    "        self.rnn_kwargs = rnn_kwargs\n",
    "\n",
    "    def build_model(self, x):\n",
    "        rnn = self.rnn_class(\n",
    "            self.encoder_dims, return_sequences=True, **self.rnn_kwargs)\n",
    "        if self.bidirectional:\n",
    "            word_activations = Bidirectional(rnn)(x)\n",
    "        else:\n",
    "            word_activations = rnn(x)\n",
    "\n",
    "        attention_layer = AttentionLayer()\n",
    "        doc_vector = attention_layer(word_activations)\n",
    "        self.attention_tensor = attention_layer.get_attention_tensor()\n",
    "        return doc_vector\n",
    "\n",
    "    def get_attention_tensor(self):\n",
    "        if not hasattr(self, 'attention_tensor'):\n",
    "            raise ValueError('You need to build the model first')\n",
    "        return self.attention_tensor\n",
    "\n",
    "    def allows_dynamic_length(self):\n",
    "        return True\n",
    "\n",
    "    def __str__(self):\n",
    "        bi = 'bi' if self.bidirectional else 'nobi'\n",
    "        rnn_kwargs_str = str(self.rnn_kwargs) if len(\n",
    "            self.rnn_kwargs) > 0 else ''\n",
    "        li = ['stacked', str(self.rnn_class), str(self.encoder_dims),\n",
    "              bi, 'do', to_fixed_digits(self.dropout_rate), rnn_kwargs_str]\n",
    "\n",
    "        return '_'.join(li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.train(x=ds.X, y=ds.y, validation_split=0.1, model=model,\n",
    "    word_encoder_model=word_encoder_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ = np.reshape(X_train_, (X_train_.shape[0], 1, X_train_.shape[1]))\n",
    "X_test_ = np.reshape(X_test_, (X_test_.shape[0], 1, X_test_.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience 2 :  RCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57288\n",
      "21168\n",
      "2262\n"
     ]
    }
   ],
   "source": [
    "# Model constants.\n",
    "EMBEDDING_DIM =300\n",
    "MAX_SEQUENCE_LENGTH =300\n",
    "MAX_NB_WORDS = 50000\n",
    "\n",
    "def vectorize(df_train,df_test,MAX_NB_WORDS,MAX_SEQUENCE_LENGTH ):\n",
    "    tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "    tokenizer.fit_on_texts(df_train.values)\n",
    "    word_index = tokenizer.word_index\n",
    "    print(len(word_index))\n",
    "\n",
    "    X_train = tokenizer.texts_to_sequences(df_train.values)\n",
    "    X_test = tokenizer.texts_to_sequences(df_test.values)\n",
    "    word2index_inputs =  tokenizer.word_index\n",
    "\n",
    "    X_train = pad_sequences(X_train,MAX_SEQUENCE_LENGTH)\n",
    "    X_test = pad_sequences(X_test,MAX_SEQUENCE_LENGTH)\n",
    "    return (X_train, X_test)\n",
    "\n",
    "\n",
    "TRAIN = []\n",
    "for col in ['DESCRIPTION_INCIDENT', 'ETAT_PATIENT', 'FABRICANT'] : \n",
    "    X_train,X_test = vectorize(train[col],test[col],MAX_NB_WORDS,MAX_SEQUENCE_LENGTH )\n",
    "    TRAIN.append((X_train,X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_22 (InputLayer)           (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_23 (InputLayer)           (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_24 (InputLayer)           (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_12 (Embedding)        (None, 300, 300)     15000000    input_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_13 (Embedding)        (None, 300, 300)     15000000    input_23[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_14 (Embedding)        (None, 300, 300)     15000000    input_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "simple_rnn_16 (SimpleRNN)       (None, 300, 128)     54912       embedding_12[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "simple_rnn_18 (SimpleRNN)       (None, 300, 128)     54912       embedding_13[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "simple_rnn_20 (SimpleRNN)       (None, 300, 128)     54912       embedding_14[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "simple_rnn_15 (SimpleRNN)       (None, 300, 128)     54912       embedding_12[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lambda_7 (Lambda)               (None, 300, 128)     0           simple_rnn_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "simple_rnn_17 (SimpleRNN)       (None, 300, 128)     54912       embedding_13[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lambda_8 (Lambda)               (None, 300, 128)     0           simple_rnn_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "simple_rnn_19 (SimpleRNN)       (None, 300, 128)     54912       embedding_14[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lambda_9 (Lambda)               (None, 300, 128)     0           simple_rnn_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 300, 256)     0           simple_rnn_15[0][0]              \n",
      "                                                                 lambda_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 300, 256)     0           simple_rnn_17[0][0]              \n",
      "                                                                 lambda_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 300, 256)     0           simple_rnn_19[0][0]              \n",
      "                                                                 lambda_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 300, 64)      16448       concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 300, 64)      16448       concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 300, 64)      16448       concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 64)           0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_4 (GlobalM (None, 64)           0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_5 (GlobalM (None, 64)           0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 192)          0           global_max_pooling1d_3[0][0]     \n",
      "                                                                 global_max_pooling1d_4[0][0]     \n",
      "                                                                 global_max_pooling1d_5[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 273)          52689       concatenate_11[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 45,431,505\n",
      "Trainable params: 45,431,505\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs_1 = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "inputs_2 = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "inputs_3 = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "#x = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH)(inputs)\n",
    "#x = Reshape((MAX_SEQUENCE_LENGTH,1,))(inputs_1)\n",
    "\n",
    "x = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH)(inputs_1)\n",
    "x_1 = SimpleRNN(128, return_sequences=True)(x)\n",
    "x_2 = SimpleRNN(128, return_sequences=True, go_backwards=True)(x)\n",
    "x_2 = Lambda(lambda x: tensorflow.reverse(x, axis=[1]))(x_2)\n",
    "x = Concatenate(axis=2)([x_1,x_2])\n",
    "x = Conv1D(64, kernel_size=1, activation='tanh')(x)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "\n",
    "\n",
    "\n",
    "#y = Reshape((MAX_SEQUENCE_LENGTH,1,))(inputs_2)\n",
    "y = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH)(inputs_2)\n",
    "y_1 = SimpleRNN(128, return_sequences=True)(y)\n",
    "y_2 = SimpleRNN(128, return_sequences=True, go_backwards=True)(y)\n",
    "y_2 = Lambda(lambda x: tensorflow.reverse(x, axis=[1]))(y_2)\n",
    "y = Concatenate(axis=2)([y_1,y_2])\n",
    "y = Conv1D(64, kernel_size=1, activation='tanh')(y)\n",
    "y = GlobalMaxPooling1D()(y)\n",
    "\n",
    "\n",
    "#z = Reshape((MAX_SEQUENCE_LENGTH,1,))(inputs_3)\n",
    "z = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH)(inputs_3)\n",
    "z_1 = SimpleRNN(128, return_sequences=True)(z)\n",
    "z_2 = SimpleRNN(128, return_sequences=True, go_backwards=True)(z)\n",
    "z_2 = Lambda(lambda x: tensorflow.reverse(x, axis=[1]))(z_2)\n",
    "z = Concatenate(axis=2)([z_1,z_2])\n",
    "z = Conv1D(64, kernel_size=1, activation='tanh')(z)\n",
    "z = GlobalMaxPooling1D()(z)\n",
    "\n",
    "w = concatenate([x, y, z])\n",
    "\n",
    "out =  Dense(y_train.shape[1],activation='softmax')(w)\n",
    "\n",
    "model = keras.models.Model(inputs=[inputs_1,inputs_2,inputs_3], outputs=out)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user1/DGS-env/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 21059 samples, validate on 5265 samples\n",
      "Epoch 1/5\n",
      "21059/21059 [==============================] - 506s 24ms/step - loss: 0.0185 - categorical_accuracy: 0.5723 - val_loss: 0.0144 - val_categorical_accuracy: 0.5590\n",
      "Epoch 2/5\n",
      "21059/21059 [==============================] - 522s 25ms/step - loss: 0.0159 - categorical_accuracy: 0.5943 - val_loss: 0.0140 - val_categorical_accuracy: 0.5975\n",
      "Epoch 3/5\n",
      "21059/21059 [==============================] - 507s 24ms/step - loss: 0.0147 - categorical_accuracy: 0.6144 - val_loss: 0.0134 - val_categorical_accuracy: 0.6122\n",
      "Epoch 4/5\n",
      "21059/21059 [==============================] - 493s 23ms/step - loss: 0.0139 - categorical_accuracy: 0.6368 - val_loss: 0.0133 - val_categorical_accuracy: 0.6049\n",
      "Epoch 5/5\n",
      "21059/21059 [==============================] - 487s 23ms/step - loss: 0.0131 - categorical_accuracy: 0.6556 - val_loss: 0.0133 - val_categorical_accuracy: 0.6220\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([TRAIN[0][0],TRAIN[1][0],TRAIN[2][0]], y_train, epochs=5, validation_split=0.2, verbose=1, batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################################\n",
      "For threshold:  0.01\n",
      "Samples-average quality numbers\n",
      "Precision: 0.3180, Recall: 0.8617, F1-measure: 0.4037\n",
      "For threshold:  0.04\n",
      "Samples-average quality numbers\n",
      "Precision: 0.5216, Recall: 0.7714, F1-measure: 0.5794\n",
      "For threshold:  0.06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user1/DGS-env/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples-average quality numbers\n",
      "Precision: 0.5691, Recall: 0.7391, F1-measure: 0.6093\n",
      "For threshold:  0.08\n",
      "Samples-average quality numbers\n",
      "Precision: 0.5970, Recall: 0.7106, F1-measure: 0.6204\n",
      "For threshold:  0.1\n",
      "Samples-average quality numbers\n",
      "Precision: 0.6198, Recall: 0.6853, F1-measure: 0.6242\n",
      "For threshold:  0.12\n",
      "Samples-average quality numbers\n",
      "Precision: 0.6335, Recall: 0.6640, F1-measure: 0.6222\n",
      "For threshold:  0.14\n",
      "Samples-average quality numbers\n",
      "Precision: 0.6413, Recall: 0.6517, F1-measure: 0.6211\n",
      "For threshold:  0.16\n",
      "Samples-average quality numbers\n",
      "Precision: 0.6416, Recall: 0.6410, F1-measure: 0.6182\n",
      "For threshold:  0.2\n",
      "Samples-average quality numbers\n",
      "Precision: 0.6261, Recall: 0.6135, F1-measure: 0.6024\n",
      "For threshold:  0.25\n",
      "Samples-average quality numbers\n",
      "Precision: 0.5872, Recall: 0.5840, F1-measure: 0.5779\n",
      "For threshold:  0.3\n",
      "Samples-average quality numbers\n",
      "Precision: 0.5632, Recall: 0.5599, F1-measure: 0.5576\n",
      "For threshold:  0.35\n",
      "Samples-average quality numbers\n",
      "Precision: 0.5532, Recall: 0.5468, F1-measure: 0.5472\n",
      "For threshold:  0.4\n",
      "Samples-average quality numbers\n",
      "Precision: 0.5403, Recall: 0.5320, F1-measure: 0.5340\n",
      "For threshold:  0.5\n",
      "Samples-average quality numbers\n",
      "Precision: 0.5068, Recall: 0.5004, F1-measure: 0.5024\n",
      "For threshold:  0.6\n",
      "Samples-average quality numbers\n",
      "Precision: 0.4748, Recall: 0.4707, F1-measure: 0.4720\n",
      "For threshold:  0.7\n",
      "Samples-average quality numbers\n",
      "Precision: 0.4363, Recall: 0.4336, F1-measure: 0.4345\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict([TRAIN[0][1],TRAIN[1][1],TRAIN[2][1]])\n",
    "print('####################################')\n",
    "\n",
    "thresholds = [0.01,0.04,0.06,0.08,0.1,0.12,0.14,0.16,0.2,0.25,0.3,0.35,0.4,0.5,0.6,0.7]\n",
    "for val in thresholds:\n",
    "    print(\"For threshold: \", val)\n",
    "    pred=y_pred.copy()\n",
    "  \n",
    "    pred[pred>=val]=1\n",
    "    pred[pred<val]=0\n",
    "  \n",
    "    precision = precision_score(y_test, pred, average='samples')\n",
    "    recall = recall_score(y_test, pred, average='samples')\n",
    "    f1 = f1_score(y_test, pred, average='samples')\n",
    "   \n",
    "    print(\"Samples-average quality numbers\")\n",
    "    print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Commentaire :\n",
    "Nous observons que notre modèle sur apprend très rapidement et de manière importante. nous avons deux solutions classiques pour contrer cet effet : \n",
    "- Regularisation\n",
    "- Drop Out\n",
    "- Netoyer les données avec clean text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vriante : https://github.com/airalcorn2/Recurrent-Convolutional-Neural-Network-Text-Classifier/blob/master/recurrent_convolutional_keras.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer,HashingVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "X_train = train[['FABRICANT','CLASSIFICATION','DESCRIPTION_INCIDENT','ETAT_PATIENT']]\n",
    "y_train = mlb.fit_transform(train['TEF_ID'])\n",
    "X_test = test[['FABRICANT','CLASSIFICATION','DESCRIPTION_INCIDENT','ETAT_PATIENT']]\n",
    "y_test = mlb.transform(test['TEF_ID'])\n",
    "\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    [('description_tfidf',TfidfVectorizer(sublinear_tf=True, min_df=3,\n",
    "                            ngram_range=(1, 1),\n",
    "                            \n",
    "                            max_features = 10000,norm = 'l2'), 'DESCRIPTION_INCIDENT'),\n",
    "     \n",
    "     ('etat_pat_tfidf', TfidfVectorizer(sublinear_tf=True, min_df=3,ngram_range=(1, 1),\n",
    "                                       \n",
    "                                       max_features = 10000,norm = 'l2'), 'ETAT_PATIENT'),\n",
    "     \n",
    "     ('fabricant_tfidf',TfidfVectorizer(sublinear_tf=True, min_df=3,\n",
    "                            ngram_range=(1, 1),\n",
    "                            \n",
    "                            max_features = 5000,norm = 'l2'), 'FABRICANT')\n",
    "     ],\n",
    "    \n",
    "    remainder='passthrough')\n",
    "\n",
    "X_train_, X_test_ =preprocess.fit_transform(X_train),preprocess.transform(X_test)\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=1000)\n",
    "X_train_ = svd.fit_transform(X_train_)\n",
    "X_test_ = svd.transform(X_test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ = np.reshape(X_train_, (X_train_.shape[0], 1, X_train_.shape[1]))\n",
    "X_test_ = np.reshape(X_test_, (X_test_.shape[0], 1, X_test_.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend\n",
    "\n",
    "hidden_dim_1 = 200\n",
    "hidden_dim_2 = 100\n",
    "NUM_CLASSES = y_train.shape[1]\n",
    "\n",
    "document = Input(shape = (1,1000, ), dtype = \"float32\")\n",
    "#left_context = Input(shape = (1,1000, ), dtype = \"float32\")\n",
    "#right_context = Input(shape = (1,1000, ), dtype = \"float32\")\n",
    "\n",
    "\n",
    "\n",
    "x_1 = LSTM(200, return_sequences=True)(document)\n",
    "x_2 = LSTM(20, return_sequences=True, go_backwards=True)(document)\n",
    "x_2 = Lambda(lambda x: tensorflow.reverse(x, axis=[1]))(x_2)\n",
    "x = Concatenate(axis=2)([x_1,x_2])\n",
    "x = Conv1D(64, kernel_size=1, activation='tanh')(x)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "\n",
    "\n",
    "\n",
    "out =  Dense(y_train.shape[1],activation='softmax')(x)\n",
    "\n",
    "model = keras.models.Model(inputs=[document], outputs=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 21059 samples, validate on 5265 samples\n",
      "Epoch 1/10\n",
      "21059/21059 [==============================] - 17s 788us/step - loss: 0.0171 - categorical_accuracy: 0.5876 - val_loss: 0.0138 - val_categorical_accuracy: 0.5996\n",
      "Epoch 2/10\n",
      "21059/21059 [==============================] - 15s 732us/step - loss: 0.0147 - categorical_accuracy: 0.6234 - val_loss: 0.0126 - val_categorical_accuracy: 0.6340\n",
      "Epoch 3/10\n",
      "21059/21059 [==============================] - 15s 711us/step - loss: 0.0135 - categorical_accuracy: 0.6528 - val_loss: 0.0122 - val_categorical_accuracy: 0.6391\n",
      "Epoch 4/10\n",
      "21059/21059 [==============================] - 15s 720us/step - loss: 0.0128 - categorical_accuracy: 0.6657 - val_loss: 0.0120 - val_categorical_accuracy: 0.6414\n",
      "Epoch 5/10\n",
      "21059/21059 [==============================] - 16s 736us/step - loss: 0.0123 - categorical_accuracy: 0.6751 - val_loss: 0.0119 - val_categorical_accuracy: 0.6481\n",
      "Epoch 6/10\n",
      "21059/21059 [==============================] - 15s 717us/step - loss: 0.0118 - categorical_accuracy: 0.6846 - val_loss: 0.0120 - val_categorical_accuracy: 0.6437\n",
      "Epoch 7/10\n",
      "21059/21059 [==============================] - 17s 815us/step - loss: 0.0115 - categorical_accuracy: 0.6901 - val_loss: 0.0120 - val_categorical_accuracy: 0.6448\n",
      "Epoch 8/10\n",
      "21059/21059 [==============================] - 16s 748us/step - loss: 0.0112 - categorical_accuracy: 0.6978 - val_loss: 0.0120 - val_categorical_accuracy: 0.6397\n",
      "Epoch 9/10\n",
      "21059/21059 [==============================] - 16s 768us/step - loss: 0.0109 - categorical_accuracy: 0.7053 - val_loss: 0.0121 - val_categorical_accuracy: 0.6384\n",
      "Epoch 10/10\n",
      "21059/21059 [==============================] - 15s 715us/step - loss: 0.0106 - categorical_accuracy: 0.7147 - val_loss: 0.0122 - val_categorical_accuracy: 0.6378\n",
      "6580/6580 [==============================] - 2s 268us/step\n",
      "loss :  0.012681970073300835\n",
      "categorical accuracy:  0.6524316072463989\n",
      "####################################\n",
      "For threshold:  0.01\n",
      "Samples-average quality numbers\n",
      "Precision: 0.3606, Recall: 0.9045, F1-measure: 0.4613\n",
      "For threshold:  0.04\n",
      "Samples-average quality numbers\n",
      "Precision: 0.5393, Recall: 0.8327, F1-measure: 0.6125\n",
      "For threshold:  0.06\n",
      "Samples-average quality numbers\n",
      "Precision: 0.5869, Recall: 0.8049, F1-measure: 0.6420\n",
      "For threshold:  0.08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user1/DGS-env/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples-average quality numbers\n",
      "Precision: 0.6180, Recall: 0.7844, F1-measure: 0.6574\n",
      "For threshold:  0.1\n",
      "Samples-average quality numbers\n",
      "Precision: 0.6416, Recall: 0.7623, F1-measure: 0.6644\n",
      "For threshold:  0.12\n",
      "Samples-average quality numbers\n",
      "Precision: 0.6552, Recall: 0.7443, F1-measure: 0.6662\n",
      "For threshold:  0.14\n",
      "Samples-average quality numbers\n",
      "Precision: 0.6595, Recall: 0.7250, F1-measure: 0.6630\n",
      "For threshold:  0.16\n",
      "Samples-average quality numbers\n",
      "Precision: 0.6616, Recall: 0.7095, F1-measure: 0.6599\n",
      "For threshold:  0.2\n",
      "Samples-average quality numbers\n",
      "Precision: 0.6562, Recall: 0.6794, F1-measure: 0.6480\n",
      "For threshold:  0.25\n",
      "Samples-average quality numbers\n",
      "Precision: 0.6411, Recall: 0.6511, F1-measure: 0.6333\n",
      "For threshold:  0.3\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer = \"adam\", loss = \"binary_crossentropy\", metrics = [\"categorical_accuracy\"])\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "history = model.fit(X_train_, y_train, epochs=epochs, batch_size=batch_size,validation_split=0.2)\n",
    "\n",
    "score,cat_acc = model.evaluate(X_test_,y_test)\n",
    "\n",
    "y_pred = model.predict(X_test_)\n",
    "\n",
    "print('loss : ', score)\n",
    "print('categorical accuracy: ',cat_acc)\n",
    "\n",
    "print('####################################')\n",
    "\n",
    "thresholds = [0.01,0.04,0.06,0.08,0.1,0.12,0.14,0.16,0.2,0.25,0.3,0.35,0.4,0.5,0.6,0.7]\n",
    "for val in thresholds:\n",
    "    print(\"For threshold: \", val)\n",
    "    pred=y_pred.copy()\n",
    "  \n",
    "    pred[pred>=val]=1\n",
    "    pred[pred<val]=0\n",
    "  \n",
    "    precision = precision_score(y_test, pred, average='samples')\n",
    "    recall = recall_score(y_test, pred, average='samples')\n",
    "    f1 = f1_score(y_test, pred, average='samples')\n",
    "   \n",
    "    print(\"Samples-average quality numbers\")\n",
    "    print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26324, 1, 999)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_[:,:,].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DGS-env",
   "language": "python",
   "name": "dgs-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
