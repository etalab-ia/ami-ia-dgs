{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inférence de la variable DCO à partir des données de MRV (Stratégie ML 1.1)\n",
    "La variable DCO représente le nom des dispositifs.Il y en a 1500 de différents et notre objectif est d'identifier le dispositif impliqué dans l'incident à partir de deux variables:\n",
    "- DESCRIPTION DE L'INCIDENT\n",
    "- LIBELLE COMMERCIAL\n",
    "\n",
    "Dans ce problème de classification de texte multiclasse, le pipeline sera le suivant :\n",
    "1. Nettoyer les données textuelles (orthographe, ponctuation, majuscule, tokénisation et/ou lemmatisation etc.)\n",
    "2. Nettoyage des observations (suppression des NaN, des targets ou le nombre d'observations est trop faible)\n",
    "3. Construction des features textuelles (CountVectorizer, tf-idf, Word2vec, Fastext, Camenbert etc.)\n",
    "4. Entrainement de l'algorithme de classification\n",
    "5. Evaluation du modèle\n",
    "\n",
    "A travers ce pipeline les choix sont nombreux et nous allons devellopper différentes startégies.\n",
    "L'objectif de cette stratégie est d'identifier les élements qui fonctionnent bien et de comprendre pourquoi :\n",
    "* Quel est l'impact des bigrams ?\n",
    "* Quel est l'impact de la lemmatisation ?\n",
    "* Quel est l'impact des paramètres du SVM sur les performances ?\n",
    "* Quel est l'impact des doublons sur nos performances ?\n",
    "\n",
    "Pui de tester de nouvelles intuitions :\n",
    "\n",
    "**Stratégie ML 2**\n",
    "* A définir ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from time import time\n",
    "import logging\n",
    "\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "lang ='french'\n",
    "\n",
    "import clean_text\n",
    "\n",
    "\n",
    "\n",
    "from scipy.stats import randint\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,HashingVectorizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.feature_extraction.text import TfidfTransformer,CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, balanced_accuracy_score\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.decomposition import TruncatedSVD,IncrementalPCA,SparsePCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "import spacy\n",
    "nlp =spacy.load('fr')\n",
    "from spacy.lang.fr.stop_words import STOP_WORDS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Chargement et nettoyage des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user1/DGS-env/lib/python3.7/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "df_declaration_mrv = pd.read_csv(\"data/data_mrv/declaration_mrv.csv\",delimiter=';',encoding='ISO-8859-1')\n",
    "id_to_dco = pd.read_csv(\"data/ref_MRV/referentiel_dispositif.csv\",delimiter=';',encoding='ISO-8859-1')\n",
    "\n",
    "df = df_declaration_mrv[['DESCRIPTION_INCIDENT','LIBELLE_COMMERCIAL','DCO_ID']]\n",
    "\n",
    "\n",
    "df['Text'] = df['LIBELLE_COMMERCIAL']+ ' ' + df['DESCRIPTION_INCIDENT']\n",
    "\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36463"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.groupby('Text'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remarques :  \n",
    "--> En fesant la jointure des collones descripton et libelle, nous avons 36 000 ligne unique au lieu de 33 000 environ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.42 s, sys: 28 ms, total: 9.45 s\n",
      "Wall time: 9.48 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df.Text = df.Text.map(lambda x: clean_text.preprocess_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 52 ms, sys: 8 ms, total: 60 ms\n",
      "Wall time: 59.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def select_raw_by_nb_obs(df:pd.DataFrame, seuil:int)->pd.DataFrame :\n",
    "    \"\"\"\n",
    "    Renvoie les lignes ou le nombre d'observations est supérieur au seuil entrée\n",
    "    \"\"\"\n",
    "    S = df.groupby('DCO_ID').count()>seuil\n",
    "    liste_DCO =S[S['Text']==True].index\n",
    "    df_utilisable= df[df['DCO_ID'].isin(liste_DCO)]\n",
    "    #df_reduit = df_utilisale[df_utilisale['DCO_ID']>2900]\n",
    "    #print(len(df_reduit))\n",
    "    return(df_utilisable)\n",
    "\n",
    "df_utilisable_10 = select_raw_by_nb_obs(df,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Construction du pipeline et optimisation des paramètres\n",
    "### Impact des paramètres de la tf-idf et de la vectorisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['vect', 'tfidf', 'clf']\n",
      "parameters:\n",
      "{'tfidf__norm': ('l1', 'l2'),\n",
      " 'tfidf__use_idf': (True, False),\n",
      " 'vect__max_features': [5000, 10000],\n",
      " 'vect__min_df': [5],\n",
      " 'vect__ngram_range': ((1, 1), (1, 2))}\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  48 out of  48 | elapsed: 40.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2607.827s\n",
      "\n",
      "Best score: 0.769\n",
      "Best parameters set:\n",
      "\ttfidf__norm: 'l2'\n",
      "\ttfidf__use_idf: True\n",
      "\tvect__max_features: 10000\n",
      "\tvect__min_df: 5\n",
      "\tvect__ngram_range: (1, 1)\n"
     ]
    }
   ],
   "source": [
    "#Pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', LinearSVC(class_weight='balanced')),\n",
    "])\n",
    "\n",
    "# Paramètres\n",
    "parameters = {\n",
    "    #'max_df': [0.75],#(0.5, 0.75, 1.0),\n",
    "    'vect__max_features': [5000,10000],#, 10000, 50000),\n",
    "    'vect__min_df': [5],\n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    'tfidf__use_idf': (True, False),\n",
    "    'tfidf__norm': ('l1', 'l2')\n",
    "    #'clf__C': [1, 10, 100]\n",
    "}\n",
    "#Grid search\n",
    "if __name__ == \"__main__\":\n",
    "   \n",
    "    grid_search = GridSearchCV(pipeline, parameters, cv=3,n_jobs=-1, verbose=1)\n",
    "\n",
    "    print(\"Performing grid search...\")\n",
    "    print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "    print(\"parameters:\")\n",
    "    pprint(parameters)\n",
    "    t0 = time()\n",
    "    grid_search.fit(df_utilisable_10.Text, df_utilisable_10.DCO_ID)\n",
    "    print(\"done in %0.3fs\" % (time() - t0))\n",
    "    print()\n",
    "\n",
    "    print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commentaire : \n",
    " * Les bigrammes n'apporte pas de meilleurs performances\n",
    " * le calcul de l'idf améliore les performances\n",
    " * Il faudrai essayer avec un plus grand nombre de features (max_features>=10000 ?)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Normalisation des données\n",
    "Nous regardons ici, l'impact de la lemmatisation sur les données\n",
    "Attention, il faut avoir le fichier df_text_Libelle_Descr_clean.csv dans le même répertoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "un exemple de donnée : sonde attain performa le guide est resté coincé à intérieur de la sonde , on ne peut plus le bouger . changement de sonde \n",
      "\n",
      "un exemple de donnée lemmatisée :  sond attain performer le guide être resté coincer à intérieur de le sonde , on ne pouvoir plus le bouger . changement de sonde \n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(df_utilisable_10.Text[0])\n",
    "print('un exemple de donnée :', df_utilisable_10.Text[0],'\\n')\n",
    "print('un exemple de donnée lemmatisée : ', \" \".join([elt.lemma_ for elt in doc]),'\\n')\n",
    "#print('un exemple de donnée Normalisée : ',\"\".join([elt.lemma_+'_'+elt.pos_ for elt in doc]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Le texte contient égalemen le pos_tag, cette fonction permet de ne se servir que des lemmes\n",
    "def get_lem(x):\n",
    "\n",
    "    lem = \"\"\n",
    "    for elt in x.split() :\n",
    "        lem = lem +\" \" +elt.split('_')[0]\n",
    "    return(lem)\n",
    "\n",
    "\n",
    "# Pour obtenir quelque statistiques sur les nombres de mots suite à la lemmaisation\n",
    "def nltk_tokenisation(text,sw=True):\n",
    "    \"\"\"\n",
    "    Transforme le texte en liste de tokens, en miniscule, en ayant suprimé la  ponctuatiuon et les mots frequents\n",
    "    Entrées\n",
    "    - x::type:str\n",
    "    Sortie:\n",
    "    - tokens::type:list(str) liste de tokens\n",
    "    - lemmas::type:list(str) liste des lemmes\n",
    "    Exemple : Entrée = \"je suis heureux aujourd'hui\"; Sortie : ['je', 'suis', 'heureux', \"aujourd'hui\"]\n",
    "    \"\"\"\n",
    "    if type(text)!= str :\n",
    "        return ([])\n",
    "    txt = text.lower()\n",
    "    if sw==False :\n",
    "        tokens = nltk.word_tokenize(txt, language=lang, preserve_line=False)\n",
    "    else : \n",
    "        words = nltk.word_tokenize(txt, language=lang, preserve_line=False)\n",
    "        tokens = [word for word in words  if word not in STOP_WORDS]\n",
    "    return(tokens)\n",
    "\n",
    "def nb_mots(x):\n",
    "    try :\n",
    "        return(len(x))\n",
    "    except :\n",
    "        return(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des donnée normalisé\n",
    "df_utilisable_10_norm = pd.read_csv('df_text_Libelle_Descr_clean.csv')\n",
    "df_utilisable_10_norm = df_utilisable_10_norm.dropna()\n",
    "df_utilisable_10_norm['Lem'] = df_utilisable_10_norm.Text.map(lambda x:get_lem(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 10s, sys: 604 ms, total: 1min 11s\n",
      "Wall time: 1min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_utilisable_10_norm['Lem'+'_token'] = df_utilisable_10_norm['Lem'].map(lambda x: nltk_tokenisation(x,sw=False)) # Les mots\n",
    "df_utilisable_10_norm['Lem'+'_nb_mots'] = df_utilisable_10_norm['Lem'+'_token'].map(lambda x:nb_mots(x)) # On renvoie 0 pour les NaN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    76753.000000\n",
       "mean        75.383829\n",
       "std         87.174306\n",
       "min          2.000000\n",
       "25%         28.000000\n",
       "50%         49.000000\n",
       "75%         87.000000\n",
       "max        884.000000\n",
       "Name: Lem_nb_mots, dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_utilisable_10_norm['Lem'+'_nb_mots'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['vect', 'tfidf', 'clf']\n",
      "parameters:\n",
      "{'vect__max_features': [10000],\n",
      " 'vect__min_df': [5, 10],\n",
      " 'vect__ngram_range': [(1, 1)]}\n",
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user1/DGS-env/lib/python3.7/site-packages/sklearn/model_selection/_split.py:672: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   6 | elapsed:  6.6min remaining:  6.6min\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   6 | elapsed:  7.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 925.888s\n",
      "\n",
      "Best score: 0.026\n",
      "Best parameters set:\n",
      "\tvect__max_features: 10000\n",
      "\tvect__min_df: 10\n",
      "\tvect__ngram_range: (1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user1/DGS-env/lib/python3.7/site-packages/sklearn/svm/_base.py:975: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# création du pipeline avec les paramètres précédents\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', LinearSVC(class_weight='balanced')),\n",
    "])\n",
    "\n",
    "# Paramètres\n",
    "parameters = {\n",
    "    #'max_df': [0.75],#(0.5, 0.75, 1.0),\n",
    "    'vect__max_features': [10000],#, 10000, 50000),\n",
    "    'vect__min_df': [5,10],\n",
    "    'vect__ngram_range': [(1, 1)],  # unigrams or bigrams\n",
    "    #'tfidf__use_idf': (True, False),\n",
    "    #'tfidf__norm': ('l1', 'l2')\n",
    "    #'clf__C': [1, 10, 100]\n",
    "}\n",
    "\n",
    "#Grid search\n",
    "if __name__ == \"__main__\":\n",
    "   \n",
    "    grid_search = GridSearchCV(pipeline, parameters, cv=3,n_jobs=-1, verbose=1)\n",
    "\n",
    "    print(\"Performing grid search...\")\n",
    "    print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "    print(\"parameters:\")\n",
    "    pprint(parameters)\n",
    "    t0 = time()\n",
    "    grid_search.fit(df_utilisable_10_norm.Lem, df_utilisable_10_norm.DCO_ID)\n",
    "    print(\"done in %0.3fs\" % (time() - t0))\n",
    "    print()\n",
    "\n",
    "    print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) L'impact des doublons :  changeons le train et test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_10 = select_raw_by_nb_obs(df,10)\n",
    "train_index,test_index = next(GroupShuffleSplit(random_state=1029).split(df_10, groups=df_10['DESCRIPTION_INCIDENT']))\n",
    "df_train, df_test = df_10.iloc[train_index], df_10.iloc[test_index]\n",
    "\n",
    "X_train_a, X_test_a, y_train_a, y_test_a = train_test_split(df_10.Text,df_10.DCO_ID,test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user1/DGS-env/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:386: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['qu', 'quelqu'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5,\n",
    "                            ngram_range=(1, 2),\n",
    "                            stop_words=STOP_WORDS,\n",
    "                            max_features = 10000)\n",
    "\n",
    "vect_tf= tfidf.fit(df_train.Text)\n",
    "\n",
    "X_train = vect_tf.transform(df_train.Text)\n",
    "X_test = vect_tf.transform(df_test.Text)\n",
    "\n",
    "y_train = df_train.DCO_ID\n",
    "y_test = df_test.DCO_ID\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearSVC(class_weight='balanced')\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t Métriques de CLASSIFICATIION \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      2288.0       0.00      0.00      0.00         2\n",
      "      2291.0       1.00      1.00      1.00         6\n",
      "      2293.0       1.00      0.60      0.75         5\n",
      "      2294.0       1.00      1.00      1.00         2\n",
      "      2296.0       0.50      1.00      0.67         1\n",
      "      2297.0       1.00      0.40      0.57         5\n",
      "      2298.0       1.00      0.80      0.89         5\n",
      "      2300.0       0.79      1.00      0.88        11\n",
      "      2306.0       1.00      0.71      0.83         7\n",
      "      2309.0       0.33      0.75      0.46         4\n",
      "      2310.0       0.71      1.00      0.83         5\n",
      "      2312.0       0.50      0.50      0.50         2\n",
      "      2315.0       0.65      0.79      0.71        14\n",
      "      2316.0       0.80      0.67      0.73         6\n",
      "      2319.0       0.00      0.00      0.00         3\n",
      "      2320.0       1.00      0.80      0.89         5\n",
      "      2321.0       0.00      0.00      0.00         2\n",
      "      2322.0       0.59      0.81      0.68        16\n",
      "      2324.0       0.00      0.00      0.00         4\n",
      "      2326.0       0.00      0.00      0.00         3\n",
      "      2330.0       0.00      0.00      0.00         1\n",
      "      2331.0       0.20      0.33      0.25         3\n",
      "      2333.0       0.74      0.76      0.75        37\n",
      "      2334.0       0.92      0.95      0.94        63\n",
      "      2337.0       0.80      0.40      0.53        10\n",
      "      2338.0       0.33      1.00      0.50         2\n",
      "      2340.0       0.40      0.50      0.44         4\n",
      "      2341.0       0.33      0.33      0.33         3\n",
      "      2342.0       0.53      0.38      0.44        21\n",
      "      2344.0       0.75      0.75      0.75         8\n",
      "      2346.0       0.67      0.56      0.61        39\n",
      "      2347.0       0.57      0.80      0.67         5\n",
      "      2348.0       0.00      0.00      0.00         5\n",
      "      2353.0       0.87      0.76      0.81        17\n",
      "      2354.0       0.81      0.89      0.85        44\n",
      "      2355.0       0.71      0.71      0.71         7\n",
      "      2356.0       0.63      0.84      0.72        31\n",
      "      2357.0       0.71      0.88      0.79        17\n",
      "      2360.0       0.61      0.85      0.71        13\n",
      "      2361.0       0.67      0.80      0.73         5\n",
      "      2367.0       0.75      0.75      0.75         4\n",
      "      2368.0       0.00      0.00      0.00         2\n",
      "      2370.0       0.93      0.78      0.85        36\n",
      "      2371.0       1.00      0.67      0.80         6\n",
      "      2372.0       0.17      1.00      0.29         2\n",
      "      2374.0       0.84      0.91      0.87        23\n",
      "      2375.0       1.00      0.33      0.50         6\n",
      "      2379.0       0.40      0.67      0.50         3\n",
      "      2382.0       1.00      0.80      0.89         5\n",
      "      2384.0       1.00      1.00      1.00         2\n",
      "      2389.0       0.00      0.00      0.00         2\n",
      "      2394.0       1.00      1.00      1.00         3\n",
      "      2395.0       0.60      0.60      0.60         5\n",
      "      2397.0       1.00      0.38      0.55         8\n",
      "      2402.0       0.77      0.83      0.80        12\n",
      "      2403.0       0.75      0.75      0.75         4\n",
      "      2404.0       1.00      0.25      0.40         4\n",
      "      2410.0       0.40      0.40      0.40         5\n",
      "      2411.0       0.00      0.00      0.00         3\n",
      "      2419.0       0.15      0.50      0.24         4\n",
      "      2425.0       0.43      0.91      0.59        35\n",
      "      2431.0       0.50      0.40      0.44         5\n",
      "      2433.0       0.71      0.56      0.63         9\n",
      "      2434.0       0.82      0.75      0.78        12\n",
      "      2435.0       0.00      0.00      0.00         0\n",
      "      2437.0       0.82      0.94      0.87        33\n",
      "      2438.0       0.00      0.00      0.00         4\n",
      "      2439.0       0.79      0.92      0.85        12\n",
      "      2443.0       0.43      0.50      0.46         6\n",
      "      2457.0       0.29      0.83      0.43         6\n",
      "      2458.0       1.00      1.00      1.00         1\n",
      "      2464.0       1.00      0.50      0.67         2\n",
      "      2466.0       0.92      1.00      0.96        23\n",
      "      2467.0       0.59      0.95      0.73        37\n",
      "      2469.0       0.67      0.20      0.31        10\n",
      "      2471.0       0.63      0.43      0.51        28\n",
      "      2475.0       1.00      1.00      1.00         4\n",
      "      2483.0       0.00      0.00      0.00         6\n",
      "      2484.0       0.12      0.25      0.17         4\n",
      "      2489.0       0.50      0.07      0.12        15\n",
      "      2491.0       0.60      0.90      0.72        29\n",
      "      2493.0       0.00      0.00      0.00         0\n",
      "      2494.0       0.50      0.17      0.25         6\n",
      "      2497.0       0.67      0.44      0.53         9\n",
      "      2499.0       1.00      0.10      0.18        10\n",
      "      2500.0       0.50      0.60      0.55         5\n",
      "      2501.0       0.67      1.00      0.80         2\n",
      "      2505.0       0.00      0.00      0.00         3\n",
      "      2506.0       1.00      1.00      1.00         3\n",
      "      2507.0       0.00      0.00      0.00         0\n",
      "      2508.0       0.85      1.00      0.92        11\n",
      "      2509.0       0.57      0.76      0.65        17\n",
      "      2513.0       0.50      0.40      0.44         5\n",
      "      2517.0       0.00      0.00      0.00         1\n",
      "      2518.0       0.20      0.33      0.25         3\n",
      "      2521.0       0.22      1.00      0.36         2\n",
      "      2523.0       0.00      0.00      0.00         0\n",
      "      2524.0       0.47      0.29      0.36        24\n",
      "      2531.0       0.62      0.62      0.62        29\n",
      "      2533.0       0.52      0.93      0.67        14\n",
      "      2534.0       1.00      0.50      0.67         2\n",
      "      2537.0       0.72      0.93      0.81        14\n",
      "      2539.0       1.00      0.33      0.50         9\n",
      "      2540.0       0.50      0.50      0.50         2\n",
      "      2541.0       0.25      0.33      0.29         6\n",
      "      2542.0       0.94      0.86      0.90       486\n",
      "      2543.0       0.48      0.68      0.57        19\n",
      "      2545.0       0.30      0.45      0.36        22\n",
      "      2546.0       0.75      0.75      0.75         4\n",
      "      2547.0       0.91      0.87      0.89       202\n",
      "      2549.0       0.81      0.73      0.77       176\n",
      "      2550.0       0.36      0.33      0.35        48\n",
      "      2554.0       0.70      0.60      0.65        50\n",
      "      2555.0       0.56      1.00      0.71         5\n",
      "      2556.0       0.25      0.20      0.22         5\n",
      "      2558.0       0.57      0.76      0.65        82\n",
      "      2559.0       0.00      0.00      0.00         3\n",
      "      2561.0       1.00      1.00      1.00        32\n",
      "      2566.0       0.67      0.43      0.53        23\n",
      "      2571.0       1.00      1.00      1.00         3\n",
      "      2572.0       0.50      0.14      0.22         7\n",
      "      2573.0       0.83      1.00      0.91         5\n",
      "      2575.0       0.60      0.75      0.67         4\n",
      "      2577.0       0.95      0.99      0.97      3499\n",
      "      2578.0       0.27      0.53      0.36        17\n",
      "      2579.0       1.00      1.00      1.00         4\n",
      "      2589.0       0.86      0.86      0.86        44\n",
      "      2592.0       0.40      0.44      0.42         9\n",
      "      2593.0       0.00      0.00      0.00         4\n",
      "      2600.0       0.75      0.33      0.46         9\n",
      "      2603.0       0.50      0.25      0.33         4\n",
      "      2604.0       0.63      0.47      0.54        72\n",
      "      2605.0       1.00      0.50      0.67         2\n",
      "      2608.0       0.83      0.75      0.79        20\n",
      "      2612.0       1.00      1.00      1.00         6\n",
      "      2613.0       0.00      0.00      0.00         4\n",
      "      2615.0       0.00      0.00      0.00         0\n",
      "      2617.0       0.77      0.84      0.80        51\n",
      "      2620.0       0.33      1.00      0.50         6\n",
      "      2621.0       0.68      0.90      0.77        30\n",
      "      2626.0       0.00      0.00      0.00         1\n",
      "      2627.0       0.31      0.24      0.27        17\n",
      "      2629.0       0.67      0.94      0.78        17\n",
      "      2631.0       0.56      0.56      0.56         9\n",
      "      2640.0       0.78      0.61      0.68        23\n",
      "      2641.0       0.50      0.09      0.15        11\n",
      "      2645.0       0.42      0.57      0.48        23\n",
      "      2647.0       0.80      0.33      0.47        12\n",
      "      2649.0       0.52      0.84      0.64        31\n",
      "      2650.0       0.50      0.35      0.41        20\n",
      "      2651.0       1.00      0.33      0.50         3\n",
      "      2652.0       0.40      0.18      0.25        22\n",
      "      2653.0       0.60      0.49      0.54        51\n",
      "      2654.0       0.90      0.62      0.73        29\n",
      "      2655.0       0.00      0.00      0.00         1\n",
      "      2656.0       0.56      0.74      0.63        34\n",
      "      2657.0       0.00      0.00      0.00        12\n",
      "      2658.0       0.50      0.14      0.22         7\n",
      "      2659.0       0.73      0.55      0.63        20\n",
      "      2661.0       0.00      0.00      0.00         3\n",
      "      2662.0       0.00      0.00      0.00         0\n",
      "      2663.0       1.00      0.25      0.40         8\n",
      "      2664.0       0.71      0.77      0.74       117\n",
      "      2667.0       0.00      0.00      0.00         0\n",
      "      2668.0       0.60      0.71      0.65        34\n",
      "      2669.0       0.00      0.00      0.00         6\n",
      "      2670.0       0.88      0.85      0.87        96\n",
      "      2671.0       0.72      0.90      0.80        29\n",
      "      2672.0       0.83      0.62      0.71         8\n",
      "      2673.0       0.50      1.00      0.67         6\n",
      "      2683.0       1.00      0.92      0.96        13\n",
      "      2685.0       0.75      0.80      0.77        15\n",
      "      2686.0       0.89      0.92      0.90        60\n",
      "      2687.0       0.00      0.00      0.00         5\n",
      "      2688.0       0.00      0.00      0.00         3\n",
      "      2690.0       0.00      0.00      0.00         3\n",
      "      2692.0       0.50      0.67      0.57         3\n",
      "      2694.0       0.50      0.57      0.53         7\n",
      "      2695.0       0.94      0.94      0.94        36\n",
      "      2696.0       0.92      0.94      0.93       231\n",
      "      2697.0       0.67      0.37      0.47        60\n",
      "      2698.0       0.00      0.00      0.00        11\n",
      "      2699.0       0.57      1.00      0.73         4\n",
      "      2700.0       1.00      0.92      0.96        12\n",
      "      2701.0       0.25      0.33      0.29         3\n",
      "      2702.0       0.78      0.88      0.82         8\n",
      "      2703.0       0.56      0.90      0.69        10\n",
      "      2705.0       0.82      0.88      0.85        26\n",
      "      2708.0       0.38      0.38      0.38         8\n",
      "      2709.0       0.65      0.65      0.65        34\n",
      "      2711.0       0.84      0.76      0.80        34\n",
      "      2712.0       0.20      1.00      0.33         1\n",
      "      2714.0       0.50      1.00      0.67         1\n",
      "      2716.0       1.00      1.00      1.00         3\n",
      "      2718.0       0.83      0.96      0.89        26\n",
      "      2720.0       0.56      0.83      0.67         6\n",
      "      2721.0       0.50      0.29      0.36         7\n",
      "      2726.0       1.00      0.50      0.67         2\n",
      "      2730.0       1.00      0.33      0.50         3\n",
      "      2733.0       0.33      0.33      0.33         6\n",
      "      2734.0       0.74      0.83      0.78        24\n",
      "      2736.0       0.50      0.67      0.57         3\n",
      "      2737.0       0.67      0.50      0.57         8\n",
      "      2739.0       0.73      0.62      0.67        13\n",
      "      2740.0       1.00      1.00      1.00         3\n",
      "      2741.0       0.60      0.60      0.60        15\n",
      "      2742.0       0.58      0.64      0.61        28\n",
      "      2743.0       0.25      0.10      0.14        10\n",
      "      2745.0       0.85      0.92      0.88        12\n",
      "      2746.0       1.00      0.50      0.67         8\n",
      "      2749.0       0.88      1.00      0.94        15\n",
      "      2750.0       0.25      0.23      0.24        13\n",
      "      2752.0       0.14      0.67      0.23         6\n",
      "      2753.0       0.97      1.00      0.99        33\n",
      "      2754.0       0.00      0.00      0.00         5\n",
      "      2756.0       0.62      0.43      0.51        30\n",
      "      2757.0       0.18      0.38      0.24         8\n",
      "      2760.0       1.00      0.67      0.80         3\n",
      "      2764.0       1.00      1.00      1.00         3\n",
      "      2766.0       0.52      0.70      0.60        20\n",
      "      2767.0       0.67      1.00      0.80         2\n",
      "      2770.0       0.17      0.20      0.18         5\n",
      "      2774.0       0.85      0.93      0.89        61\n",
      "      2775.0       0.57      0.57      0.57        44\n",
      "      2776.0       0.00      0.00      0.00         1\n",
      "      2777.0       0.77      1.00      0.87        10\n",
      "      2778.0       0.84      0.89      0.87        66\n",
      "      2779.0       0.74      0.80      0.77        46\n",
      "      2783.0       0.41      0.88      0.56         8\n",
      "      2784.0       0.60      0.27      0.37        11\n",
      "      2785.0       0.95      0.87      0.91        23\n",
      "      2789.0       0.33      0.50      0.40         2\n",
      "      2793.0       0.00      0.00      0.00         3\n",
      "      2798.0       0.55      0.50      0.52        12\n",
      "      2799.0       0.83      1.00      0.91        34\n",
      "      2801.0       0.00      0.00      0.00         1\n",
      "      2804.0       0.67      0.44      0.53         9\n",
      "      2805.0       0.80      1.00      0.89         8\n",
      "      2806.0       1.00      1.00      1.00         5\n",
      "      2808.0       0.00      0.00      0.00         4\n",
      "      2824.0       1.00      0.50      0.67         4\n",
      "      2827.0       0.00      0.00      0.00         0\n",
      "      2829.0       0.64      0.58      0.61        12\n",
      "      2830.0       0.67      0.78      0.72        18\n",
      "      2832.0       0.56      1.00      0.72        14\n",
      "      2834.0       0.80      1.00      0.89        12\n",
      "      2835.0       0.00      0.00      0.00         2\n",
      "      2836.0       1.00      1.00      1.00         6\n",
      "      2837.0       0.95      0.68      0.79        28\n",
      "      2840.0       1.00      1.00      1.00         8\n",
      "      2841.0       0.96      0.87      0.91       161\n",
      "      2842.0       0.69      0.75      0.72        12\n",
      "      2844.0       0.48      0.75      0.59        16\n",
      "      2847.0       0.50      0.73      0.59        11\n",
      "      2850.0       0.81      0.96      0.88        72\n",
      "      2851.0       0.90      0.71      0.79       111\n",
      "      2856.0       0.46      0.67      0.54        33\n",
      "      2857.0       1.00      0.96      0.98        55\n",
      "      2858.0       0.67      1.00      0.80         4\n",
      "      2864.0       1.00      1.00      1.00         4\n",
      "      2865.0       1.00      0.33      0.50         6\n",
      "      2868.0       0.75      0.75      0.75         8\n",
      "      2869.0       0.78      0.64      0.70        11\n",
      "      2870.0       0.00      0.00      0.00         5\n",
      "      2873.0       0.42      0.19      0.26        27\n",
      "      2874.0       0.40      0.18      0.25        22\n",
      "      2875.0       1.00      0.50      0.67        18\n",
      "      2876.0       0.49      0.82      0.61        22\n",
      "      2877.0       0.90      1.00      0.95        18\n",
      "      2881.0       0.83      0.62      0.71         8\n",
      "      2883.0       0.00      0.00      0.00         0\n",
      "      2886.0       0.00      0.00      0.00         8\n",
      "      2890.0       0.29      0.14      0.19        14\n",
      "      2892.0       1.00      1.00      1.00         1\n",
      "      2898.0       0.67      1.00      0.80         2\n",
      "      2900.0       0.70      0.73      0.71        22\n",
      "      2902.0       0.90      0.86      0.88        21\n",
      "      2906.0       1.00      0.50      0.67         4\n",
      "      2910.0       0.50      0.43      0.46         7\n",
      "      2912.0       0.50      0.31      0.38        13\n",
      "      2913.0       0.38      0.38      0.38         8\n",
      "      2919.0       0.79      0.83      0.81       128\n",
      "      2920.0       0.00      0.00      0.00         2\n",
      "      2926.0       1.00      0.33      0.50         3\n",
      "      2928.0       0.69      0.60      0.64        15\n",
      "      2929.0       0.39      0.40      0.39        35\n",
      "      2931.0       1.00      0.83      0.91         6\n",
      "      2932.0       0.00      0.00      0.00         2\n",
      "      2934.0       0.50      0.50      0.50         4\n",
      "      2937.0       0.57      1.00      0.73         4\n",
      "      2938.0       0.36      0.29      0.32        14\n",
      "      2939.0       0.62      0.83      0.71         6\n",
      "      2940.0       1.00      0.50      0.67         6\n",
      "      2942.0       0.40      0.33      0.36         6\n",
      "      2943.0       0.00      0.00      0.00         0\n",
      "      2950.0       0.00      0.00      0.00         4\n",
      "      2957.0       0.82      1.00      0.90         9\n",
      "      2964.0       0.29      0.83      0.43         6\n",
      "      2965.0       0.61      0.60      0.61       231\n",
      "      2968.0       0.00      0.00      0.00         4\n",
      "      2969.0       0.61      0.55      0.58       135\n",
      "      2972.0       0.00      0.00      0.00         3\n",
      "      2973.0       0.00      0.00      0.00         4\n",
      "      2974.0       0.71      0.72      0.71       123\n",
      "      2975.0       0.74      0.69      0.72       370\n",
      "      2978.0       0.50      0.75      0.60         8\n",
      "      2979.0       0.79      0.70      0.74       169\n",
      "      2982.0       0.88      0.95      0.92        86\n",
      "      2983.0       0.85      0.65      0.73        34\n",
      "      2984.0       0.79      0.76      0.78       220\n",
      "      2985.0       0.48      0.61      0.54        36\n",
      "      2986.0       0.66      0.58      0.62        90\n",
      "      2987.0       0.78      0.97      0.86        68\n",
      "      2988.0       0.00      0.00      0.00         4\n",
      "      2990.0       0.27      0.21      0.24        28\n",
      "      2991.0       0.50      0.33      0.40         6\n",
      "      2993.0       0.00      0.00      0.00         4\n",
      "      2995.0       0.60      0.50      0.55        12\n",
      "      2997.0       0.65      0.37      0.47        76\n",
      "      3001.0       1.00      1.00      1.00         4\n",
      "      3006.0       1.00      0.78      0.88         9\n",
      "      3009.0       0.48      0.50      0.49        24\n",
      "      3010.0       0.86      0.75      0.80        24\n",
      "      3011.0       0.98      0.96      0.97       328\n",
      "      3012.0       0.00      0.00      0.00         8\n",
      "      3013.0       0.29      0.22      0.25         9\n",
      "      3016.0       0.91      0.86      0.89       222\n",
      "      3018.0       0.62      0.83      0.71         6\n",
      "      3021.0       0.81      0.78      0.79       112\n",
      "      3024.0       0.22      0.40      0.29        10\n",
      "      3025.0       0.70      0.82      0.76        34\n",
      "      3026.0       0.52      0.87      0.65        15\n",
      "      3027.0       0.33      0.20      0.25        10\n",
      "      3028.0       0.56      0.53      0.55        34\n",
      "      3029.0       0.92      0.78      0.84       116\n",
      "      3030.0       0.00      0.00      0.00         6\n",
      "      3031.0       0.00      0.00      0.00         2\n",
      "      3032.0       0.35      0.31      0.33        58\n",
      "      3033.0       0.17      0.33      0.22         3\n",
      "      3035.0       0.33      1.00      0.50         1\n",
      "      3036.0       0.27      0.60      0.37         5\n",
      "      3043.0       0.67      1.00      0.80         2\n",
      "      3049.0       0.40      0.50      0.44         4\n",
      "      3050.0       0.65      0.70      0.67        43\n",
      "      3051.0       0.79      0.96      0.86        23\n",
      "      3058.0       0.00      0.00      0.00         0\n",
      "      3060.0       0.75      1.00      0.86         3\n",
      "      3064.0       0.32      0.86      0.46         7\n",
      "      3067.0       0.67      1.00      0.80        12\n",
      "      3068.0       0.40      0.67      0.50         3\n",
      "      3069.0       0.00      0.00      0.00         3\n",
      "      3070.0       0.33      0.67      0.44         3\n",
      "      3071.0       1.00      1.00      1.00         1\n",
      "      3073.0       0.68      0.92      0.78        39\n",
      "      3076.0       0.00      0.00      0.00         3\n",
      "      3077.0       1.00      0.40      0.57         5\n",
      "      3081.0       0.88      1.00      0.93         7\n",
      "      3084.0       0.67      0.67      0.67         6\n",
      "      3088.0       0.77      0.85      0.81        20\n",
      "      3093.0       0.75      0.81      0.78       128\n",
      "      3094.0       0.67      0.67      0.67         3\n",
      "      3095.0       0.83      0.67      0.74        30\n",
      "      3096.0       0.40      0.44      0.42        50\n",
      "      3101.0       0.25      0.38      0.30         8\n",
      "      3102.0       0.00      0.00      0.00         6\n",
      "      3103.0       0.47      0.89      0.62         9\n",
      "      3106.0       0.78      0.64      0.71        28\n",
      "      3114.0       0.00      0.00      0.00         2\n",
      "      3115.0       0.86      1.00      0.92         6\n",
      "      3127.0       0.78      0.54      0.64        13\n",
      "      3131.0       0.61      0.74      0.67        19\n",
      "      3132.0       0.33      0.22      0.27         9\n",
      "      3133.0       0.30      0.50      0.37         6\n",
      "      3145.0       1.00      1.00      1.00         3\n",
      "      3159.0       0.83      0.50      0.62        10\n",
      "      3160.0       0.41      0.47      0.44        15\n",
      "      3161.0       0.00      0.00      0.00         4\n",
      "      3162.0       0.44      0.33      0.38        12\n",
      "      3166.0       0.88      0.64      0.74        11\n",
      "      3167.0       0.00      0.00      0.00         1\n",
      "      3170.0       0.33      0.50      0.40         2\n",
      "      3173.0       0.60      0.75      0.67         4\n",
      "      3180.0       0.62      1.00      0.76         8\n",
      "      3194.0       0.00      0.00      0.00         6\n",
      "      3196.0       0.42      0.42      0.42        19\n",
      "      3197.0       0.14      0.40      0.21         5\n",
      "      3199.0       0.20      0.50      0.29         4\n",
      "      3200.0       0.89      0.73      0.80        11\n",
      "      3201.0       0.50      0.40      0.44         5\n",
      "      3203.0       0.60      0.90      0.72        10\n",
      "      3205.0       0.00      0.00      0.00         1\n",
      "      3212.0       0.44      0.67      0.53         6\n",
      "      3213.0       0.00      0.00      0.00         2\n",
      "      3218.0       0.33      1.00      0.50         1\n",
      "      3219.0       0.83      0.71      0.77         7\n",
      "      3224.0       1.00      0.42      0.59        12\n",
      "      3225.0       0.75      1.00      0.86         3\n",
      "      3227.0       1.00      0.33      0.50         3\n",
      "      3228.0       1.00      0.75      0.86        12\n",
      "      3232.0       1.00      0.50      0.67         2\n",
      "      3234.0       0.93      0.93      0.93        15\n",
      "      3236.0       0.67      0.72      0.69        25\n",
      "      3242.0       0.33      0.67      0.44         3\n",
      "      3245.0       0.74      0.91      0.82        22\n",
      "      3246.0       1.00      0.50      0.67         2\n",
      "      3248.0       0.67      0.22      0.33         9\n",
      "      3252.0       0.71      0.92      0.80        24\n",
      "      3253.0       0.67      1.00      0.80         2\n",
      "      3255.0       0.00      0.00      0.00         0\n",
      "      3256.0       0.50      0.33      0.40         3\n",
      "      3262.0       0.77      0.99      0.86       201\n",
      "      3263.0       0.43      0.75      0.55         4\n",
      "      3264.0       0.00      0.00      0.00         2\n",
      "      3268.0       0.56      0.56      0.56         9\n",
      "      3271.0       0.00      0.00      0.00        10\n",
      "      3273.0       0.50      1.00      0.67         2\n",
      "      3274.0       0.99      0.93      0.96       108\n",
      "      3278.0       1.00      1.00      1.00         2\n",
      "      3282.0       0.33      0.50      0.40         2\n",
      "      3283.0       0.29      0.57      0.38        14\n",
      "      3285.0       0.00      0.00      0.00         1\n",
      "      3286.0       0.13      0.14      0.14        14\n",
      "      3287.0       0.29      0.33      0.31        15\n",
      "      3290.0       0.00      0.00      0.00         0\n",
      "      3296.0       0.89      0.68      0.77        25\n",
      "      3298.0       0.75      1.00      0.86         3\n",
      "      3299.0       0.93      0.93      0.93        14\n",
      "      3300.0       0.25      0.40      0.31         5\n",
      "      3301.0       0.96      0.92      0.94        78\n",
      "      3302.0       0.50      0.60      0.55         5\n",
      "      3304.0       0.67      0.40      0.50         5\n",
      "      3305.0       0.81      0.88      0.84        24\n",
      "      3308.0       0.78      0.88      0.82         8\n",
      "      3310.0       0.95      0.78      0.86        74\n",
      "      3311.0       0.74      0.59      0.65        29\n",
      "      3317.0       0.50      0.67      0.57         6\n",
      "      3319.0       0.89      0.89      0.89         9\n",
      "      3320.0       0.00      0.00      0.00         2\n",
      "      3323.0       0.80      0.67      0.73         6\n",
      "      3326.0       0.25      0.50      0.33         2\n",
      "      3328.0       0.90      0.97      0.94       127\n",
      "      3330.0       0.79      0.79      0.79        14\n",
      "      3332.0       0.73      0.65      0.69        17\n",
      "      3333.0       0.00      0.00      0.00         8\n",
      "      3335.0       0.84      0.82      0.83       116\n",
      "      3336.0       0.71      0.67      0.69        15\n",
      "      3337.0       0.17      0.17      0.17         6\n",
      "      3338.0       0.85      0.85      0.85        13\n",
      "      3343.0       0.48      0.45      0.47        22\n",
      "      3344.0       0.81      0.91      0.86        23\n",
      "      3345.0       0.80      0.50      0.62         8\n",
      "      3346.0       0.73      0.89      0.80        18\n",
      "      3356.0       0.57      1.00      0.73         4\n",
      "      3357.0       0.92      0.67      0.77        18\n",
      "      3358.0       1.00      0.14      0.25         7\n",
      "      3361.0       0.00      0.00      0.00         2\n",
      "      3368.0       0.57      0.67      0.62        12\n",
      "      3378.0       0.75      0.43      0.55         7\n",
      "      3382.0       1.00      0.71      0.83         7\n",
      "      3398.0       0.36      0.40      0.38        10\n",
      "      3453.0       0.78      0.64      0.70        11\n",
      "      3463.0       0.25      0.50      0.33         2\n",
      "      3464.0       0.80      0.67      0.73         6\n",
      "      3475.0       1.00      0.75      0.86         4\n",
      "      3477.0       0.00      0.00      0.00         3\n",
      "      3479.0       0.00      0.00      0.00         6\n",
      "      3481.0       0.00      0.00      0.00         3\n",
      "      3491.0       0.50      0.62      0.56         8\n",
      "      3492.0       0.33      0.25      0.29         4\n",
      "      3499.0       0.33      0.33      0.33         3\n",
      "      3509.0       0.90      0.90      0.90        10\n",
      "      3528.0       0.50      1.00      0.67         1\n",
      "      3531.0       0.25      1.00      0.40         1\n",
      "      3536.0       0.82      0.82      0.82        11\n",
      "      3537.0       0.67      0.50      0.57         4\n",
      "      3538.0       0.81      1.00      0.90        13\n",
      "      3635.0       0.83      0.62      0.71         8\n",
      "      3746.0       0.00      0.00      0.00         0\n",
      "      3785.0       1.00      1.00      1.00         4\n",
      "      3806.0       1.00      1.00      1.00         1\n",
      "      3819.0       1.00      1.00      1.00         4\n",
      "      3843.0       0.67      1.00      0.80         2\n",
      "      3846.0       1.00      0.29      0.44         7\n",
      "      3855.0       0.75      1.00      0.86         9\n",
      "      3857.0       0.67      1.00      0.80        10\n",
      "      3886.0       0.33      0.50      0.40         2\n",
      "      3887.0       0.81      0.87      0.84        15\n",
      "      3893.0       0.00      0.00      0.00         1\n",
      "      3894.0       0.00      0.00      0.00         6\n",
      "      3902.0       0.11      0.25      0.15         4\n",
      "      3904.0       0.17      0.18      0.17        11\n",
      "      3941.0       1.00      0.33      0.50         6\n",
      "      3944.0       0.00      0.00      0.00         1\n",
      "      3961.0       0.00      0.00      0.00        62\n",
      "      4001.0       0.00      0.00      0.00       360\n",
      "      4021.0       0.89      0.73      0.80        11\n",
      "      4068.0       1.00      0.18      0.31        11\n",
      "      4069.0       1.00      0.25      0.40         4\n",
      "      4070.0       0.00      0.00      0.00         4\n",
      "      4101.0       0.00      0.00      0.00         2\n",
      "      4121.0       0.05      1.00      0.09         2\n",
      "      4123.0       0.83      1.00      0.91         5\n",
      "      4221.0       0.33      0.15      0.21        13\n",
      "      4222.0       0.50      0.33      0.40        12\n",
      "      4223.0       0.09      0.69      0.16        13\n",
      "      4281.0       0.29      0.71      0.41        14\n",
      "      4301.0       0.39      0.81      0.52        21\n",
      "      4302.0       0.95      0.90      0.92       204\n",
      "      4381.0       0.27      0.75      0.40         8\n",
      "      4441.0       1.00      0.46      0.63        13\n",
      "      4461.0       0.88      0.83      0.86        18\n",
      "      4481.0       0.50      0.13      0.21        15\n",
      "      4482.0       1.00      0.46      0.63        13\n",
      "      4521.0       0.83      1.00      0.91         5\n",
      "      4601.0       0.67      0.33      0.44         6\n",
      "      4642.0       0.00      0.00      0.00         4\n",
      "      4841.0       0.64      0.54      0.58        13\n",
      "      4862.0       0.50      0.38      0.43         8\n",
      "      4864.0       0.00      0.00      0.00         2\n",
      "      4865.0       1.00      0.40      0.57         5\n",
      "      4926.0       0.75      0.83      0.79        18\n",
      "\n",
      "    accuracy                           0.77     14697\n",
      "   macro avg       0.56      0.56      0.53     14697\n",
      "weighted avg       0.77      0.77      0.76     14697\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user1/DGS-env/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/user1/DGS-env/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print('\\t\\t Métriques de CLASSIFICATIION \\n')\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "précison: 0.7675035721575831\n",
      "Balanced_accuracy :  0.5774459012789024\n"
     ]
    }
   ],
   "source": [
    "print(\"précison:\", accuracy_score(y_test,y_pred) )\n",
    "print(\"Balanced_accuracy : \", balanced_accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user1/DGS-env/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:386: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['qu', 'quelqu'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "précison: 0.8860996627950544\n",
      "Balanced_accuracy :  0.7440963277648411\n",
      "CPU times: user 1min 45s, sys: 440 ms, total: 1min 45s\n",
      "Wall time: 1min 45s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user1/DGS-env/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1814: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5,\n",
    "                            ngram_range=(1, 2),\n",
    "                            stop_words=STOP_WORDS,\n",
    "                            max_features = 10000)\n",
    "\n",
    "vect_tf= tfidf.fit(X_train_a.values)\n",
    "\n",
    "X_train_a = vect_tf.transform(X_train_a.values)\n",
    "X_test_a = vect_tf.transform(X_test_a.values)\n",
    "\n",
    "\n",
    "model = LinearSVC(class_weight='balanced')\n",
    "model.fit(X_train_a, y_train_a)\n",
    "y_pred_a = model.predict(X_test_a)\n",
    "\n",
    "print(\"précison:\", accuracy_score(y_test_a,y_pred_a) )\n",
    "print(\"Balanced_accuracy : \", balanced_accuracy_score(y_test_a,y_pred_a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commentaire :\n",
    "Le choix du train_test est important pour évaluer correctement notre modèle\n",
    "Lors de nos études, nous avions procédés par cross validation (5 folds) ce qui avait éffacé l'effet que l'on observe si dessus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Si on  utilisait seulement ~33 000 lignes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_10_unique = df_10.drop_duplicates('DESCRIPTION_INCIDENT') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user1/DGS-env/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:386: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['qu', 'quelqu'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "précison: 0.7563855730971634\n",
      "Balanced_accuracy :  0.600639466198176\n",
      "CPU times: user 21.2 s, sys: 116 ms, total: 21.3 s\n",
      "Wall time: 21.3 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user1/DGS-env/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1814: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train_u, X_test_u, y_train_u, y_test_u = train_test_split(df_10_unique.Text,df_10_unique.DCO_ID,test_size=0.25)\n",
    "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5,\n",
    "                            ngram_range=(1, 2),\n",
    "                            stop_words=STOP_WORDS,\n",
    "                            max_features = 10000)\n",
    "\n",
    "vect_tf= tfidf.fit(X_train_u.values)\n",
    "\n",
    "X_train_u = vect_tf.transform(X_train_u.values)\n",
    "X_test_u = vect_tf.transform(X_test_u.values)\n",
    "\n",
    "\n",
    "model = LinearSVC(class_weight='balanced')\n",
    "model.fit(X_train_u, y_train_u)\n",
    "y_pred_u = model.predict(X_test_u)\n",
    "\n",
    "print(\"précison:\", accuracy_score(y_test_u,y_pred_u) )\n",
    "print(\"Balanced_accuracy : \", balanced_accuracy_score(y_test_u,y_pred_u))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COmmentaire : \n",
    "Nous observons une baisse sensible de la performance de notre modèle. Toutefois,..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Créer deux tfidf différents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = ColumnTransformer(\n",
    "    [('libelle_tfidf', TfidfVectorizer(sublinear_tf=True, min_df=3,ngram_range=(1, 3),\n",
    "                                       stop_words=STOP_WORDS,\n",
    "                                       max_features = 10000), 'LIBELLE_COMMERCIAL'),\n",
    "     ('description_tfidf',TfidfVectorizer(sublinear_tf=True, min_df=5,\n",
    "                            ngram_range=(1, 1),\n",
    "                            stop_words=STOP_WORDS,\n",
    "                            max_features = 10000), 'DESCRIPTION_INCIDENT')],\n",
    "    remainder='passthrough')\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', preprocess),\n",
    "    ('clf', LinearSVC(class_weight='balanced')),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train[['DESCRIPTION_INCIDENT','LIBELLE_COMMERCIAL']]\n",
    "X_test  = df_test[['DESCRIPTION_INCIDENT','LIBELLE_COMMERCIAL']]\n",
    "y_train = df_train.DCO_ID\n",
    "y_test = df_test.DCO_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user1/DGS-env/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:386: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['qu', 'quelqu'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/home/user1/DGS-env/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:386: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['qu', 'quelqu'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "précison: 0.8313941620738926\n",
      "Balanced_accuracy :  0.682057446796152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user1/DGS-env/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1814: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n"
     ]
    }
   ],
   "source": [
    "pipeline.fit(X_train,y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "print(\"précison:\", accuracy_score(y_test,y_pred) )\n",
    "print(\"Balanced_accuracy : \", balanced_accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commentaire : \n",
    "Faire une transformation pour chaque variable améliore sensiblement le résultat. Par exemple, le résultat s'améliore d'environ 4%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Une moyenne d'un embedding ? \n",
    "Fait dans n autre notebook, comme attendu moyener les embeddings des mots n'apporte pas de bon résultats\n",
    "0.20 appliqué à df.Text\n",
    "     appliqué à df.LIBELLE_COMMERCIALE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 ) Un modèle avec des scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user1/DGS-env/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:386: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['qu', 'quelqu'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/home/user1/DGS-env/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:386: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['qu', 'quelqu'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "précison: 0.8331945600888149\n",
      "Balanced_accuracy :  0.6892819196582894\n",
      "CPU times: user 1min 50s, sys: 636 ms, total: 1min 51s\n",
      "Wall time: 1min 51s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user1/DGS-env/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1814: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', preprocess),\n",
    "    ('clf',CalibratedClassifierCV(LinearSVC(class_weight='balanced'),cv=3, method='isotonic'))\n",
    "])\n",
    "\n",
    "\n",
    "df_10 = select_raw_by_nb_obs(df,20)\n",
    "train_index,test_index = next(GroupShuffleSplit(random_state=1029).split(df_10, groups=df_10['DESCRIPTION_INCIDENT']))\n",
    "df_train, df_test = df_10.iloc[train_index], df_10.iloc[test_index]\n",
    "\n",
    "X_train = df_train[['DESCRIPTION_INCIDENT','LIBELLE_COMMERCIAL']]\n",
    "X_test  = df_test[['DESCRIPTION_INCIDENT','LIBELLE_COMMERCIAL']]\n",
    "y_train = df_train.DCO_ID\n",
    "y_test = df_test.DCO_ID\n",
    "\n",
    "pipeline.fit(X_train,y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "print(\"précison:\", accuracy_score(y_test,y_pred) )\n",
    "print(\"Balanced_accuracy : \", balanced_accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00000000e+00, 0.00000000e+00, 2.52375699e-05, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 2.33958916e-05, 6.53236519e-05,\n",
       "       0.00000000e+00, 0.00000000e+00, 6.74504386e-05, 6.15025935e-04,\n",
       "       0.00000000e+00, 0.00000000e+00, 2.35467604e-05, 3.00416184e-05,\n",
       "       0.00000000e+00, 1.87375497e-04, 0.00000000e+00, 7.91395822e-05,\n",
       "       2.85927372e-04, 0.00000000e+00, 1.47905782e-05, 6.92042091e-04,\n",
       "       4.37566211e-05, 0.00000000e+00, 1.68017359e-04, 0.00000000e+00,\n",
       "       4.57381385e-05, 3.49301755e-04, 4.70061856e-05, 0.00000000e+00,\n",
       "       3.53314427e-04, 0.00000000e+00, 1.22685049e-03, 0.00000000e+00,\n",
       "       1.90707073e-04, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       5.71980269e-05, 7.31995772e-05, 4.34283358e-05, 0.00000000e+00,\n",
       "       1.45459719e-05, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       1.83088456e-05, 6.12604196e-05, 2.89650911e-05, 1.50996898e-05,\n",
       "       4.91077672e-05, 2.65398138e-04, 6.42498384e-05, 3.01007119e-05,\n",
       "       0.00000000e+00, 0.00000000e+00, 1.45708937e-05, 0.00000000e+00,\n",
       "       3.67324647e-04, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       2.97500202e-05, 0.00000000e+00, 1.19609153e-04, 2.14728160e-04,\n",
       "       0.00000000e+00, 8.01234533e-04, 1.20452524e-04, 4.45032491e-02,\n",
       "       8.87670136e-05, 5.32354639e-04, 6.54615695e-05, 0.00000000e+00,\n",
       "       1.76690234e-04, 0.00000000e+00, 0.00000000e+00, 7.37300754e-05,\n",
       "       0.00000000e+00, 2.40054998e-05, 7.94634352e-04, 1.10935303e-03,\n",
       "       0.00000000e+00, 1.36915823e-05, 6.81270570e-05, 1.20072682e-03,\n",
       "       9.07388998e-05, 3.73458914e-03, 0.00000000e+00, 0.00000000e+00,\n",
       "       6.62604382e-05, 8.29319422e-05, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 9.65309412e-05, 0.00000000e+00, 1.86241500e-05,\n",
       "       6.35191504e-05, 1.42949356e-04, 0.00000000e+00, 4.12088190e-05,\n",
       "       0.00000000e+00, 3.88900917e-04, 1.31068447e-04, 1.54061874e-05,\n",
       "       7.00986688e-05, 5.94402745e-05, 0.00000000e+00, 1.58479573e-05,\n",
       "       1.48283354e-05, 2.71140531e-05, 1.23443605e-02, 4.45674342e-03,\n",
       "       5.83788086e-05, 0.00000000e+00, 5.75304565e-04, 3.50810465e-04,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 2.71425822e-05,\n",
       "       0.00000000e+00, 1.37170638e-05, 4.36128875e-05, 2.10368657e-03,\n",
       "       1.22901330e-04, 0.00000000e+00, 6.53298090e-05, 0.00000000e+00,\n",
       "       0.00000000e+00, 1.98650548e-04, 5.94873458e-05, 1.94833364e-04,\n",
       "       0.00000000e+00, 8.37288858e-04, 0.00000000e+00, 1.29982110e-04,\n",
       "       8.59805354e-05, 0.00000000e+00, 7.31395240e-05, 1.36582359e-05,\n",
       "       1.86898567e-04, 1.48631495e-05, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 2.49180451e-04, 0.00000000e+00,\n",
       "       2.90498988e-05, 7.38031558e-05, 1.43624019e-05, 0.00000000e+00,\n",
       "       0.00000000e+00, 4.47835496e-05, 0.00000000e+00, 1.34537969e-05,\n",
       "       0.00000000e+00, 0.00000000e+00, 1.52829313e-05, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 5.30202235e-04, 2.95044380e-05,\n",
       "       0.00000000e+00, 1.20733537e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "       8.02419700e-04, 1.52134969e-05, 8.21409604e-01, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       5.00778443e-05, 2.73924677e-04, 1.24522975e-04, 3.00481937e-05,\n",
       "       0.00000000e+00, 4.05619809e-03, 3.29089274e-05, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.07484626e-03,\n",
       "       0.00000000e+00, 1.58985672e-04, 1.27313785e-03, 0.00000000e+00,\n",
       "       4.31417869e-05, 0.00000000e+00, 3.16783718e-05, 9.84967932e-05,\n",
       "       0.00000000e+00, 0.00000000e+00, 2.88511165e-05, 3.67895123e-04,\n",
       "       2.25507271e-05, 8.53207718e-05, 1.63478476e-04, 4.85853596e-05,\n",
       "       2.81477881e-04, 0.00000000e+00, 2.24490491e-04, 1.54396156e-05,\n",
       "       2.08367977e-05, 1.34608172e-05, 1.19127058e-04, 2.69906462e-05,\n",
       "       2.22768608e-05, 2.20850361e-03, 1.09961462e-04, 2.20866079e-02,\n",
       "       6.09303299e-05, 7.78508477e-04, 1.17142278e-02, 7.20567322e-04,\n",
       "       3.90096970e-04, 0.00000000e+00, 1.28481528e-04, 4.97247329e-04,\n",
       "       8.29944053e-05, 1.29863619e-04, 1.57550539e-05, 0.00000000e+00,\n",
       "       1.18026666e-04, 6.89300268e-05, 1.87453723e-04, 0.00000000e+00,\n",
       "       0.00000000e+00, 1.57297216e-05, 3.07336750e-05, 0.00000000e+00,\n",
       "       9.87662734e-05, 0.00000000e+00, 0.00000000e+00, 1.75735341e-04,\n",
       "       0.00000000e+00, 1.37516512e-04, 5.99064202e-05, 0.00000000e+00,\n",
       "       0.00000000e+00, 3.19808217e-05, 1.25712731e-04, 1.85568845e-04,\n",
       "       5.41001628e-05, 2.96579690e-04, 8.29807205e-05, 1.36814161e-05,\n",
       "       0.00000000e+00, 2.90636403e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 1.35101656e-05, 0.00000000e+00, 2.52114055e-05,\n",
       "       0.00000000e+00, 1.80031191e-04, 0.00000000e+00, 1.92389581e-03,\n",
       "       0.00000000e+00, 3.68014023e-05, 0.00000000e+00, 0.00000000e+00,\n",
       "       1.42246824e-04, 0.00000000e+00, 3.55191071e-04, 1.40372804e-05,\n",
       "       0.00000000e+00, 1.47797961e-05, 3.86280396e-05, 0.00000000e+00,\n",
       "       1.35406594e-05, 0.00000000e+00, 2.59412503e-04, 2.17996239e-04,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 2.68180789e-05, 0.00000000e+00, 7.14832701e-05,\n",
       "       4.81361937e-05, 0.00000000e+00, 7.18870982e-05, 0.00000000e+00,\n",
       "       0.00000000e+00, 7.28064545e-05, 0.00000000e+00, 0.00000000e+00,\n",
       "       1.53049288e-04, 1.94498642e-05, 0.00000000e+00, 8.14405535e-05,\n",
       "       1.01171641e-04, 2.25551370e-05, 5.03533679e-04, 7.32269712e-03,\n",
       "       3.39872334e-04, 9.24038375e-05, 0.00000000e+00, 4.82883945e-05,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.87337038e-05,\n",
       "       1.54967775e-05, 0.00000000e+00, 2.29090759e-04, 0.00000000e+00,\n",
       "       2.76056624e-04, 3.14891731e-05, 2.30564693e-03, 0.00000000e+00,\n",
       "       0.00000000e+00, 9.80747764e-04, 2.71032884e-04, 2.34235087e-05,\n",
       "       0.00000000e+00, 7.51471826e-05, 0.00000000e+00, 0.00000000e+00,\n",
       "       1.02018730e-04, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       6.29710870e-04, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 9.05426998e-05, 5.46143866e-04, 1.43837522e-04,\n",
       "       1.41291227e-03, 9.78053407e-04, 1.88106804e-02, 0.00000000e+00,\n",
       "       2.18324241e-05, 4.53282554e-05, 0.00000000e+00, 3.93571825e-05,\n",
       "       3.20369056e-05, 1.04692817e-04, 4.24306319e-04, 3.89351261e-05,\n",
       "       5.88421747e-04, 2.88468662e-05, 0.00000000e+00, 0.00000000e+00,\n",
       "       3.97278379e-03, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       2.75498586e-05, 0.00000000e+00])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.predict_proba(X_test)[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9) UTilisation de XGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost.sklearn import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_xgb = Pipeline([\n",
    "    ('vect', preprocess),\n",
    "    ('clf', XGBClassifier())\n",
    "]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user1/DGS-env/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:386: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['qu', 'quelqu'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/home/user1/DGS-env/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:386: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['qu', 'quelqu'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "précison: 0.7422980849292257\n",
      "Balanced_accuracy :  0.5458820929051504\n",
      "CPU times: user 19h 59min 5s, sys: 5min 20s, total: 20h 4min 26s\n",
      "Wall time: 1h 47min 17s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user1/DGS-env/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1814: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pipeline_xgb.fit(X_train,y_train)\n",
    "y_pred = pipeline_xgb.predict(X_test)\n",
    "\n",
    "print(\"précison:\", accuracy_score(y_test,y_pred) )\n",
    "print(\"Balanced_accuracy : \", balanced_accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autres tests..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user1/DGS-env/lib/python3.7/site-packages/sklearn/svm/_base.py:975: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "précison: 0.14580926710306763\n",
      "Balanced_accuracy :  0.1034307203391295\n",
      "CPU times: user 32min 55s, sys: 3.29 s, total: 32min 58s\n",
      "Wall time: 32min 59s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user1/DGS-env/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1814: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import spacy \n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class SpacyVectorTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, nlp):\n",
    "        self.nlp = nlp\n",
    "        self.dim = 300\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Doc.vector defaults to an average of the token vectors.\n",
    "        # https://spacy.io/api/doc#vector\n",
    "        return [self.nlp(text).vector for text in X]\n",
    "\n",
    "\n",
    "\n",
    "embeddings_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"mean_embeddings\", SpacyVectorTransformer(nlp)),\n",
    "        (\"classifier\", LinearSVC(class_weight='balanced')),\n",
    "    ]\n",
    ")\n",
    "X_train_u, X_test_u, y_train_u, y_test_u = train_test_split(df_10_unique.Text,df_10_unique.DCO_ID,test_size=0.25)\n",
    "embeddings_pipeline.fit(X_train_u, y_train_u)\n",
    "y_pred_u = embeddings_pipeline.predict(X_test_u)\n",
    "\n",
    "print(\"précison:\", accuracy_score(y_test_u,y_pred_u) )\n",
    "print(\"Balanced_accuracy : \", balanced_accuracy_score(y_test_u,y_pred_u))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = Pipeline([\n",
    "    ('parser', HTMLParser()),\n",
    "    ('text_union', FeatureUnion(\n",
    "        transformer_list = [\n",
    "            ('entity_feature', Pipeline([\n",
    "                ('entity_extractor', EntityExtractor()),\n",
    "                ('entity_vect', CountVectorizer()),\n",
    "            ])),\n",
    "            ('keyphrase_feature', Pipeline([\n",
    "                ('keyphrase_extractor', KeyphraseExtractor()),\n",
    "                ('keyphrase_vect', TfidfVectorizer()),\n",
    "            ])),\n",
    "        ],\n",
    "        transformer_weights= {\n",
    "            'entity_feature': 0.6,\n",
    "            'keyphrase_feature': 0.2,\n",
    "        }\n",
    "    )),\n",
    "    ('clf', LogisticRegression()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = linear_svc.fit(X_train_transformed,y_train_lables_trf)\n",
    "\n",
    "calibrated_svc = CalibratedClassifierCV(base_estimator=linear_svc,\n",
    "                                        cv=\"prefit\")\n",
    "\n",
    "calibrated_svc.fit(X_train_transformed,y_train_lables_trf)\n",
    "predicted = calibrated_svc.predict(X_test_transformed)\n",
    "    \n",
    "to_predict = [\"I have outdated information on my credit report that I have previously disputed that has yet to be removed this information is more then seven years old and does not meet credit reporting requirements\"]\n",
    "p_count = count_vect.transform(to_predict)\n",
    "p_tfidf = tf_transformer.transform(p_count)\n",
    "print('Average accuracy on test set={}'.format(np.mean(predicted == labels.transform(y_test))))\n",
    "print('Predicted probabilities of demo input string are')\n",
    "print(calibrated_svc.predict_proba(p_tfidf))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DGS-env",
   "language": "python",
   "name": "dgs-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
