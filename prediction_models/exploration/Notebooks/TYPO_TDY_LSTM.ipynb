{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Inférence de l'effet - Stratégie Multilabels - Approche deep Learning\n",
    "Dans ce Notebook, nous cosntruisons un modèle qui permet d'inférer l'EFFET à partir de la classification de l'incident et des données textuelles en ce basant sur des reseau récurents commes GRU/LSTM etc.\n",
    "\n",
    "En effet, ces approches ont montré des réultats très encourageant et nous voulons explorer cette direction pour peut être faire des réceau recurent notre modèle par défault.\n",
    "\n",
    "Nous considérons ce problème comme un problème de classification multiclasses et multilabels. En effet, il y a plusieurs effets possibles et un incidents peut entrainer plusieurs effets.\n",
    "\n",
    "Dans ce note book nous nous posons les questions suivantes : \n",
    "- Quel est l'impact du drop out ?\n",
    "- Rajouter des couches augmentent-ils les performaces ?\n",
    "- L'utilisation de réseaux bidirectionnel est-elle pertinente ?\n",
    "- Une couche d'attention est-elle utile ?\n",
    "- Attention is all we need, really ?\n",
    "- Utilisation des embeddings \n",
    "- Concaténation des modèles sur différentes entrées ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,LSTM,Embedding,SpatialDropout1D, Bidirectional,SimpleRNN,Input, concatenate, Reshape\n",
    "import tensorflow \n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer\n",
    "from keras.layers import Concatenate, GlobalMaxPool1D, Dropout\n",
    "from tensorflow.keras.layers import Attention\n",
    "\n",
    "from sklearn.metrics import  precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "tensorflow.random.set_seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user1/DGS-env/lib/python3.7/site-packages/sklearn/preprocessing/_label.py:995: UserWarning: unknown class(es) [175, 220, 229, 23, 254, 277, 278, 373, 440, 446, 465, 541, 564, 566, 572, 581, 583, 588, 592, 594, 613, 618] will be ignored\n",
      "  .format(sorted(unknown, key=str)))\n"
     ]
    }
   ],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "train = pd.read_pickle('./data_split/train.pkl')\n",
    "# Pour faire un modèle sans le \n",
    "#train = train[~train['TEF_ID'].map(lambda x : 106 in x)]\n",
    "X_train = train[['FABRICANT','CLASSIFICATION','DESCRIPTION_INCIDENT','ETAT_PATIENT']]\n",
    "y_train = mlb.fit_transform(train['TDY_ID'])\n",
    "test =  pd.read_pickle('./data_split/test.pkl')\n",
    "#test = test[~test['TEF_ID'].map(lambda x : k in x)]\n",
    "X_test = test[['FABRICANT','CLASSIFICATION','DESCRIPTION_INCIDENT','ETAT_PATIENT']]\n",
    "y_test = mlb.transform(test['TDY_ID'])\n",
    "\n",
    "\n",
    "X_train_dgs = np.load('results/dgs_camenbert_train_vec.npy')\n",
    "X_test_dgs =np.load('results/dgs_camenbert_test_vec.npy')\n",
    "\n",
    "\n",
    "\n",
    "df_effets = pd.read_csv(\"data/ref_MRV/referentiel_dispositif_effets_connus.csv\",delimiter=';',encoding='ISO-8859-1')\n",
    "df_dys = pd.read_csv(\"data/ref_MRV/referentiel_dispositif_dysfonctionnement.csv\",delimiter=';',encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 LSTM et TFIDF, une première baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user1/DGS-env/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:386: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['qu', 'quelqu'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/home/user1/DGS-env/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:386: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['qu', 'quelqu'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/home/user1/DGS-env/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:386: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['qu', 'quelqu'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 4s, sys: 2min 52s, total: 5min 57s\n",
      "Wall time: 1min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,HashingVectorizer\n",
    "import spacy\n",
    "nlp =spacy.load('fr')\n",
    "from spacy.lang.fr.stop_words import STOP_WORDS\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    [('description_tfidf',TfidfVectorizer(sublinear_tf=True, min_df=3,\n",
    "                            ngram_range=(1, 1),\n",
    "                            stop_words=STOP_WORDS,\n",
    "                            max_features = 10000,norm = 'l2'), 'DESCRIPTION_INCIDENT'),\n",
    "     \n",
    "     ('etat_pat_tfidf', TfidfVectorizer(sublinear_tf=True, min_df=3,ngram_range=(1, 1),\n",
    "                                       stop_words=STOP_WORDS,\n",
    "                                       max_features = 10000,norm = 'l2'), 'ETAT_PATIENT'),\n",
    "     \n",
    "     ('fabricant_tfidf',TfidfVectorizer(sublinear_tf=True, min_df=3,\n",
    "                            ngram_range=(1, 1),\n",
    "                            stop_words=STOP_WORDS,\n",
    "                            max_features = 5000,norm = 'l2'), 'FABRICANT')\n",
    "     ],\n",
    "    \n",
    "    remainder='passthrough')\n",
    "\n",
    "X_train_, X_test_ =preprocess.fit_transform(X_train),preprocess.transform(X_test)\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=1000)\n",
    "X_train_ = svd.fit_transform(X_train_)\n",
    "X_test_ = svd.transform(X_test_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ = np.reshape(X_train_, (X_train_.shape[0], 1, X_train_.shape[1]))\n",
    "X_test_ = np.reshape(X_test_, (X_test_.shape[0], 1, X_test_.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 21059 samples, validate on 5265 samples\n",
      "Epoch 1/10\n",
      "21059/21059 [==============================] - 10s 470us/step - loss: 0.0103 - categorical_accuracy: 0.2212 - val_loss: 0.0095 - val_categorical_accuracy: 0.2577\n",
      "Epoch 2/10\n",
      "21059/21059 [==============================] - 11s 520us/step - loss: 0.0082 - categorical_accuracy: 0.3351 - val_loss: 0.0084 - val_categorical_accuracy: 0.3077\n",
      "Epoch 3/10\n",
      "21059/21059 [==============================] - 12s 569us/step - loss: 0.0069 - categorical_accuracy: 0.4104 - val_loss: 0.0075 - val_categorical_accuracy: 0.3510\n",
      "Epoch 4/10\n",
      "21059/21059 [==============================] - 12s 547us/step - loss: 0.0059 - categorical_accuracy: 0.4705 - val_loss: 0.0070 - val_categorical_accuracy: 0.3787\n",
      "Epoch 5/10\n",
      "21059/21059 [==============================] - 11s 537us/step - loss: 0.0052 - categorical_accuracy: 0.5120 - val_loss: 0.0067 - val_categorical_accuracy: 0.3951\n",
      "Epoch 6/10\n",
      "21059/21059 [==============================] - 11s 513us/step - loss: 0.0047 - categorical_accuracy: 0.5484 - val_loss: 0.0066 - val_categorical_accuracy: 0.4040\n",
      "Epoch 7/10\n",
      "21059/21059 [==============================] - 14s 643us/step - loss: 0.0044 - categorical_accuracy: 0.5727 - val_loss: 0.0065 - val_categorical_accuracy: 0.4011\n",
      "Epoch 8/10\n",
      "21059/21059 [==============================] - 14s 676us/step - loss: 0.0041 - categorical_accuracy: 0.5974 - val_loss: 0.0065 - val_categorical_accuracy: 0.4053\n",
      "Epoch 9/10\n",
      "21059/21059 [==============================] - 15s 716us/step - loss: 0.0038 - categorical_accuracy: 0.6176 - val_loss: 0.0065 - val_categorical_accuracy: 0.4002\n",
      "Epoch 10/10\n",
      "21059/21059 [==============================] - 13s 625us/step - loss: 0.0036 - categorical_accuracy: 0.6386 - val_loss: 0.0066 - val_categorical_accuracy: 0.3994\n",
      "6580/6580 [==============================] - 1s 182us/step\n",
      "loss :  0.005527377376576325\n",
      "categorical accuracy:  0.47446808218955994\n",
      "####################################\n",
      "For threshold:  0.01\n",
      "Samples-average quality numbers\n",
      "Precision: 0.2136, Recall: 0.8540, F1-measure: 0.3037\n",
      "For threshold:  0.04\n",
      "Samples-average quality numbers\n",
      "Precision: 0.3524, Recall: 0.7313, F1-measure: 0.4347\n",
      "For threshold:  0.06\n",
      "Samples-average quality numbers\n",
      "Precision: 0.3984, Recall: 0.6844, F1-measure: 0.4677\n",
      "For threshold:  0.08\n",
      "Samples-average quality numbers\n",
      "Precision: 0.4270, Recall: 0.6489, F1-measure: 0.4848\n",
      "For threshold:  0.1\n",
      "Samples-average quality numbers\n",
      "Precision: 0.4479, Recall: 0.6204, F1-measure: 0.4938\n",
      "For threshold:  0.12\n",
      "Samples-average quality numbers\n",
      "Precision: 0.4602, Recall: 0.5948, F1-measure: 0.4961\n",
      "For threshold:  0.14\n",
      "Samples-average quality numbers\n",
      "Precision: 0.4692, Recall: 0.5725, F1-measure: 0.4956\n",
      "For threshold:  0.16\n",
      "Samples-average quality numbers\n",
      "Precision: 0.4770, Recall: 0.5541, F1-measure: 0.4951\n",
      "For threshold:  0.2\n",
      "Samples-average quality numbers\n",
      "Precision: 0.4784, Recall: 0.5199, F1-measure: 0.4845\n",
      "For threshold:  0.25\n",
      "Samples-average quality numbers\n",
      "Precision: 0.4692, Recall: 0.4823, F1-measure: 0.4654\n",
      "For threshold:  0.3\n",
      "Samples-average quality numbers\n",
      "Precision: 0.4557, Recall: 0.4500, F1-measure: 0.4453\n",
      "For threshold:  0.35\n",
      "Samples-average quality numbers\n",
      "Precision: 0.4347, Recall: 0.4205, F1-measure: 0.4215\n",
      "For threshold:  0.4\n",
      "Samples-average quality numbers\n",
      "Precision: 0.4132, Recall: 0.3946, F1-measure: 0.3990\n",
      "For threshold:  0.5\n",
      "Samples-average quality numbers\n",
      "Precision: 0.3644, Recall: 0.3461, F1-measure: 0.3519\n",
      "For threshold:  0.6\n",
      "Samples-average quality numbers\n",
      "Precision: 0.3167, Recall: 0.3029, F1-measure: 0.3073\n",
      "For threshold:  0.7\n",
      "Samples-average quality numbers\n",
      "Precision: 0.2733, Recall: 0.2633, F1-measure: 0.2664\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(200))\n",
    "model.add(Dense(y_train.shape[1], activation='softmax'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "history = model.fit(X_train_, y_train, epochs=epochs, batch_size=batch_size,validation_split=0.2)\n",
    "\n",
    "score,cat_acc = model.evaluate(X_test_,y_test)\n",
    "\n",
    "y_pred = model.predict(X_test_)\n",
    "\n",
    "print('loss : ', score)\n",
    "print('categorical accuracy: ',cat_acc)\n",
    "\n",
    "print('####################################')\n",
    "\n",
    "thresholds = [0.01,0.04,0.06,0.08,0.1,0.12,0.14,0.16,0.2,0.25,0.3,0.35,0.4,0.5,0.6,0.7]\n",
    "for val in thresholds:\n",
    "    print(\"For threshold: \", val)\n",
    "    pred=y_pred.copy()\n",
    "  \n",
    "    pred[pred>=val]=1\n",
    "    pred[pred<val]=0\n",
    "  \n",
    "    precision = precision_score(y_test, pred, average='samples')\n",
    "    recall = recall_score(y_test, pred, average='samples')\n",
    "    f1 = f1_score(y_test, pred, average='samples')\n",
    "   \n",
    "    print(\"Samples-average quality numbers\")\n",
    "    print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commentaires : \n",
    "\n",
    "Nous obtenons un score de base line de 0.6730. Nous allons essayer d'améliorer ce score avec difféntes approches : \n",
    "- Remplacer le tfidf par un embedding entrainé par le modèle \n",
    "    - Embedding simple\n",
    "    - Embedding par colonnes\n",
    "    \n",
    "- Essayer un modèle biLSTM ou GRU ou bi GRU\n",
    "- Essayer de rajouter une couche d'attention dans Keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 21059 samples, validate on 5265 samples\n",
      "Epoch 1/4\n",
      "21059/21059 [==============================] - 17s 812us/step - loss: 0.0093 - categorical_accuracy: 0.2753 - val_loss: 0.0084 - val_categorical_accuracy: 0.3102\n",
      "Epoch 2/4\n",
      "21059/21059 [==============================] - 14s 664us/step - loss: 0.0069 - categorical_accuracy: 0.4050 - val_loss: 0.0070 - val_categorical_accuracy: 0.3789\n",
      "Epoch 3/4\n",
      "21059/21059 [==============================] - 17s 792us/step - loss: 0.0056 - categorical_accuracy: 0.4744 - val_loss: 0.0066 - val_categorical_accuracy: 0.3964\n",
      "Epoch 4/4\n",
      "21059/21059 [==============================] - 16s 772us/step - loss: 0.0049 - categorical_accuracy: 0.5174 - val_loss: 0.0065 - val_categorical_accuracy: 0.3985\n",
      "6580/6580 [==============================] - 2s 255us/step\n",
      "loss :  0.005549738023396139\n",
      "categorical accuracy:  0.46702128648757935\n",
      "####################################\n",
      "For threshold:  0.01\n",
      "Samples-average quality numbers\n",
      "Precision: 0.2107, Recall: 0.8676, F1-measure: 0.2918\n",
      "For threshold:  0.04\n",
      "Samples-average quality numbers\n",
      "Precision: 0.3481, Recall: 0.7245, F1-measure: 0.4276\n",
      "For threshold:  0.06\n",
      "Samples-average quality numbers\n",
      "Precision: 0.3930, Recall: 0.6726, F1-measure: 0.4608\n",
      "For threshold:  0.08\n",
      "Samples-average quality numbers\n",
      "Precision: 0.4210, Recall: 0.6315, F1-measure: 0.4759\n",
      "For threshold:  0.1\n",
      "Samples-average quality numbers\n",
      "Precision: 0.4426, Recall: 0.6019, F1-measure: 0.4852\n",
      "For threshold:  0.12\n",
      "Samples-average quality numbers\n",
      "Precision: 0.4576, Recall: 0.5746, F1-measure: 0.4878\n",
      "For threshold:  0.14\n",
      "Samples-average quality numbers\n",
      "Precision: 0.4641, Recall: 0.5503, F1-measure: 0.4848\n",
      "For threshold:  0.16\n",
      "Samples-average quality numbers\n",
      "Precision: 0.4627, Recall: 0.5262, F1-measure: 0.4765\n",
      "For threshold:  0.2\n",
      "Samples-average quality numbers\n",
      "Precision: 0.4608, Recall: 0.4899, F1-measure: 0.4629\n",
      "For threshold:  0.25\n",
      "Samples-average quality numbers\n",
      "Precision: 0.4444, Recall: 0.4475, F1-measure: 0.4370\n",
      "For threshold:  0.3\n",
      "Samples-average quality numbers\n",
      "Precision: 0.4241, Recall: 0.4139, F1-measure: 0.4125\n",
      "For threshold:  0.35\n",
      "Samples-average quality numbers\n",
      "Precision: 0.4021, Recall: 0.3844, F1-measure: 0.3882\n",
      "For threshold:  0.4\n",
      "Samples-average quality numbers\n",
      "Precision: 0.3763, Recall: 0.3572, F1-measure: 0.3628\n",
      "For threshold:  0.5\n",
      "Samples-average quality numbers\n",
      "Precision: 0.3260, Recall: 0.3106, F1-measure: 0.3155\n",
      "For threshold:  0.6\n",
      "Samples-average quality numbers\n",
      "Precision: 0.2865, Recall: 0.2755, F1-measure: 0.2790\n",
      "For threshold:  0.7\n",
      "Samples-average quality numbers\n",
      "Precision: 0.2456, Recall: 0.2377, F1-measure: 0.2402\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(200, dropout=0.15))\n",
    "model.add(Dense(y_train.shape[1], activation='softmax'))\n",
    "\n",
    "\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['categorical_accuracy'])\n",
    "\n",
    "epochs = 4\n",
    "batch_size = 32\n",
    "\n",
    "history = model.fit(X_train_, y_train, epochs=epochs, batch_size=batch_size,validation_split=0.2)\n",
    "\n",
    "score,cat_acc = model.evaluate(X_test_,y_test)\n",
    "\n",
    "y_pred = model.predict(X_test_)\n",
    "\n",
    "print('loss : ', score)\n",
    "print('categorical accuracy: ',cat_acc)\n",
    "\n",
    "print('####################################')\n",
    "\n",
    "thresholds = [0.01,0.04,0.06,0.08,0.1,0.12,0.14,0.16,0.2,0.25,0.3,0.35,0.4,0.5,0.6,0.7]\n",
    "for val in thresholds:\n",
    "    print(\"For threshold: \", val)\n",
    "    pred=y_pred.copy()\n",
    "  \n",
    "    pred[pred>=val]=1\n",
    "    pred[pred<val]=0\n",
    "  \n",
    "    precision = precision_score(y_test, pred, average='samples')\n",
    "    recall = recall_score(y_test, pred, average='samples')\n",
    "    f1 = f1_score(y_test, pred, average='samples')\n",
    "   \n",
    "    print(\"Samples-average quality numbers\")\n",
    "    print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Comment performe un reseau de BiLSTM  ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 21059 samples, validate on 5265 samples\n",
      "Epoch 1/5\n",
      "21059/21059 [==============================] - 25s 1ms/step - loss: 0.0100 - categorical_accuracy: 0.2330 - val_loss: 0.0092 - val_categorical_accuracy: 0.2705\n",
      "Epoch 2/5\n",
      "21059/21059 [==============================] - 23s 1ms/step - loss: 0.0078 - categorical_accuracy: 0.3578 - val_loss: 0.0079 - val_categorical_accuracy: 0.3273\n",
      "Epoch 3/5\n",
      "21059/21059 [==============================] - 22s 1ms/step - loss: 0.0065 - categorical_accuracy: 0.4308 - val_loss: 0.0071 - val_categorical_accuracy: 0.3719\n",
      "Epoch 4/5\n",
      "21059/21059 [==============================] - 17s 813us/step - loss: 0.0055 - categorical_accuracy: 0.4848 - val_loss: 0.0067 - val_categorical_accuracy: 0.3911\n",
      "Epoch 5/5\n",
      "21059/21059 [==============================] - 23s 1ms/step - loss: 0.0049 - categorical_accuracy: 0.5310 - val_loss: 0.0065 - val_categorical_accuracy: 0.3985\n",
      "6580/6580 [==============================] - 2s 329us/step\n",
      "loss :  0.00564778517014531\n",
      "categorical accuracy:  0.4729483425617218\n",
      "####################################\n",
      "For threshold:  0.01\n",
      "Samples-average quality numbers\n",
      "Precision: 0.2043, Recall: 0.8603, F1-measure: 0.2868\n",
      "For threshold:  0.04\n",
      "Samples-average quality numbers\n",
      "Precision: 0.3467, Recall: 0.7155, F1-measure: 0.4253\n",
      "For threshold:  0.06\n",
      "Samples-average quality numbers\n",
      "Precision: 0.3912, Recall: 0.6592, F1-measure: 0.4568\n",
      "For threshold:  0.08\n",
      "Samples-average quality numbers\n",
      "Precision: 0.4220, Recall: 0.6217, F1-measure: 0.4742\n",
      "For threshold:  0.1\n",
      "Samples-average quality numbers\n",
      "Precision: 0.4435, Recall: 0.5902, F1-measure: 0.4825\n",
      "For threshold:  0.12\n",
      "Samples-average quality numbers\n",
      "Precision: 0.4546, Recall: 0.5640, F1-measure: 0.4831\n",
      "For threshold:  0.14\n",
      "Samples-average quality numbers\n",
      "Precision: 0.4604, Recall: 0.5404, F1-measure: 0.4797\n",
      "For threshold:  0.16\n",
      "Samples-average quality numbers\n",
      "Precision: 0.4639, Recall: 0.5205, F1-measure: 0.4757\n",
      "For threshold:  0.2\n",
      "Samples-average quality numbers\n",
      "Precision: 0.4582, Recall: 0.4849, F1-measure: 0.4597\n",
      "For threshold:  0.25\n",
      "Samples-average quality numbers\n",
      "Precision: 0.4442, Recall: 0.4489, F1-measure: 0.4382\n",
      "For threshold:  0.3\n",
      "Samples-average quality numbers\n",
      "Precision: 0.4243, Recall: 0.4160, F1-measure: 0.4139\n",
      "For threshold:  0.35\n",
      "Samples-average quality numbers\n",
      "Precision: 0.4022, Recall: 0.3862, F1-measure: 0.3891\n",
      "For threshold:  0.4\n",
      "Samples-average quality numbers\n",
      "Precision: 0.3758, Recall: 0.3568, F1-measure: 0.3621\n",
      "For threshold:  0.5\n",
      "Samples-average quality numbers\n",
      "Precision: 0.3274, Recall: 0.3121, F1-measure: 0.3170\n",
      "For threshold:  0.6\n",
      "Samples-average quality numbers\n",
      "Precision: 0.2842, Recall: 0.2726, F1-measure: 0.2763\n",
      "For threshold:  0.7\n",
      "Samples-average quality numbers\n",
      "Precision: 0.2436, Recall: 0.2356, F1-measure: 0.2381\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(200, dropout=0.1)))\n",
    "model.add(Dense(y_train.shape[1], activation='softmax'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n",
    "\n",
    "epochs = 5\n",
    "batch_size = 32\n",
    "\n",
    "history = model.fit(X_train_, y_train, epochs=epochs, batch_size=batch_size,validation_split=0.2)\n",
    "\n",
    "score,cat_acc = model.evaluate(X_test_,y_test)\n",
    "\n",
    "y_pred = model.predict(X_test_)\n",
    "\n",
    "print('loss : ', score)\n",
    "print('categorical accuracy: ',cat_acc)\n",
    "\n",
    "print('####################################')\n",
    "\n",
    "thresholds = [0.01,0.04,0.06,0.08,0.1,0.12,0.14,0.16,0.2,0.25,0.3,0.35,0.4,0.5,0.6,0.7]\n",
    "for val in thresholds:\n",
    "    print(\"For threshold: \", val)\n",
    "    pred=y_pred.copy()\n",
    "  \n",
    "    pred[pred>=val]=1\n",
    "    pred[pred<val]=0\n",
    "  \n",
    "    precision = precision_score(y_test, pred, average='samples')\n",
    "    recall = recall_score(y_test, pred, average='samples')\n",
    "    f1 = f1_score(y_test, pred, average='samples')\n",
    "   \n",
    "    print(\"Samples-average quality numbers\")\n",
    "    print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commentaire ?\n",
    "\n",
    "L'augementation de la performance est très minime : 0.6733 ?\n",
    "Nous remarquons que notre modèle sur apprend assez rapidement, nous allons essayer d'ajouter une couche de dropout pour limiter ce sur apprentissage.\n",
    "## Si on ajoute du dropout ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 21059 samples, validate on 5265 samples\n",
      "Epoch 1/10\n",
      "21059/21059 [==============================] - 25s 1ms/step - loss: 0.0096 - categorical_accuracy: 0.2406 - val_loss: 0.0090 - val_categorical_accuracy: 0.2651\n",
      "Epoch 2/10\n",
      "21059/21059 [==============================] - 22s 1ms/step - loss: 0.0077 - categorical_accuracy: 0.3513 - val_loss: 0.0079 - val_categorical_accuracy: 0.3206\n",
      "Epoch 3/10\n",
      "21059/21059 [==============================] - 22s 1ms/step - loss: 0.0066 - categorical_accuracy: 0.4090 - val_loss: 0.0073 - val_categorical_accuracy: 0.3499\n",
      "Epoch 4/10\n",
      "21059/21059 [==============================] - 21s 1ms/step - loss: 0.0060 - categorical_accuracy: 0.4440 - val_loss: 0.0069 - val_categorical_accuracy: 0.3742\n",
      "Epoch 5/10\n",
      "21059/21059 [==============================] - 21s 1ms/step - loss: 0.0055 - categorical_accuracy: 0.4749 - val_loss: 0.0067 - val_categorical_accuracy: 0.3871\n",
      "Epoch 6/10\n",
      "21059/21059 [==============================] - 19s 900us/step - loss: 0.0051 - categorical_accuracy: 0.5049 - val_loss: 0.0067 - val_categorical_accuracy: 0.3918\n",
      "Epoch 7/10\n",
      "21059/21059 [==============================] - 19s 909us/step - loss: 0.0048 - categorical_accuracy: 0.5244 - val_loss: 0.0066 - val_categorical_accuracy: 0.3958\n",
      "Epoch 8/10\n",
      "21059/21059 [==============================] - 23s 1ms/step - loss: 0.0046 - categorical_accuracy: 0.5402 - val_loss: 0.0066 - val_categorical_accuracy: 0.4013\n",
      "Epoch 9/10\n",
      "21059/21059 [==============================] - 23s 1ms/step - loss: 0.0044 - categorical_accuracy: 0.5580 - val_loss: 0.0066 - val_categorical_accuracy: 0.3983\n",
      "Epoch 10/10\n",
      "21059/21059 [==============================] - 25s 1ms/step - loss: 0.0042 - categorical_accuracy: 0.5718 - val_loss: 0.0067 - val_categorical_accuracy: 0.3920\n",
      "6580/6580 [==============================] - 2s 249us/step\n",
      "loss :  0.005607755800375098\n",
      "categorical accuracy:  0.4648936092853546\n",
      "####################################\n",
      "For threshold:  0.01\n",
      "Samples-average quality numbers\n",
      "Precision: 0.2078, Recall: 0.8618, F1-measure: 0.2914\n",
      "For threshold:  0.04\n",
      "Samples-average quality numbers\n",
      "Precision: 0.3473, Recall: 0.7267, F1-measure: 0.4258\n",
      "For threshold:  0.06\n",
      "Samples-average quality numbers\n",
      "Precision: 0.3890, Recall: 0.6740, F1-measure: 0.4567\n",
      "For threshold:  0.08\n",
      "Samples-average quality numbers\n",
      "Precision: 0.4199, Recall: 0.6365, F1-measure: 0.4757\n",
      "For threshold:  0.1\n",
      "Samples-average quality numbers\n",
      "Precision: 0.4423, Recall: 0.6074, F1-measure: 0.4860\n",
      "For threshold:  0.12\n",
      "Samples-average quality numbers\n",
      "Precision: 0.4577, Recall: 0.5834, F1-measure: 0.4907\n",
      "For threshold:  0.14\n",
      "Samples-average quality numbers\n",
      "Precision: 0.4654, Recall: 0.5602, F1-measure: 0.4889\n",
      "For threshold:  0.16\n",
      "Samples-average quality numbers\n",
      "Precision: 0.4721, Recall: 0.5416, F1-measure: 0.4874\n",
      "For threshold:  0.2\n",
      "Samples-average quality numbers\n",
      "Precision: 0.4739, Recall: 0.5086, F1-measure: 0.4771\n",
      "For threshold:  0.25\n",
      "Samples-average quality numbers\n",
      "Precision: 0.4635, Recall: 0.4709, F1-measure: 0.4571\n",
      "For threshold:  0.3\n",
      "Samples-average quality numbers\n",
      "Precision: 0.4457, Recall: 0.4384, F1-measure: 0.4348\n",
      "For threshold:  0.35\n",
      "Samples-average quality numbers\n",
      "Precision: 0.4221, Recall: 0.4067, F1-measure: 0.4088\n",
      "For threshold:  0.4\n",
      "Samples-average quality numbers\n",
      "Precision: 0.4001, Recall: 0.3797, F1-measure: 0.3853\n",
      "For threshold:  0.5\n",
      "Samples-average quality numbers\n",
      "Precision: 0.3492, Recall: 0.3304, F1-measure: 0.3363\n",
      "For threshold:  0.6\n",
      "Samples-average quality numbers\n",
      "Precision: 0.3009, Recall: 0.2875, F1-measure: 0.2918\n",
      "For threshold:  0.7\n",
      "Samples-average quality numbers\n",
      "Precision: 0.2573, Recall: 0.2477, F1-measure: 0.2508\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(200, return_sequences=True, dropout=0.1)))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dense(100, activation=\"elu\"))\n",
    "model.add(Dropout(0.1))\n",
    "model.add( Dense(y_train.shape[1], activation=\"softmax\"))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n",
    "\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "history = model.fit(X_train_, y_train, epochs=epochs, batch_size=batch_size,validation_split=0.2)\n",
    "\n",
    "score,cat_acc = model.evaluate(X_test_,y_test)\n",
    "\n",
    "y_pred = model.predict(X_test_)\n",
    "\n",
    "print('loss : ', score)\n",
    "print('categorical accuracy: ',cat_acc)\n",
    "\n",
    "print('####################################')\n",
    "\n",
    "thresholds = [0.01,0.04,0.06,0.08,0.1,0.12,0.14,0.16,0.2,0.25,0.3,0.35,0.4,0.5,0.6,0.7]\n",
    "for val in thresholds:\n",
    "    print(\"For threshold: \", val)\n",
    "    pred=y_pred.copy()\n",
    "  \n",
    "    pred[pred>=val]=1\n",
    "    pred[pred<val]=0\n",
    "  \n",
    "    precision = precision_score(y_test, pred, average='samples')\n",
    "    recall = recall_score(y_test, pred, average='samples')\n",
    "    f1 = f1_score(y_test, pred, average='samples')\n",
    "   \n",
    "    print(\"Samples-average quality numbers\")\n",
    "    print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commentaire ?\n",
    "\n",
    "Les performances changent très peu,  nous allons essayer d'augementer la profondeur de notre reseau.\n",
    "\n",
    "## Si on ajoute plusieurs couches ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 21059 samples, validate on 5265 samples\n",
      "Epoch 1/10\n",
      "21059/21059 [==============================] - 31s 1ms/step - loss: 0.0095 - categorical_accuracy: 0.2496 - val_loss: 0.0089 - val_categorical_accuracy: 0.2832\n",
      "Epoch 2/10\n",
      "21059/21059 [==============================] - 25s 1ms/step - loss: 0.0075 - categorical_accuracy: 0.3683 - val_loss: 0.0077 - val_categorical_accuracy: 0.3328\n",
      "Epoch 3/10\n",
      "21059/21059 [==============================] - 26s 1ms/step - loss: 0.0064 - categorical_accuracy: 0.4279 - val_loss: 0.0071 - val_categorical_accuracy: 0.3654\n",
      "Epoch 4/10\n",
      "21059/21059 [==============================] - 21s 1ms/step - loss: 0.0057 - categorical_accuracy: 0.4646 - val_loss: 0.0068 - val_categorical_accuracy: 0.3869\n",
      "Epoch 5/10\n",
      "21059/21059 [==============================] - 24s 1ms/step - loss: 0.0053 - categorical_accuracy: 0.4910 - val_loss: 0.0067 - val_categorical_accuracy: 0.3892\n",
      "Epoch 6/10\n",
      "21059/21059 [==============================] - 22s 1ms/step - loss: 0.0050 - categorical_accuracy: 0.5165 - val_loss: 0.0067 - val_categorical_accuracy: 0.3979\n",
      "Epoch 7/10\n",
      "21059/21059 [==============================] - 23s 1ms/step - loss: 0.0047 - categorical_accuracy: 0.5390 - val_loss: 0.0066 - val_categorical_accuracy: 0.3932\n",
      "Epoch 8/10\n",
      "21059/21059 [==============================] - 22s 1ms/step - loss: 0.0045 - categorical_accuracy: 0.5543 - val_loss: 0.0066 - val_categorical_accuracy: 0.3854\n",
      "Epoch 9/10\n",
      "21059/21059 [==============================] - 20s 937us/step - loss: 0.0043 - categorical_accuracy: 0.5687 - val_loss: 0.0066 - val_categorical_accuracy: 0.4006\n",
      "Epoch 10/10\n",
      "21059/21059 [==============================] - 20s 966us/step - loss: 0.0041 - categorical_accuracy: 0.5865 - val_loss: 0.0066 - val_categorical_accuracy: 0.3953\n",
      "6580/6580 [==============================] - 1s 141us/step\n",
      "loss :  0.0056513636981598515\n",
      "categorical accuracy:  0.46702128648757935\n",
      "####################################\n",
      "For threshold:  0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user1/DGS-env/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples-average quality numbers\n",
      "Precision: 0.2242, Recall: 0.8423, F1-measure: 0.3096\n",
      "For threshold:  0.04\n",
      "Samples-average quality numbers\n",
      "Precision: 0.3604, Recall: 0.7147, F1-measure: 0.4368\n",
      "For threshold:  0.06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user1/DGS-env/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples-average quality numbers\n",
      "Precision: 0.4026, Recall: 0.6706, F1-measure: 0.4675\n",
      "For threshold:  0.08\n",
      "Samples-average quality numbers\n",
      "Precision: 0.4274, Recall: 0.6379, F1-measure: 0.4820\n",
      "For threshold:  0.1\n",
      "Samples-average quality numbers\n",
      "Precision: 0.4460, Recall: 0.6097, F1-measure: 0.4897\n",
      "For threshold:  0.12\n",
      "Samples-average quality numbers\n",
      "Precision: 0.4597, Recall: 0.5851, F1-measure: 0.4931\n",
      "For threshold:  0.14\n",
      "Samples-average quality numbers\n",
      "Precision: 0.4677, Recall: 0.5606, F1-measure: 0.4914\n",
      "For threshold:  0.16\n",
      "Samples-average quality numbers\n",
      "Precision: 0.4719, Recall: 0.5412, F1-measure: 0.4877\n",
      "For threshold:  0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user1/DGS-env/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1465: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  average, \"true nor predicted\", 'F-score is', len(true_sum)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples-average quality numbers\n",
      "Precision: 0.4770, Recall: 0.5113, F1-measure: 0.4805\n",
      "For threshold:  0.25\n",
      "Samples-average quality numbers\n",
      "Precision: 0.4709, Recall: 0.4773, F1-measure: 0.4647\n",
      "For threshold:  0.3\n",
      "Samples-average quality numbers\n",
      "Precision: 0.4571, Recall: 0.4485, F1-measure: 0.4458\n",
      "For threshold:  0.35\n",
      "Samples-average quality numbers\n",
      "Precision: 0.4375, Recall: 0.4211, F1-measure: 0.4236\n",
      "For threshold:  0.4\n",
      "Samples-average quality numbers\n",
      "Precision: 0.4177, Recall: 0.3962, F1-measure: 0.4023\n",
      "For threshold:  0.5\n",
      "Samples-average quality numbers\n",
      "Precision: 0.3723, Recall: 0.3537, F1-measure: 0.3596\n",
      "For threshold:  0.6\n",
      "Samples-average quality numbers\n",
      "Precision: 0.3266, Recall: 0.3120, F1-measure: 0.3166\n",
      "For threshold:  0.7\n",
      "Samples-average quality numbers\n",
      "Precision: 0.2781, Recall: 0.2691, F1-measure: 0.2719\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "reg = regularizers.l2(1e-4)\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(256,  return_sequences=True, dropout=0.1)))\n",
    "model.add((SimpleRNN(128,return_sequences=True)))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(y_train.shape[1], activation=\"softmax\"))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n",
    "\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "history = model.fit(X_train_, y_train, epochs=epochs, batch_size=batch_size,validation_split=0.2)\n",
    "\n",
    "score,cat_acc = model.evaluate(X_test_,y_test)\n",
    "\n",
    "y_pred = model.predict(X_test_)\n",
    "\n",
    "print('loss : ', score)\n",
    "print('categorical accuracy: ',cat_acc)\n",
    "\n",
    "print('####################################')\n",
    "\n",
    "thresholds = [0.01,0.04,0.06,0.08,0.1,0.12,0.14,0.16,0.2,0.25,0.3,0.35,0.4,0.5,0.6,0.7]\n",
    "for val in thresholds:\n",
    "    print(\"For threshold: \", val)\n",
    "    pred=y_pred.copy()\n",
    "  \n",
    "    pred[pred>=val]=1\n",
    "    pred[pred<val]=0\n",
    "  \n",
    "    precision = precision_score(y_test, pred, average='samples')\n",
    "    recall = recall_score(y_test, pred, average='samples')\n",
    "    f1 = f1_score(y_test, pred, average='samples')\n",
    "   \n",
    "    print(\"Samples-average quality numbers\")\n",
    "    print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commentaires : \n",
    "\n",
    "Encore une fois, le modèle semble avoir atteint un plafond de verre qui ne soit pas dépassable ni par la prodondeur, ni par l'ajout de droptout. Nous allons essayer d'ajouter une couche d'attention.\n",
    "\n",
    "## Une couche d'attention ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 21059 samples, validate on 5265 samples\n",
      "Epoch 1/10\n",
      "21059/21059 [==============================] - 17s 806us/step - loss: 0.1038 - categorical_accuracy: 0.1660 - val_loss: 0.0116 - val_categorical_accuracy: 0.1896\n",
      "Epoch 2/10\n",
      "21059/21059 [==============================] - 15s 701us/step - loss: 0.0104 - categorical_accuracy: 0.1873 - val_loss: 0.0106 - val_categorical_accuracy: 0.1896\n",
      "Epoch 3/10\n",
      "21059/21059 [==============================] - 14s 653us/step - loss: 0.0099 - categorical_accuracy: 0.1999 - val_loss: 0.0102 - val_categorical_accuracy: 0.2053\n",
      "Epoch 4/10\n",
      "21059/21059 [==============================] - 17s 793us/step - loss: 0.0094 - categorical_accuracy: 0.2421 - val_loss: 0.0099 - val_categorical_accuracy: 0.2251\n",
      "Epoch 5/10\n",
      "21059/21059 [==============================] - 16s 765us/step - loss: 0.0090 - categorical_accuracy: 0.2904 - val_loss: 0.0096 - val_categorical_accuracy: 0.2574\n",
      "Epoch 6/10\n",
      "21059/21059 [==============================] - 16s 767us/step - loss: 0.0087 - categorical_accuracy: 0.3167 - val_loss: 0.0092 - val_categorical_accuracy: 0.2741\n",
      "Epoch 7/10\n",
      "21059/21059 [==============================] - 17s 807us/step - loss: 0.0083 - categorical_accuracy: 0.3386 - val_loss: 0.0088 - val_categorical_accuracy: 0.2898\n",
      "Epoch 8/10\n",
      "21059/21059 [==============================] - 15s 727us/step - loss: 0.0078 - categorical_accuracy: 0.3668 - val_loss: 0.0084 - val_categorical_accuracy: 0.3107\n",
      "Epoch 9/10\n",
      "21059/21059 [==============================] - 18s 845us/step - loss: 0.0073 - categorical_accuracy: 0.3970 - val_loss: 0.0080 - val_categorical_accuracy: 0.3187\n",
      "Epoch 10/10\n",
      "21059/21059 [==============================] - 20s 929us/step - loss: 0.0068 - categorical_accuracy: 0.4227 - val_loss: 0.0076 - val_categorical_accuracy: 0.3407\n",
      "6580/6580 [==============================] - 2s 297us/step\n",
      "loss :  0.006936190838962341\n",
      "categorical accuracy:  0.3971124589443207\n",
      "####################################\n",
      "For threshold:  0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user1/DGS-env/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples-average quality numbers\n",
      "Precision: 0.1190, Recall: 0.7778, F1-measure: 0.1915\n",
      "For threshold:  0.04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user1/DGS-env/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples-average quality numbers\n",
      "Precision: 0.2654, Recall: 0.5994, F1-measure: 0.3397\n",
      "For threshold:  0.06\n",
      "Samples-average quality numbers\n",
      "Precision: 0.3179, Recall: 0.5383, F1-measure: 0.3727\n",
      "For threshold:  0.08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user1/DGS-env/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1465: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  average, \"true nor predicted\", 'F-score is', len(true_sum)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples-average quality numbers\n",
      "Precision: 0.3401, Recall: 0.4933, F1-measure: 0.3796\n",
      "For threshold:  0.1\n",
      "Samples-average quality numbers\n",
      "Precision: 0.3529, Recall: 0.4598, F1-measure: 0.3798\n",
      "For threshold:  0.12\n",
      "Samples-average quality numbers\n",
      "Precision: 0.3520, Recall: 0.4315, F1-measure: 0.3713\n",
      "For threshold:  0.14\n",
      "Samples-average quality numbers\n",
      "Precision: 0.3498, Recall: 0.4107, F1-measure: 0.3635\n",
      "For threshold:  0.16\n",
      "Samples-average quality numbers\n",
      "Precision: 0.3476, Recall: 0.3930, F1-measure: 0.3569\n",
      "For threshold:  0.2\n",
      "Samples-average quality numbers\n",
      "Precision: 0.3395, Recall: 0.3666, F1-measure: 0.3431\n",
      "For threshold:  0.25\n",
      "Samples-average quality numbers\n",
      "Precision: 0.3286, Recall: 0.3412, F1-measure: 0.3279\n",
      "For threshold:  0.26\n",
      "Samples-average quality numbers\n",
      "Precision: 0.3255, Recall: 0.3363, F1-measure: 0.3242\n",
      "For threshold:  0.27\n",
      "Samples-average quality numbers\n",
      "Precision: 0.3235, Recall: 0.3324, F1-measure: 0.3218\n",
      "For threshold:  0.28\n",
      "Samples-average quality numbers\n",
      "Precision: 0.3213, Recall: 0.3276, F1-measure: 0.3187\n",
      "For threshold:  0.29\n",
      "Samples-average quality numbers\n",
      "Precision: 0.3163, Recall: 0.3209, F1-measure: 0.3133\n",
      "For threshold:  0.3\n",
      "Samples-average quality numbers\n",
      "Precision: 0.3124, Recall: 0.3153, F1-measure: 0.3088\n",
      "For threshold:  0.35\n",
      "Samples-average quality numbers\n",
      "Precision: 0.2973, Recall: 0.2948, F1-measure: 0.2921\n",
      "For threshold:  0.4\n",
      "Samples-average quality numbers\n",
      "Precision: 0.2863, Recall: 0.2799, F1-measure: 0.2799\n",
      "For threshold:  0.5\n",
      "Samples-average quality numbers\n",
      "Precision: 0.2586, Recall: 0.2491, F1-measure: 0.2512\n",
      "For threshold:  0.6\n",
      "Samples-average quality numbers\n",
      "Precision: 0.2286, Recall: 0.2179, F1-measure: 0.2209\n",
      "For threshold:  0.7\n",
      "Samples-average quality numbers\n",
      "Precision: 0.2007, Recall: 0.1906, F1-measure: 0.1937\n"
     ]
    }
   ],
   "source": [
    "from keras_self_attention import SeqSelfAttention\n",
    "import keras\n",
    "model = Sequential()\n",
    "model.add(LSTM(200, return_sequences=True))\n",
    "model.add(SeqSelfAttention(attention_activation='sigmoid'))\n",
    "model.add(keras.layers.Flatten(name='Flatten'))\n",
    "model.add(Dense(y_train.shape[1], activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n",
    "\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "history = model.fit(X_train_, y_train, epochs=epochs, batch_size=batch_size,validation_split=0.2)\n",
    "\n",
    "score,cat_acc = model.evaluate(X_test_,y_test)\n",
    "\n",
    "y_pred = model.predict(X_test_)\n",
    "\n",
    "print('loss : ', score)\n",
    "print('categorical accuracy: ',cat_acc)\n",
    "\n",
    "print('####################################')\n",
    "\n",
    "thresholds = [0.01,0.04,0.06,0.08,0.1,0.12,0.14,0.16,0.2,0.25,0.26,0.27,0.28,0.29,0.3,0.35,0.4,0.5,0.6,0.7]\n",
    "for val in thresholds:\n",
    "    print(\"For threshold: \", val)\n",
    "    pred=y_pred.copy()\n",
    "  \n",
    "    pred[pred>=val]=1\n",
    "    pred[pred<val]=0\n",
    "  \n",
    "    precision = precision_score(y_test, pred, average='samples')\n",
    "    recall = recall_score(y_test, pred, average='samples')\n",
    "    f1 = f1_score(y_test, pred, average='samples')\n",
    "   \n",
    "    print(\"Samples-average quality numbers\")\n",
    "    print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commentaire : \n",
    "Les résultats sembles similaire. Toutefois, la loss de validation est bien meilleur sans pour autant augmenter le f1 score. Nous allons essayer de faire seulement un réseau basé sur l'attention. Nous utilison ici, une verion multiplicative de l'attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 21059 samples, validate on 5265 samples\n",
      "Epoch 1/20\n",
      "21059/21059 [==============================] - 18s 831us/step - loss: 0.0192 - categorical_accuracy: 0.2267 - val_loss: 0.0149 - val_categorical_accuracy: 0.2500\n",
      "Epoch 2/20\n",
      "21059/21059 [==============================] - 18s 844us/step - loss: 0.0127 - categorical_accuracy: 0.3561 - val_loss: 0.0139 - val_categorical_accuracy: 0.2976\n",
      "Epoch 3/20\n",
      "21059/21059 [==============================] - 18s 834us/step - loss: 0.0119 - categorical_accuracy: 0.4041 - val_loss: 0.0136 - val_categorical_accuracy: 0.3151\n",
      "Epoch 4/20\n",
      "21059/21059 [==============================] - 15s 722us/step - loss: 0.0113 - categorical_accuracy: 0.4303 - val_loss: 0.0135 - val_categorical_accuracy: 0.3303\n",
      "Epoch 5/20\n",
      "21059/21059 [==============================] - 13s 597us/step - loss: 0.0110 - categorical_accuracy: 0.4531 - val_loss: 0.0134 - val_categorical_accuracy: 0.3371\n",
      "Epoch 6/20\n",
      "21059/21059 [==============================] - 15s 731us/step - loss: 0.0108 - categorical_accuracy: 0.4711 - val_loss: 0.0134 - val_categorical_accuracy: 0.3468\n",
      "Epoch 7/20\n",
      "21059/21059 [==============================] - 16s 749us/step - loss: 0.0106 - categorical_accuracy: 0.4853 - val_loss: 0.0135 - val_categorical_accuracy: 0.3500\n",
      "Epoch 8/20\n",
      "21059/21059 [==============================] - 18s 877us/step - loss: 0.0104 - categorical_accuracy: 0.4954 - val_loss: 0.0133 - val_categorical_accuracy: 0.3592\n",
      "Epoch 9/20\n",
      "21059/21059 [==============================] - 16s 763us/step - loss: 0.0102 - categorical_accuracy: 0.5013 - val_loss: 0.0132 - val_categorical_accuracy: 0.3614\n",
      "Epoch 10/20\n",
      "21059/21059 [==============================] - 19s 925us/step - loss: 0.0101 - categorical_accuracy: 0.5003 - val_loss: 0.0133 - val_categorical_accuracy: 0.3622\n",
      "Epoch 11/20\n",
      "21059/21059 [==============================] - 16s 770us/step - loss: 0.0100 - categorical_accuracy: 0.5149 - val_loss: 0.0134 - val_categorical_accuracy: 0.3632\n",
      "Epoch 12/20\n",
      "21059/21059 [==============================] - 19s 900us/step - loss: 0.0099 - categorical_accuracy: 0.5234 - val_loss: 0.0135 - val_categorical_accuracy: 0.3673\n",
      "Epoch 13/20\n",
      "21059/21059 [==============================] - 16s 770us/step - loss: 0.0098 - categorical_accuracy: 0.5335 - val_loss: 0.0136 - val_categorical_accuracy: 0.3671\n",
      "Epoch 14/20\n",
      "21059/21059 [==============================] - 15s 710us/step - loss: 0.0098 - categorical_accuracy: 0.5385 - val_loss: 0.0136 - val_categorical_accuracy: 0.3666\n",
      "Epoch 15/20\n",
      "21059/21059 [==============================] - 14s 674us/step - loss: 0.0097 - categorical_accuracy: 0.5425 - val_loss: 0.0137 - val_categorical_accuracy: 0.3670\n",
      "Epoch 16/20\n",
      "21059/21059 [==============================] - 14s 663us/step - loss: 0.0097 - categorical_accuracy: 0.5455 - val_loss: 0.0138 - val_categorical_accuracy: 0.3726\n",
      "Epoch 17/20\n",
      "21059/21059 [==============================] - 16s 751us/step - loss: 0.0096 - categorical_accuracy: 0.5516 - val_loss: 0.0139 - val_categorical_accuracy: 0.3723\n",
      "Epoch 18/20\n",
      "21059/21059 [==============================] - 13s 597us/step - loss: 0.0096 - categorical_accuracy: 0.5551 - val_loss: 0.0140 - val_categorical_accuracy: 0.3713\n",
      "Epoch 19/20\n",
      "21059/21059 [==============================] - 15s 726us/step - loss: 0.0095 - categorical_accuracy: 0.5592 - val_loss: 0.0140 - val_categorical_accuracy: 0.3753\n",
      "Epoch 20/20\n",
      "21059/21059 [==============================] - 14s 688us/step - loss: 0.0095 - categorical_accuracy: 0.5631 - val_loss: 0.0141 - val_categorical_accuracy: 0.3736\n",
      "6580/6580 [==============================] - 2s 314us/step\n",
      "loss :  0.012975156790715583\n",
      "categorical accuracy:  0.4311550259590149\n",
      "####################################\n",
      "For threshold:  0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user1/DGS-env/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/user1/DGS-env/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples-average quality numbers\n",
      "Precision: 0.2380, Recall: 0.6459, F1-measure: 0.3210\n",
      "For threshold:  0.04\n",
      "Samples-average quality numbers\n",
      "Precision: 0.2799, Recall: 0.6191, F1-measure: 0.3582\n",
      "For threshold:  0.06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user1/DGS-env/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1465: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  average, \"true nor predicted\", 'F-score is', len(true_sum)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples-average quality numbers\n",
      "Precision: 0.3050, Recall: 0.5986, F1-measure: 0.3772\n",
      "For threshold:  0.08\n",
      "Samples-average quality numbers\n",
      "Precision: 0.3287, Recall: 0.5798, F1-measure: 0.3936\n",
      "For threshold:  0.1\n",
      "Samples-average quality numbers\n",
      "Precision: 0.3483, Recall: 0.5579, F1-measure: 0.4040\n",
      "For threshold:  0.12\n",
      "Samples-average quality numbers\n",
      "Precision: 0.3625, Recall: 0.5363, F1-measure: 0.4097\n",
      "For threshold:  0.14\n",
      "Samples-average quality numbers\n",
      "Precision: 0.3767, Recall: 0.5146, F1-measure: 0.4140\n",
      "For threshold:  0.16\n",
      "Samples-average quality numbers\n",
      "Precision: 0.3848, Recall: 0.4945, F1-measure: 0.4139\n",
      "For threshold:  0.2\n",
      "Samples-average quality numbers\n",
      "Precision: 0.3857, Recall: 0.4539, F1-measure: 0.4025\n",
      "For threshold:  0.25\n",
      "Samples-average quality numbers\n",
      "Precision: 0.3708, Recall: 0.4057, F1-measure: 0.3774\n",
      "For threshold:  0.3\n",
      "Samples-average quality numbers\n",
      "Precision: 0.3503, Recall: 0.3649, F1-measure: 0.3503\n",
      "For threshold:  0.35\n",
      "Samples-average quality numbers\n",
      "Precision: 0.3292, Recall: 0.3325, F1-measure: 0.3259\n",
      "For threshold:  0.4\n",
      "Samples-average quality numbers\n",
      "Precision: 0.3033, Recall: 0.3019, F1-measure: 0.2990\n",
      "For threshold:  0.5\n",
      "Samples-average quality numbers\n",
      "Precision: 0.2510, Recall: 0.2436, F1-measure: 0.2451\n",
      "For threshold:  0.6\n",
      "Samples-average quality numbers\n",
      "Precision: 0.2101, Recall: 0.2017, F1-measure: 0.2040\n",
      "For threshold:  0.7\n",
      "Samples-average quality numbers\n",
      "Precision: 0.1754, Recall: 0.1669, F1-measure: 0.1695\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras_self_attention import SeqSelfAttention\n",
    "\n",
    "inputs = keras.layers.Input(shape=(1,1000))\n",
    "#lstm = keras.layers.Bidirectional(keras.layers.LSTM(units=256,\n",
    "                                                    #return_sequences=True))(inputs)\n",
    "att = SeqSelfAttention(attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL,\n",
    "                       kernel_regularizer=keras.regularizers.l2(1e-4),\n",
    "                       bias_regularizer=keras.regularizers.l1(1e-4),\n",
    "                       attention_regularizer_weight=1e-4,\n",
    "                       name='Attention')(inputs)\n",
    "\n",
    "flatten = keras.layers.Flatten(name='Flatten')(att)\n",
    "dense = keras.layers.Dense(units=y_train.shape[1], name='Dense')(flatten)\n",
    "\n",
    "model = keras.models.Model(inputs=inputs, outputs=[dense])\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n",
    "\n",
    "\n",
    "epochs = 20\n",
    "batch_size = 32\n",
    "\n",
    "history = model.fit(X_train_, y_train, epochs=epochs, batch_size=batch_size,validation_split=0.2)\n",
    "\n",
    "score,cat_acc = model.evaluate(X_test_,y_test)\n",
    "\n",
    "y_pred = model.predict(X_test_)\n",
    "\n",
    "print('loss : ', score)\n",
    "print('categorical accuracy: ',cat_acc)\n",
    "\n",
    "print('####################################')\n",
    "\n",
    "thresholds = [0.01,0.04,0.06,0.08,0.1,0.12,0.14,0.16,0.2,0.25,0.3,0.35,0.4,0.5,0.6,0.7]\n",
    "for val in thresholds:\n",
    "    print(\"For threshold: \", val)\n",
    "    pred=y_pred.copy()\n",
    "  \n",
    "    pred[pred>=val]=1\n",
    "    pred[pred<val]=0\n",
    "  \n",
    "    precision = precision_score(y_test, pred, average='samples')\n",
    "    recall = recall_score(y_test, pred, average='samples')\n",
    "    f1 = f1_score(y_test, pred, average='samples')\n",
    "   \n",
    "    print(\"Samples-average quality numbers\")\n",
    "    print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commentaire : \n",
    "\n",
    "Le modèle semble réellement bloqué a une valeure seuil de 0.67. Que ce soit des réseau recurent ou bien des mécanismes d'attention le score reste identique après 10 epochs. Nous allons donc changer de représentation car nous pensons avoir atteint les limite de la représentation en tfidf+SVD.\n",
    "\n",
    "## Et si l'on changait de représentation ? Utilisation d'Embedding globaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = train['DESCRIPTION_INCIDENT']+'. '+train['ETAT_PATIENT']#+ '. '+train['ACTION_PATIENT']\n",
    "df_test = test['DESCRIPTION_INCIDENT']+'. '+test['ETAT_PATIENT']#+'. '+test['ACTION_PATIENT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61213\n",
      "(26324, 300) (6580, 300)\n"
     ]
    }
   ],
   "source": [
    "# The maximum number of words to be used. \n",
    "MAX_NB_WORDS = 100000\n",
    "# Max number of words in each complaint.\n",
    "MAX_SEQUENCE_LENGTH = 300\n",
    "# This is fixed.\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(df_train.values)\n",
    "word_index = tokenizer.word_index\n",
    "print(len(word_index))\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(df_train.values)\n",
    "X_test = tokenizer.texts_to_sequences(df_test.values)\n",
    "word2index_inputs =  tokenizer.word_index\n",
    "\n",
    "X_train = pad_sequences(X_train,MAX_SEQUENCE_LENGTH)\n",
    "X_test = pad_sequences(X_test,MAX_SEQUENCE_LENGTH)\n",
    "print(X_train.shape,X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user1/DGS-env/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 21059 samples, validate on 5265 samples\n",
      "Epoch 1/10\n",
      "21059/21059 [==============================] - 826s 39ms/step - loss: 0.0103 - categorical_accuracy: 0.1853 - val_loss: 0.0104 - val_categorical_accuracy: 0.1896\n",
      "Epoch 2/10\n",
      "21059/21059 [==============================] - 855s 41ms/step - loss: 0.0096 - categorical_accuracy: 0.1999 - val_loss: 0.0099 - val_categorical_accuracy: 0.1987\n",
      "Epoch 3/10\n",
      "21059/21059 [==============================] - 845s 40ms/step - loss: 0.0091 - categorical_accuracy: 0.2434 - val_loss: 0.0098 - val_categorical_accuracy: 0.1937\n",
      "Epoch 4/10\n",
      "21059/21059 [==============================] - 827s 39ms/step - loss: 0.0087 - categorical_accuracy: 0.2716 - val_loss: 0.0095 - val_categorical_accuracy: 0.2133\n",
      "Epoch 5/10\n",
      "21056/21059 [============================>.] - ETA: 0s - loss: 0.0082 - categorical_accuracy: 0.3124"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X_train.shape[1]))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(200))\n",
    "model.add(Dense(y_train.shape[1],activation='softmax'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,validation_split=0.2)\n",
    "\n",
    "\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print('loss : ', score)\n",
    "print('categorical accuracy: ',cat_acc)\n",
    "\n",
    "print('####################################')\n",
    "\n",
    "thresholds = [0.01,0.04,0.06,0.08,0.1,0.12,0.14,0.16,0.2,0.25,0.3,0.35,0.4,0.5,0.6,0.7]\n",
    "for val in thresholds:\n",
    "    print(\"For threshold: \", val)\n",
    "    pred=y_pred.copy()\n",
    "  \n",
    "    pred[pred>=val]=1\n",
    "    pred[pred<val]=0\n",
    "  \n",
    "    precision = precision_score(y_test, pred, average='samples')\n",
    "    recall = recall_score(y_test, pred, average='samples')\n",
    "    f1 = f1_score(y_test, pred, average='samples')\n",
    "   \n",
    "    print(\"Samples-average quality numbers\")\n",
    "    print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commentaire : \n",
    "Les embedding appris n'offre pas de meilleurs résultats pour l'instant. Nous allons essayer de créer des embeddings par colonnes de texte, cette solution c'etait avérée fructueuse dans le cas du tfidf.\n",
    "\n",
    "## Et si l'on séparait les entrées ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model constants.\n",
    "EMBEDDING_DIM =300\n",
    "MAX_SEQUENCE_LENGTH =300\n",
    "MAX_NB_WORDS = 50000\n",
    "\n",
    "def vectorize(df_train,df_test,MAX_NB_WORDS,MAX_SEQUENCE_LENGTH ):\n",
    "    tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "    tokenizer.fit_on_texts(df_train.values)\n",
    "    word_index = tokenizer.word_index\n",
    "    print(len(word_index))\n",
    "\n",
    "    X_train = tokenizer.texts_to_sequences(df_train.values)\n",
    "    X_test = tokenizer.texts_to_sequences(df_test.values)\n",
    "    word2index_inputs =  tokenizer.word_index\n",
    "\n",
    "    X_train = pad_sequences(X_train,MAX_SEQUENCE_LENGTH)\n",
    "    X_test = pad_sequences(X_test,MAX_SEQUENCE_LENGTH)\n",
    "    return (X_train, X_test)\n",
    "TRAIN = []\n",
    "for col in ['DESCRIPTION_INCIDENT', 'ETAT_PATIENT', 'FABRICANT'] : \n",
    "    X_train,X_test = vectorize(train[col],test[col],MAX_NB_WORDS,MAX_SEQUENCE_LENGTH )\n",
    "    TRAIN.append((X_train,X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_1 = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "inputs_2 = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "inputs_3 = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "#x = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH)(inputs)\n",
    "#x = Reshape((MAX_SEQUENCE_LENGTH,1,))(inputs_1)\n",
    "x = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH)(inputs_1)\n",
    "x = SpatialDropout1D(0.2)(x)\n",
    "x = LSTM(200)(x)\n",
    "\n",
    "\n",
    "#y = Reshape((MAX_SEQUENCE_LENGTH,1,))(inputs_2)\n",
    "y = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH)(inputs_2)\n",
    "y = SpatialDropout1D(0.2)(y)\n",
    "y = LSTM(200)(y)\n",
    "\n",
    "\n",
    "#z = Reshape((MAX_SEQUENCE_LENGTH,1,))(inputs_3)\n",
    "z = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH)(inputs_3)\n",
    "z = SpatialDropout1D(0.2)(z)\n",
    "z = LSTM(200)(z)\n",
    "\n",
    "\n",
    "w = concatenate([x, y, z])\n",
    "\n",
    "out =  Dense(y_train.shape[1],activation='softmax')(w)\n",
    "\n",
    "model = keras.models.Model(inputs=[inputs_1,inputs_2,inputs_3], outputs=out)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit([TRAIN[0][0],TRAIN[1][0],TRAIN[2][0]], y_train, epochs=10, validation_split=0.2, verbose=1, batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict([TRAIN[0][1],TRAIN[1][1],TRAIN[2][1]])\n",
    "\n",
    "print('loss : ', score)\n",
    "print('categorical accuracy: ',cat_acc)\n",
    "\n",
    "print('####################################')\n",
    "\n",
    "thresholds = [0.01,0.04,0.06,0.08,0.1,0.12,0.14,0.16,0.2,0.25,0.3,0.35,0.4,0.5,0.6,0.7]\n",
    "for val in thresholds:\n",
    "    print(\"For threshold: \", val)\n",
    "    pred=y_pred.copy()\n",
    "  \n",
    "    pred[pred>=val]=1\n",
    "    pred[pred<val]=0\n",
    "  \n",
    "    precision = precision_score(y_test, pred, average='samples')\n",
    "    recall = recall_score(y_test, pred, average='samples')\n",
    "    f1 = f1_score(y_test, pred, average='samples')\n",
    "   \n",
    "    print(\"Samples-average quality numbers\")\n",
    "    print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Commentaire :\n",
    "Nous observons que notre modèle sur apprend très rapidement et de manière importante. nous avons deux solutions classiques pour contrer cet effet : \n",
    "- Regularisation\n",
    "- Drop Out\n",
    "- Netoyer les données avec clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp =spacy.load('fr')\n",
    "from spacy.lang.fr.stop_words import STOP_WORDS\n",
    "\n",
    "import clean_text\n",
    "stop_words = STOP_WORDS\n",
    "\n",
    "\n",
    "def vectorize(df_train,df_test,MAX_NB_WORDS,MAX_SEQUENCE_LENGTH,stopword = False ):\n",
    "    tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "    if stopword==True : \n",
    "        df_train = df_train.map(lambda x: clean_text.preprocess_text(x))\n",
    "        df_train = df_train.map(lambda x: [item for item in x.split(\" \") if item not in stop_words])\n",
    "    \n",
    "    tokenizer.fit_on_texts(df_train.values)\n",
    "    word_index = tokenizer.word_index\n",
    "    print(len(word_index))\n",
    "\n",
    "    X_train = tokenizer.texts_to_sequences(df_train.values)\n",
    "    X_test = tokenizer.texts_to_sequences(df_test.values)\n",
    "    word2index_inputs =  tokenizer.word_index\n",
    "\n",
    "    X_train = pad_sequences(X_train,MAX_SEQUENCE_LENGTH)\n",
    "    X_test = pad_sequences(X_test,MAX_SEQUENCE_LENGTH)\n",
    "    return (X_train, X_test)\n",
    "TRAIN = []\n",
    "for col in ['DESCRIPTION_INCIDENT', 'ETAT_PATIENT', 'FABRICANT'] : \n",
    "    X_train,X_test = vectorize(train[col],test[col],MAX_NB_WORDS,MAX_SEQUENCE_LENGTH,stopword=True )\n",
    "    TRAIN.append((X_train,X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inputs_1 = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "inputs_2 = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "inputs_3 = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "#x = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH)(inputs)\n",
    "#x = Reshape((MAX_SEQUENCE_LENGTH,1,))(inputs_1)\n",
    "x = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH)(inputs_1)\n",
    "x = SpatialDropout1D(0.2)(x)\n",
    "x = LSTM(200)(x)\n",
    "\n",
    "\n",
    "#y = Reshape((MAX_SEQUENCE_LENGTH,1,))(inputs_2)\n",
    "y = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH)(inputs_2)\n",
    "y = SpatialDropout1D(0.2)(y)\n",
    "y = LSTM(200)(y)\n",
    "\n",
    "\n",
    "#z = Reshape((MAX_SEQUENCE_LENGTH,1,))(inputs_3)\n",
    "z = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH)(inputs_3)\n",
    "z = SpatialDropout1D(0.2)(z)\n",
    "z = LSTM(200)(z)\n",
    "\n",
    "\n",
    "w = concatenate([x, y, z])\n",
    "\n",
    "out =  Dense(y_train.shape[1],activation='softmax')(w)\n",
    "\n",
    "model = keras.models.Model(inputs=[inputs_1,inputs_2,inputs_3], outputs=out)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit([TRAIN[0][0],TRAIN[1][0],TRAIN[2][0]], y_train, epochs=5, validation_split=0.2, verbose=1, batch_size = 64)\n",
    "\n",
    "y_pred = model.predict([TRAIN[0][1],TRAIN[1][1],TRAIN[2][1]])\n",
    "\n",
    "print('loss : ', score)\n",
    "print('categorical accuracy: ',cat_acc)\n",
    "\n",
    "print('####################################')\n",
    "\n",
    "thresholds = [0.01,0.04,0.06,0.08,0.1,0.12,0.14,0.16,0.2,0.25,0.3,0.35,0.4,0.5,0.6,0.7]\n",
    "for val in thresholds:\n",
    "    print(\"For threshold: \", val)\n",
    "    pred=y_pred.copy()\n",
    "  \n",
    "    pred[pred>=val]=1\n",
    "    pred[pred<val]=0\n",
    "  \n",
    "    precision = precision_score(y_test, pred, average='samples')\n",
    "    recall = recall_score(y_test, pred, average='samples')\n",
    "    f1 = f1_score(y_test, pred, average='samples')\n",
    "   \n",
    "    print(\"Samples-average quality numbers\")\n",
    "    print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion : \n",
    " Suite aux diverses expériences que nousa vosn mener dans ce notebook, nos conclusions sont : \n",
    "- La tf-idf est une représentation riche qui est difficile à battre tant en termes de performances qu'en temps de calcul.\n",
    "- Les modèles d'attention régularisés évitent mieux le sur-apprentissage que les modèles recurent avec du drop out\n",
    "- séparer les collones pour encoder offre de meillers résultats mais toujours moins bon que ce proposé par le TF-iDF\n",
    "- Lef ait d'enlever les stop_words n'est pas \n",
    "\n",
    "\n",
    "Ce que nous devons essayer : \n",
    "- Multi head attention https://www.kaggle.com/fareise/multi-head-self-attention-for-text-classification, https://github.com/CyberZHG/keras-multi-head\n",
    "- Hierarchical attention : https://paperswithcode.com/paper/hierarchical-attentional-hybrid-neural\n",
    "- Concatenation des embedings et du tfidf\n",
    "- chercher de nouvelles méthodes de régularisation pour les réseaux récurrents\n",
    "- tester les CNN : https://www.kaggle.com/sanikamal/text-classification-with-python-and-keras\n",
    "- ajouter une couche de positinal encoding : https://github.com/kaushalshetty/Positional-Encoding\n",
    "- Librairie à essyaer rapidement :\n",
    "    - text-classification-keras : https://pypi.org/project/text-classification-keras/\n",
    "    - pytext :  https://github.com/facebookresearch/pytext\n",
    "\n",
    "- approche avec des emmbedings déjà entrainés : \n",
    "    - https://adventuresinmachinelearning.com/word2vec-keras-tutorial/\n",
    "    - https://medium.com/@ppasumarthi_69210/word-embeddings-in-keras-be6bb3092831\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DGS-env",
   "language": "python",
   "name": "dgs-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
